{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dfe0c3",
   "metadata": {},
   "source": [
    "## 5. ONNX\n",
    "\n",
    "ONNX (Open Neural Network eXchange) is a standardized format for representing neural networks. It abstracts\n",
    "operations, turning the framework-specific code into an **execution graph** built from standardized operators.\n",
    "It describes operations, input/output shapes, and model parameters in a hardware- and framework-agnostic way.\n",
    "Then, it can be run with via ONNX Runtime (ORT), which can execute the code with kernels and optimizations from\n",
    "specialized providers, like Intel OpenVINO or NVidia TensorRT.\n",
    "\n",
    "ONNX and ONNX Runtime have considerable advantages:\n",
    "1. Framework- and language-agnostic - ONNX runs on any framework and programming language, e.g. you can export\n",
    "   PyTorch model in Python, and then run it in a Java application.\n",
    "2. Execution graph optimization - ONNX Runtime provides a series of optimizations for the execution graph,\n",
    "   including hardware-specific operators provided by manufacturers.\n",
    "3. Lightweight deployment - ONNX & ORT have much smaller package size than the whole PyTorch (even CPU-only wheels),\n",
    "   reducing sizes of dependencies and Docker containers, and accelerating loading.\n",
    "\n",
    "In practice, `torch.compile()` works well for PyTorch optimization, but ONNX is preferable for deploying models,\n",
    "particularly for lightweight or mobile runtimes. It also supports GPU inference via NVidia TensorRT provider.\n",
    "\n",
    "Exporting to ONNX produces a raw computation graph in `.onnx` format. This file is:\n",
    "- a static description of operators, weights, and I/O tensors\n",
    "- a general graph - no hardware-specific rewrites happen during ONNX export\n",
    "- hardware-agnostic - it does not contain CUDA/CPU kernels or provider information\n",
    "\n",
    "We will export a Transformer model with dynamic batch size and dynamic sequence length.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Put the model in eval mode and move to CPU\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "# Example input for tracking (for onnx export)\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "We export on CPU in `eval()` mode to get deterministic behavior.\n",
    "\n",
    "Look at how we marked dynamic axes in `dynamic_axes`:\n",
    "1. For `input_ids` and `attention_mask`, we marked axes 0 (batch size) and 1 (sequence length) as dynamic,\n",
    "   since they can vary during inference.\n",
    "   - axis 0 - batch size, depends on number of inputs\n",
    "   - axis 1 - sequence length, depends on text length\n",
    "   - axis 2 - embedding size, fixed and constant (768), so we don't mark it\n",
    "2. For `output`, we marked only axis 0 (batch size) as dynamic, since the output will have the same number\n",
    "   of rows as the input batch size.\n",
    "\n",
    "The exported `model.onnx` is a raw graph, not yet optimized. It can be changed during InferenceSession\n",
    "creation in ONNX Runtime or when we explicitly run offline optimizations.\n",
    "\n",
    "### Optimization & inference with ONNX Runtime\n",
    "\n",
    "First, we run inference using ONNX Runtime with default settings. By default, all optimizations are applied.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Prepare input data\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create input dictionary, in same format as during export\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "# Run inference\n",
    "outputs_onnx = ort_session.run(None, inputs_onnx)\n",
    "```\n",
    "\n",
    "The raw ONNX is parsed, optimized (default level is `ORT_ENABLE_ALL`), and executed using the default\n",
    "execution provider (generally generic CPU by default).\n",
    "\n",
    "We did not specify a provider in this example to keep the code short. ONNX Runtime internally chooses\n",
    "providers based on how it was built (for example, CPU only, or CPU + CUDA). For production use, you\n",
    "should specify providers explicitly. We will do that in the next section.\n",
    "\n",
    "### Graph optimization settings\n",
    "\n",
    "ONNX Runtime groups graph optimizations into levels. Each level builds on the previous one:\n",
    "\n",
    "1. **Basic graph optimizations** - semantics-preserving rewrites that remove redundant work.\n",
    "   They run before graph partitioning, so they apply to nodes regardless of the target execution provider.\n",
    "2. **Extended graph optimizations** - They run after graph partitioning and are applied only to nodes\n",
    "   assigned to selected providers (CPU, CUDA, ROCm).\n",
    "3. **Layout optimizations** - change layout from NHCW to NCHWc for CPU provider.\n",
    "\n",
    "All optimizations are enabled by default. You can control them using the `GraphOptimizationLevel` enum:\n",
    "* `ORT_DISABLE_ALL` – disable all optimizations\n",
    "* `ORT_ENABLE_BASIC` – only basic\n",
    "* `ORT_ENABLE_EXTENDED` – basic and extended\n",
    "* `ORT_ENABLE_ALL` – basic + extended + layout optimizations (default)\n",
    "\n",
    "### Online mode (load-time optimization)\n",
    "\n",
    "In online mode, optimizations are applied each time you create an `InferenceSession`.\n",
    "This happens when you create it:\n",
    "```python\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "```\n",
    "We can control the optimization level using `SessionOptions`:\n",
    "\n",
    "```python\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Online mode is most convenient for:\n",
    "- development and experimentation - you can quickly try out different settings\n",
    "- dynamic environments - when running on different hardware or deployments, depending on settings\n",
    "\n",
    "The cost of online mode is that optimization work is repeated each time a session is created, which\n",
    "may be noticeable for large models. When you deploy to a known target each time, offline mode is\n",
    "a better choice.\n",
    "\n",
    "### Offline mode (ahead-of-time optimization)\n",
    "\n",
    "In offline mode, optimizations are applied once, and the optimized model is saved to a new ONNX file.\n",
    "This can significantly reduce startup time in production environments. The key element is setting the\n",
    "`SessionOptions.optimized_model_filepath`, which specifies where to save the optimized model.\n",
    "When enabled, ONNX Runtime runs graph optimizations according to `graph_optimization_level`, and saves\n",
    "the optimized model to the file.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "# Choose the optimization level for the offline pass\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "# Save the optimized model to this path\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "\n",
    "# Create InferenceSession, which will perform offline optimization and save the optimized model\n",
    "ort.InferenceSession(\"model.onnx\", sess_options)\n",
    "```\n",
    "\n",
    "After you can load this file and disable optimizations to avoid re-optimizing:\n",
    "\n",
    "```python\n",
    "# Load the optimized model without re-optimizing\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "\n",
    "ort_session_optimized = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\",\n",
    "    sess_options=sess_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "```\n",
    "\n",
    "Offline mode is best suited for:\n",
    "- production deployments - startup time is important, and the model changes only during training\n",
    "- limited resource environments - repeated optimization is costly\n",
    "- static hardware setups - when we know the hardware configuration, there is no need for re-optimization\n",
    "\n",
    "### Executions Providers\n",
    "\n",
    "Execution providers decide how and where the nodes of the ONNX graph are executed. They are not an\n",
    "extra optimization pass on top of the graph. Instead, they are backends that provide concrete kernel\n",
    "implementations for operators such as `MatMul`, `Conv`, `LayerNorm`, and so on.\n",
    "\n",
    "Typical providers include:\n",
    "\n",
    "* `CPUExecutionProvider`\n",
    "* `CUDAExecutionProvider`\n",
    "* `TensorrtExecutionProvider`\n",
    "* `OpenVINOExecutionProvider`\n",
    "\n",
    "The ONNX file itself is always hardware-agnostic. It does not contain any provider information.\n",
    "Providers come into play only when you create an `InferenceSession`. Provider is responsible for:\n",
    "\n",
    "* mapping ONNX operations to actual kernels, e.g. CPU BLAS vs cuBLAS vs TensorRT engines\n",
    "* deciding which fused patterns it can execute efficiently for extended optimizations\n",
    "* executing its part of the graph on the target hardware\n",
    "\n",
    "So why do we need to care about providers? In production, it is better to be explicit, so that the behavior\n",
    "does not change when you move the same model to a different environment.\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Force CPU only\n",
    "session_cpu = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "# Prefer CUDA, fall back to CPU if CUDA is not available\n",
    "session_cuda = ort.InferenceSession(\n",
    "    \"model.onnx\",\n",
    "    sess_options=options,\n",
    "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n",
    ")\n",
    "```\n",
    "\n",
    "For more information about providers, see the official [Execution Providers section](https://iot-robotics.github.io/ONNXRuntime/docs/execution-providers/).\n",
    "\n",
    "### Exercise 6 (3 points)\n",
    "\n",
    "1. Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes\n",
    "   on CPU.\n",
    "2. Measure inference time of the ONNX model on CPU using both optimization modes.\n",
    "3. Prepare deployment Docker images:\n",
    "   - build two images, for a) compiled PyTorch model b) ONNX model with ONNX Runtime\n",
    "   - select the best model in both cases in terms of the inference time\n",
    "   - install a minimal set of requirements in both cases, e.g. do not install PyTorch for ONNX image\n",
    "4. Compare for those apps:\n",
    "   - Docker container sizes\n",
    "   - response time (average of 100 requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5ca6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c583de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = 'cpu'\n",
    "model.to(device).eval()\n",
    "num_runs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13849e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_for_onnx_export = \"sample input text for ONNX export\"\n",
    "sample_input_for_export = tokenizer(\n",
    "    sample_text_for_onnx_export,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "sample_text_for_onnx_inference = \"sample input text for ONNX inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be495d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/fv7hj1555_7_dl5892_yp6jc0000gn/T/ipykernel_15579/3454461695.py:8: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1125 21:41:09.012000 15579 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W1125 21:41:09.300000 15579 torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MPNetModel([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MPNetModel([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 83 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 17},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input_ids\"<INT64,[s43,s53]>,\n",
       "                %\"attention_mask\"<INT64,[s43,s53]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[1,s53,768]>,\n",
       "                %\"tanh\"<FLOAT,[1,768]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"embeddings.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"embeddings.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.q.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.k.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.v.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.attn.o.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.attention.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.dense.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.LayerNorm.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.output.LayerNorm.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"encoder.relative_attention_bias.weight\"<FLOAT,[32,12]>{TorchTensor(...)},\n",
       "                %\"embeddings.word_embeddings.weight\"<FLOAT,[30527,768]>{TorchTensor(...)},\n",
       "                %\"embeddings.position_embeddings.weight\"<FLOAT,[514,768]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.0.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.1.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.2.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.3.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.4.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.5.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.6.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.7.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.8.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.9.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.10.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"encoder.layer.11.intermediate.dense.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"pooler.dense.weight\"<FLOAT,[768,768]>{TorchTensor(...)},\n",
       "                %\"val_34\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_34')},\n",
       "                %\"val_35\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_35')},\n",
       "                %\"scalar_tensor_default\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(8., dtype=float32), name='scalar_tensor_default')},\n",
       "                %\"val_64\"<INT64,[]>{Tensor<INT64,[]>(array(15), name='val_64')},\n",
       "                %\"val_72\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_80\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_88\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_102\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_106\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_115\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_119\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_127\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_135\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_148\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_152\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_161\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_165\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_173\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_181\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_194\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_198\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_207\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_211\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_219\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_227\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_240\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_244\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_253\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_257\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_265\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_273\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_286\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_290\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_299\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_303\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_311\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_319\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_332\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_336\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_345\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_349\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_357\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_365\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_378\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_382\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_391\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_395\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_403\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_411\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_424\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_428\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_437\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_441\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_449\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_457\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_470\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_474\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_483\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_487\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_495\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_503\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_516\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_520\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_529\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_533\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_541\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_549\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_562\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_566\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_575\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_579\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_587\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_595\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_608\"<FLOAT,[768,768]>{Tensor(...)},\n",
       "                %\"val_612\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_621\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_14\"<INT64,[1]>{TensorProtoTensor<INT64,[1]>(array([1]), name='val_14')},\n",
       "                %\"val_635\"<INT64,[2]>{Tensor<INT64,[2]>(array([1, 2]), name='val_635')},\n",
       "                %\"val_27\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(1., dtype=float32), name='val_27')},\n",
       "                %\"val_28\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(-3.4028235e+38, dtype=float32), name='val_28')},\n",
       "                %\"val_49\"<INT64,[1]>{TensorProtoTensor<INT64,[1]>(array([0]), name='val_49')},\n",
       "                %\"val_60\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(16), name='val_60')},\n",
       "                %\"val_61\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(8), name='val_61')},\n",
       "                %\"val_62\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(2.7725887, dtype=float32), name='val_62')},\n",
       "                %\"val_76\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_76')},\n",
       "                %\"val_77\"<INT64,[1]>{Tensor<INT64,[1]>(array([12]), name='val_77')},\n",
       "                %\"val_78\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_78')},\n",
       "                %\"val_100\"<INT64,[1]>{Tensor<INT64,[1]>(array([768]), name='val_100')},\n",
       "                %\"val_108\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(1.4142135, dtype=float32), name='val_108')},\n",
       "                %\"val_113\"<FLOAT,[]>{TensorProtoTensor<FLOAT,[]>(array(0.5, dtype=float32), name='val_113')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"attention_mask\") {end=1, start=0}\n",
       "              1 |  # node_Shape_1\n",
       "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"attention_mask\") {end=2, start=1}\n",
       "              2 |  # node_sym_size_int_86\n",
       "                   %\"sym_size_int_86\"<INT64,[]> ⬅️ ::Squeeze(%\"val_1\")\n",
       "              3 |  # node_Unsqueeze_24\n",
       "                   %\"unsqueeze_1\"<INT64,[s43,1,1,s53]> ⬅️ ::Unsqueeze(%\"attention_mask\", %\"val_635\"{[1, 2]})\n",
       "              4 |  # node__to_copy\n",
       "                   %\"_to_copy\"<FLOAT,[s43,1,1,s53]> ⬅️ ::Cast(%\"unsqueeze_1\") {to=1}\n",
       "              5 |  # node_sub_10\n",
       "                   %\"sub_10\"<FLOAT,[s43,1,1,s53]> ⬅️ ::Sub(%\"val_27\"{1.0}, %\"_to_copy\")\n",
       "              6 |  # node_mul_22\n",
       "                   %\"mul_22\"<FLOAT,[1,1,1,s53]> ⬅️ ::Mul(%\"sub_10\", %\"val_28\"{-3.4028234663852886e+38})\n",
       "              7 |  # node_Equal_30\n",
       "                   %\"val_30\"<BOOL,[s43,s53]> ⬅️ ::Equal(%\"input_ids\", %\"val_35\"{1})\n",
       "              8 |  # node_ne_3\n",
       "                   %\"ne_3\"<BOOL,[s43,s53]> ⬅️ ::Not(%\"val_30\")\n",
       "              9 |  # node__to_copy_1\n",
       "                   %\"_to_copy_1\"<INT32,[s43,s53]> ⬅️ ::Cast(%\"ne_3\") {to=6}\n",
       "             10 |  # node_convert_element_type_default\n",
       "                   %\"convert_element_type_default\"<INT64,[s43,s53]> ⬅️ ::Cast(%\"_to_copy_1\") {to=7}\n",
       "             11 |  # node_cumsum\n",
       "                   %\"cumsum\"<INT64,[1,s53]> ⬅️ ::CumSum(%\"convert_element_type_default\", %\"val_35\"{1}) {reverse=0, exclusive=0}\n",
       "             12 |  # node_type_as\n",
       "                   %\"type_as\"<INT32,[1,s53]> ⬅️ ::Cast(%\"cumsum\") {to=6}\n",
       "             13 |  # node_mul_34\n",
       "                   %\"mul_34\"<INT32,[s43,s53]> ⬅️ ::Mul(%\"type_as\", %\"_to_copy_1\")\n",
       "             14 |  # node__to_copy_2\n",
       "                   %\"_to_copy_2\"<INT64,[s43,s53]> ⬅️ ::Cast(%\"mul_34\") {to=7}\n",
       "             15 |  # node_add_45\n",
       "                   %\"add_45\"<INT64,[1,s53]> ⬅️ ::Add(%\"_to_copy_2\", %\"val_35\"{1})\n",
       "             16 |  # node_embedding\n",
       "                   %\"embedding\"<FLOAT,[s43,s53,768]> ⬅️ ::Gather(%\"embeddings.word_embeddings.weight\"{...}, %\"input_ids\") {axis=0}\n",
       "             17 |  # node_embedding_1\n",
       "                   %\"embedding_1\"<FLOAT,[1,s53,768]> ⬅️ ::Gather(%\"embeddings.position_embeddings.weight\"{...}, %\"add_45\") {axis=0}\n",
       "             18 |  # node_add_55\n",
       "                   %\"add_55\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"embedding\", %\"embedding_1\")\n",
       "             19 |  # node_layer_norm\n",
       "                   %\"layer_norm\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_55\", %\"embeddings.LayerNorm.weight\"{...}, %\"embeddings.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "             20 |  # node_arange\n",
       "                   %\"arange\"<INT64,[s53]> ⬅️ ::Range(%\"val_34\"{0}, %\"sym_size_int_86\", %\"val_35\"{1})\n",
       "             21 |  # node_unsqueeze_2\n",
       "                   %\"unsqueeze_2\"<INT64,[s53,1]> ⬅️ ::Unsqueeze(%\"arange\", %\"val_14\"{[1]})\n",
       "             22 |  # node_unsqueeze_3\n",
       "                   %\"unsqueeze_3\"<INT64,[1,s53]> ⬅️ ::Unsqueeze(%\"arange\", %\"val_49\"{[0]})\n",
       "             23 |  # node_sub_37\n",
       "                   %\"sub_37\"<INT64,[s53,s53]> ⬅️ ::Sub(%\"unsqueeze_3\", %\"unsqueeze_2\")\n",
       "             24 |  # node_neg\n",
       "                   %\"neg\"<INT64,[s53,s53]> ⬅️ ::Neg(%\"sub_37\")\n",
       "             25 |  # node_lt\n",
       "                   %\"lt\"<BOOL,[s53,s53]> ⬅️ ::Less(%\"neg\", %\"val_34\"{0})\n",
       "             26 |  # node__to_copy_3\n",
       "                   %\"_to_copy_3\"<INT64,[s53,s53]> ⬅️ ::Cast(%\"lt\") {to=7}\n",
       "             27 |  # node_mul_71\n",
       "                   %\"mul_71\"<INT64,[s53,s53]> ⬅️ ::Mul(%\"_to_copy_3\", %\"val_60\"{16})\n",
       "             28 |  # node_abs_1\n",
       "                   %\"abs_1\"<INT64,[s53,s53]> ⬅️ ::Abs(%\"neg\")\n",
       "             29 |  # node_lt_1\n",
       "                   %\"lt_1\"<BOOL,[s53,s53]> ⬅️ ::Less(%\"abs_1\", %\"val_61\"{8})\n",
       "             30 |  # node__to_copy_4\n",
       "                   %\"_to_copy_4\"<FLOAT,[s53,s53]> ⬅️ ::Cast(%\"abs_1\") {to=1}\n",
       "             31 |  # node_div\n",
       "                   %\"div\"<FLOAT,[s53,s53]> ⬅️ ::Div(%\"_to_copy_4\", %\"scalar_tensor_default\"{8.0})\n",
       "             32 |  # node_log\n",
       "                   %\"log\"<FLOAT,[s53,s53]> ⬅️ ::Log(%\"div\")\n",
       "             33 |  # node_div_1\n",
       "                   %\"div_1\"<FLOAT,[s53,s53]> ⬅️ ::Div(%\"log\", %\"val_62\"{2.7725887298583984})\n",
       "             34 |  # node_mul_87\n",
       "                   %\"mul_87\"<FLOAT,[s53,s53]> ⬅️ ::Mul(%\"div_1\", %\"scalar_tensor_default\"{8.0})\n",
       "             35 |  # node__to_copy_5\n",
       "                   %\"_to_copy_5\"<INT64,[s53,s53]> ⬅️ ::Cast(%\"mul_87\") {to=7}\n",
       "             36 |  # node_add_121\n",
       "                   %\"add_121\"<INT64,[s53,s53]> ⬅️ ::Add(%\"_to_copy_5\", %\"val_61\"{8})\n",
       "             37 |  # node_Shape_63\n",
       "                   %\"val_65\"<INT64,[2]> ⬅️ ::Shape(%\"add_121\") {start=0}\n",
       "             38 |  # node_full_like\n",
       "                   %\"full_like\"<INT64,[s53,s53]> ⬅️ ::Expand(%\"val_64\"{15}, %\"val_65\")\n",
       "             39 |  # node_minimum\n",
       "                   %\"minimum\"<INT64,[s53,s53]> ⬅️ ::Min(%\"add_121\", %\"full_like\")\n",
       "             40 |  # node_where\n",
       "                   %\"where\"<INT64,[s53,s53]> ⬅️ ::Where(%\"lt_1\", %\"abs_1\", %\"minimum\")\n",
       "             41 |  # node_add_140\n",
       "                   %\"add_140\"<INT64,[s53,s53]> ⬅️ ::Add(%\"mul_71\", %\"where\")\n",
       "             42 |  # node_embedding_2\n",
       "                   %\"embedding_2\"<FLOAT,[s53,s53,12]> ⬅️ ::Gather(%\"encoder.relative_attention_bias.weight\"{...}, %\"add_140\") {axis=0}\n",
       "             43 |  # node_permute\n",
       "                   %\"permute\"<FLOAT,[12,s53,s53]> ⬅️ ::Transpose(%\"embedding_2\") {perm=(2, 0, 1)}\n",
       "             44 |  # node_unsqueeze_4\n",
       "                   %\"unsqueeze_4\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Unsqueeze(%\"permute\", %\"val_49\"{[0]})\n",
       "             45 |  # node_Concat_69\n",
       "                   %\"val_71\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_14\"{[1]}, %\"val_1\", %\"val_1\") {axis=0}\n",
       "             46 |  # node_expand\n",
       "                   %\"expand\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Expand(%\"unsqueeze_4\", %\"val_71\")\n",
       "             47 |  # node_MatMul_71\n",
       "                   %\"val_73\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_72\"{...})\n",
       "             48 |  # node_linear\n",
       "                   %\"linear\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_73\", %\"encoder.layer.0.attention.attn.q.bias\"{...})\n",
       "             49 |  # node_Concat_77\n",
       "                   %\"val_79\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_76\"{[-1]}, %\"val_77\"{[12]}, %\"val_78\"{[64]}) {axis=0}\n",
       "             50 |  # node_view\n",
       "                   %\"view\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear\", %\"val_79\") {allowzero=1}\n",
       "             51 |  # node_transpose\n",
       "                   %\"transpose\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view\") {perm=(0, 2, 1, 3)}\n",
       "             52 |  # node_MatMul_79\n",
       "                   %\"val_81\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_80\"{...})\n",
       "             53 |  # node_linear_1\n",
       "                   %\"linear_1\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_81\", %\"encoder.layer.0.attention.attn.k.bias\"{...})\n",
       "             54 |  # node_view_1\n",
       "                   %\"view_1\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_1\", %\"val_79\") {allowzero=1}\n",
       "             55 |  # node_MatMul_87\n",
       "                   %\"val_89\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_88\"{...})\n",
       "             56 |  # node_linear_2\n",
       "                   %\"linear_2\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_89\", %\"encoder.layer.0.attention.attn.v.bias\"{...})\n",
       "             57 |  # node_view_2\n",
       "                   %\"view_2\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_2\", %\"val_79\") {allowzero=1}\n",
       "             58 |  # node_transpose_2\n",
       "                   %\"transpose_2\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
       "             59 |  # node_Transpose_35\n",
       "                   %\"transpose_3\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_1\") {perm=(0, 2, 3, 1)}\n",
       "             60 |  # node_matmul\n",
       "                   %\"matmul\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose\", %\"transpose_3\")\n",
       "             61 |  # node_div_2\n",
       "                   %\"div_2\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul\", %\"scalar_tensor_default\"{8.0})\n",
       "             62 |  # node_add_213\n",
       "                   %\"add_213\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_2\", %\"expand\")\n",
       "             63 |  # node_add_219\n",
       "                   %\"add_219\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_213\", %\"mul_22\")\n",
       "             64 |  # node_softmax\n",
       "                   %\"softmax\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_219\") {axis=-1}\n",
       "             65 |  # node_matmul_1\n",
       "                   %\"matmul_1\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax\", %\"transpose_2\")\n",
       "             66 |  # node_permute_1\n",
       "                   %\"permute_1\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_1\") {perm=(0, 2, 1, 3)}\n",
       "             67 |  # node_Concat_99\n",
       "                   %\"val_101\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_1\", %\"val_100\"{[768]}) {axis=0}\n",
       "             68 |  # node_view_3\n",
       "                   %\"view_3\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_1\", %\"val_101\") {allowzero=1}\n",
       "             69 |  # node_MatMul_101\n",
       "                   %\"val_103\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_3\", %\"val_102\"{...})\n",
       "             70 |  # node_linear_3\n",
       "                   %\"linear_3\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_103\", %\"encoder.layer.0.attention.attn.o.bias\"{...})\n",
       "             71 |  # node_add_253\n",
       "                   %\"add_253\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_3\", %\"layer_norm\")\n",
       "             72 |  # node_layer_norm_1\n",
       "                   %\"layer_norm_1\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_253\", %\"encoder.layer.0.attention.LayerNorm.weight\"{...}, %\"encoder.layer.0.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "             73 |  # node_MatMul_103\n",
       "                   %\"val_107\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_106\"{...})\n",
       "             74 |  # node_linear_4\n",
       "                   %\"linear_4\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_107\", %\"encoder.layer.0.intermediate.dense.bias\"{...})\n",
       "             75 |  # node_Div_105\n",
       "                   %\"val_109\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_4\", %\"val_108\"{1.4142135381698608})\n",
       "             76 |  # node_Erf_106\n",
       "                   %\"val_110\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_109\")\n",
       "             77 |  # node_Add_108\n",
       "                   %\"val_112\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_110\", %\"val_27\"{1.0})\n",
       "             78 |  # node_Mul_110\n",
       "                   %\"val_114\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_112\")\n",
       "             79 |  # node_gelu\n",
       "                   %\"gelu\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_4\", %\"val_114\")\n",
       "             80 |  # node_MatMul_112\n",
       "                   %\"val_116\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu\", %\"val_115\"{...})\n",
       "             81 |  # node_linear_5\n",
       "                   %\"linear_5\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_116\", %\"encoder.layer.0.output.dense.bias\"{...})\n",
       "             82 |  # node_add_272\n",
       "                   %\"add_272\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_5\", %\"layer_norm_1\")\n",
       "             83 |  # node_layer_norm_2\n",
       "                   %\"layer_norm_2\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_272\", %\"encoder.layer.0.output.LayerNorm.weight\"{...}, %\"encoder.layer.0.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "             84 |  # node_MatMul_114\n",
       "                   %\"val_120\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_119\"{...})\n",
       "             85 |  # node_linear_6\n",
       "                   %\"linear_6\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_120\", %\"encoder.layer.1.attention.attn.q.bias\"{...})\n",
       "             86 |  # node_view_4\n",
       "                   %\"view_4\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_6\", %\"val_79\") {allowzero=1}\n",
       "             87 |  # node_transpose_4\n",
       "                   %\"transpose_4\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
       "             88 |  # node_MatMul_122\n",
       "                   %\"val_128\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_127\"{...})\n",
       "             89 |  # node_linear_7\n",
       "                   %\"linear_7\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_128\", %\"encoder.layer.1.attention.attn.k.bias\"{...})\n",
       "             90 |  # node_view_5\n",
       "                   %\"view_5\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_7\", %\"val_79\") {allowzero=1}\n",
       "             91 |  # node_MatMul_130\n",
       "                   %\"val_136\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_135\"{...})\n",
       "             92 |  # node_linear_8\n",
       "                   %\"linear_8\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_136\", %\"encoder.layer.1.attention.attn.v.bias\"{...})\n",
       "             93 |  # node_view_6\n",
       "                   %\"view_6\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_79\") {allowzero=1}\n",
       "             94 |  # node_transpose_6\n",
       "                   %\"transpose_6\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_6\") {perm=(0, 2, 1, 3)}\n",
       "             95 |  # node_Transpose_41\n",
       "                   %\"transpose_7\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_5\") {perm=(0, 2, 3, 1)}\n",
       "             96 |  # node_matmul_2\n",
       "                   %\"matmul_2\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_4\", %\"transpose_7\")\n",
       "             97 |  # node_div_3\n",
       "                   %\"div_3\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_2\", %\"scalar_tensor_default\"{8.0})\n",
       "             98 |  # node_add_328\n",
       "                   %\"add_328\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_3\", %\"expand\")\n",
       "             99 |  # node_add_334\n",
       "                   %\"add_334\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_328\", %\"mul_22\")\n",
       "            100 |  # node_softmax_1\n",
       "                   %\"softmax_1\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_334\") {axis=-1}\n",
       "            101 |  # node_matmul_3\n",
       "                   %\"matmul_3\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_1\", %\"transpose_6\")\n",
       "            102 |  # node_permute_2\n",
       "                   %\"permute_2\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_3\") {perm=(0, 2, 1, 3)}\n",
       "            103 |  # node_view_7\n",
       "                   %\"view_7\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_2\", %\"val_101\") {allowzero=1}\n",
       "            104 |  # node_MatMul_143\n",
       "                   %\"val_149\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_7\", %\"val_148\"{...})\n",
       "            105 |  # node_linear_9\n",
       "                   %\"linear_9\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_149\", %\"encoder.layer.1.attention.attn.o.bias\"{...})\n",
       "            106 |  # node_add_368\n",
       "                   %\"add_368\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_9\", %\"layer_norm_2\")\n",
       "            107 |  # node_layer_norm_3\n",
       "                   %\"layer_norm_3\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_368\", %\"encoder.layer.1.attention.LayerNorm.weight\"{...}, %\"encoder.layer.1.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            108 |  # node_MatMul_145\n",
       "                   %\"val_153\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_152\"{...})\n",
       "            109 |  # node_linear_10\n",
       "                   %\"linear_10\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_153\", %\"encoder.layer.1.intermediate.dense.bias\"{...})\n",
       "            110 |  # node_Div_147\n",
       "                   %\"val_155\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_10\", %\"val_108\"{1.4142135381698608})\n",
       "            111 |  # node_Erf_148\n",
       "                   %\"val_156\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_155\")\n",
       "            112 |  # node_Add_150\n",
       "                   %\"val_158\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_156\", %\"val_27\"{1.0})\n",
       "            113 |  # node_Mul_152\n",
       "                   %\"val_160\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_158\")\n",
       "            114 |  # node_gelu_1\n",
       "                   %\"gelu_1\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_10\", %\"val_160\")\n",
       "            115 |  # node_MatMul_154\n",
       "                   %\"val_162\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_161\"{...})\n",
       "            116 |  # node_linear_11\n",
       "                   %\"linear_11\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_162\", %\"encoder.layer.1.output.dense.bias\"{...})\n",
       "            117 |  # node_add_387\n",
       "                   %\"add_387\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_11\", %\"layer_norm_3\")\n",
       "            118 |  # node_layer_norm_4\n",
       "                   %\"layer_norm_4\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_387\", %\"encoder.layer.1.output.LayerNorm.weight\"{...}, %\"encoder.layer.1.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            119 |  # node_MatMul_156\n",
       "                   %\"val_166\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_165\"{...})\n",
       "            120 |  # node_linear_12\n",
       "                   %\"linear_12\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_166\", %\"encoder.layer.2.attention.attn.q.bias\"{...})\n",
       "            121 |  # node_view_8\n",
       "                   %\"view_8\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_79\") {allowzero=1}\n",
       "            122 |  # node_transpose_8\n",
       "                   %\"transpose_8\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
       "            123 |  # node_MatMul_164\n",
       "                   %\"val_174\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_173\"{...})\n",
       "            124 |  # node_linear_13\n",
       "                   %\"linear_13\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_174\", %\"encoder.layer.2.attention.attn.k.bias\"{...})\n",
       "            125 |  # node_view_9\n",
       "                   %\"view_9\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_13\", %\"val_79\") {allowzero=1}\n",
       "            126 |  # node_MatMul_172\n",
       "                   %\"val_182\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_181\"{...})\n",
       "            127 |  # node_linear_14\n",
       "                   %\"linear_14\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_182\", %\"encoder.layer.2.attention.attn.v.bias\"{...})\n",
       "            128 |  # node_view_10\n",
       "                   %\"view_10\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_14\", %\"val_79\") {allowzero=1}\n",
       "            129 |  # node_transpose_10\n",
       "                   %\"transpose_10\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
       "            130 |  # node_Transpose_47\n",
       "                   %\"transpose_11\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_9\") {perm=(0, 2, 3, 1)}\n",
       "            131 |  # node_matmul_4\n",
       "                   %\"matmul_4\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_8\", %\"transpose_11\")\n",
       "            132 |  # node_div_4\n",
       "                   %\"div_4\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_4\", %\"scalar_tensor_default\"{8.0})\n",
       "            133 |  # node_add_443\n",
       "                   %\"add_443\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_4\", %\"expand\")\n",
       "            134 |  # node_add_449\n",
       "                   %\"add_449\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_443\", %\"mul_22\")\n",
       "            135 |  # node_softmax_2\n",
       "                   %\"softmax_2\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_449\") {axis=-1}\n",
       "            136 |  # node_matmul_5\n",
       "                   %\"matmul_5\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_2\", %\"transpose_10\")\n",
       "            137 |  # node_permute_3\n",
       "                   %\"permute_3\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_5\") {perm=(0, 2, 1, 3)}\n",
       "            138 |  # node_view_11\n",
       "                   %\"view_11\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_3\", %\"val_101\") {allowzero=1}\n",
       "            139 |  # node_MatMul_185\n",
       "                   %\"val_195\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_11\", %\"val_194\"{...})\n",
       "            140 |  # node_linear_15\n",
       "                   %\"linear_15\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_195\", %\"encoder.layer.2.attention.attn.o.bias\"{...})\n",
       "            141 |  # node_add_483\n",
       "                   %\"add_483\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_15\", %\"layer_norm_4\")\n",
       "            142 |  # node_layer_norm_5\n",
       "                   %\"layer_norm_5\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_483\", %\"encoder.layer.2.attention.LayerNorm.weight\"{...}, %\"encoder.layer.2.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            143 |  # node_MatMul_187\n",
       "                   %\"val_199\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_198\"{...})\n",
       "            144 |  # node_linear_16\n",
       "                   %\"linear_16\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_199\", %\"encoder.layer.2.intermediate.dense.bias\"{...})\n",
       "            145 |  # node_Div_189\n",
       "                   %\"val_201\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_16\", %\"val_108\"{1.4142135381698608})\n",
       "            146 |  # node_Erf_190\n",
       "                   %\"val_202\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_201\")\n",
       "            147 |  # node_Add_192\n",
       "                   %\"val_204\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_202\", %\"val_27\"{1.0})\n",
       "            148 |  # node_Mul_194\n",
       "                   %\"val_206\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_204\")\n",
       "            149 |  # node_gelu_2\n",
       "                   %\"gelu_2\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_16\", %\"val_206\")\n",
       "            150 |  # node_MatMul_196\n",
       "                   %\"val_208\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_207\"{...})\n",
       "            151 |  # node_linear_17\n",
       "                   %\"linear_17\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_208\", %\"encoder.layer.2.output.dense.bias\"{...})\n",
       "            152 |  # node_add_502\n",
       "                   %\"add_502\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_17\", %\"layer_norm_5\")\n",
       "            153 |  # node_layer_norm_6\n",
       "                   %\"layer_norm_6\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_502\", %\"encoder.layer.2.output.LayerNorm.weight\"{...}, %\"encoder.layer.2.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            154 |  # node_MatMul_198\n",
       "                   %\"val_212\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_211\"{...})\n",
       "            155 |  # node_linear_18\n",
       "                   %\"linear_18\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_212\", %\"encoder.layer.3.attention.attn.q.bias\"{...})\n",
       "            156 |  # node_view_12\n",
       "                   %\"view_12\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_18\", %\"val_79\") {allowzero=1}\n",
       "            157 |  # node_transpose_12\n",
       "                   %\"transpose_12\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_12\") {perm=(0, 2, 1, 3)}\n",
       "            158 |  # node_MatMul_206\n",
       "                   %\"val_220\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_219\"{...})\n",
       "            159 |  # node_linear_19\n",
       "                   %\"linear_19\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_220\", %\"encoder.layer.3.attention.attn.k.bias\"{...})\n",
       "            160 |  # node_view_13\n",
       "                   %\"view_13\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_19\", %\"val_79\") {allowzero=1}\n",
       "            161 |  # node_MatMul_214\n",
       "                   %\"val_228\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_227\"{...})\n",
       "            162 |  # node_linear_20\n",
       "                   %\"linear_20\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_228\", %\"encoder.layer.3.attention.attn.v.bias\"{...})\n",
       "            163 |  # node_view_14\n",
       "                   %\"view_14\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_79\") {allowzero=1}\n",
       "            164 |  # node_transpose_14\n",
       "                   %\"transpose_14\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_14\") {perm=(0, 2, 1, 3)}\n",
       "            165 |  # node_Transpose_53\n",
       "                   %\"transpose_15\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_13\") {perm=(0, 2, 3, 1)}\n",
       "            166 |  # node_matmul_6\n",
       "                   %\"matmul_6\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_12\", %\"transpose_15\")\n",
       "            167 |  # node_div_5\n",
       "                   %\"div_5\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_6\", %\"scalar_tensor_default\"{8.0})\n",
       "            168 |  # node_add_558\n",
       "                   %\"add_558\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_5\", %\"expand\")\n",
       "            169 |  # node_add_564\n",
       "                   %\"add_564\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_558\", %\"mul_22\")\n",
       "            170 |  # node_softmax_3\n",
       "                   %\"softmax_3\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_564\") {axis=-1}\n",
       "            171 |  # node_matmul_7\n",
       "                   %\"matmul_7\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_3\", %\"transpose_14\")\n",
       "            172 |  # node_permute_4\n",
       "                   %\"permute_4\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_7\") {perm=(0, 2, 1, 3)}\n",
       "            173 |  # node_view_15\n",
       "                   %\"view_15\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_4\", %\"val_101\") {allowzero=1}\n",
       "            174 |  # node_MatMul_227\n",
       "                   %\"val_241\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_15\", %\"val_240\"{...})\n",
       "            175 |  # node_linear_21\n",
       "                   %\"linear_21\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_241\", %\"encoder.layer.3.attention.attn.o.bias\"{...})\n",
       "            176 |  # node_add_598\n",
       "                   %\"add_598\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_21\", %\"layer_norm_6\")\n",
       "            177 |  # node_layer_norm_7\n",
       "                   %\"layer_norm_7\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_598\", %\"encoder.layer.3.attention.LayerNorm.weight\"{...}, %\"encoder.layer.3.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            178 |  # node_MatMul_229\n",
       "                   %\"val_245\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_244\"{...})\n",
       "            179 |  # node_linear_22\n",
       "                   %\"linear_22\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_245\", %\"encoder.layer.3.intermediate.dense.bias\"{...})\n",
       "            180 |  # node_Div_231\n",
       "                   %\"val_247\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_22\", %\"val_108\"{1.4142135381698608})\n",
       "            181 |  # node_Erf_232\n",
       "                   %\"val_248\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_247\")\n",
       "            182 |  # node_Add_234\n",
       "                   %\"val_250\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_248\", %\"val_27\"{1.0})\n",
       "            183 |  # node_Mul_236\n",
       "                   %\"val_252\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_250\")\n",
       "            184 |  # node_gelu_3\n",
       "                   %\"gelu_3\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_22\", %\"val_252\")\n",
       "            185 |  # node_MatMul_238\n",
       "                   %\"val_254\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_253\"{...})\n",
       "            186 |  # node_linear_23\n",
       "                   %\"linear_23\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_254\", %\"encoder.layer.3.output.dense.bias\"{...})\n",
       "            187 |  # node_add_617\n",
       "                   %\"add_617\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_23\", %\"layer_norm_7\")\n",
       "            188 |  # node_layer_norm_8\n",
       "                   %\"layer_norm_8\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_617\", %\"encoder.layer.3.output.LayerNorm.weight\"{...}, %\"encoder.layer.3.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            189 |  # node_MatMul_240\n",
       "                   %\"val_258\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_257\"{...})\n",
       "            190 |  # node_linear_24\n",
       "                   %\"linear_24\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_258\", %\"encoder.layer.4.attention.attn.q.bias\"{...})\n",
       "            191 |  # node_view_16\n",
       "                   %\"view_16\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_79\") {allowzero=1}\n",
       "            192 |  # node_transpose_16\n",
       "                   %\"transpose_16\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_16\") {perm=(0, 2, 1, 3)}\n",
       "            193 |  # node_MatMul_248\n",
       "                   %\"val_266\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_265\"{...})\n",
       "            194 |  # node_linear_25\n",
       "                   %\"linear_25\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_266\", %\"encoder.layer.4.attention.attn.k.bias\"{...})\n",
       "            195 |  # node_view_17\n",
       "                   %\"view_17\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_25\", %\"val_79\") {allowzero=1}\n",
       "            196 |  # node_MatMul_256\n",
       "                   %\"val_274\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_273\"{...})\n",
       "            197 |  # node_linear_26\n",
       "                   %\"linear_26\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_274\", %\"encoder.layer.4.attention.attn.v.bias\"{...})\n",
       "            198 |  # node_view_18\n",
       "                   %\"view_18\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_26\", %\"val_79\") {allowzero=1}\n",
       "            199 |  # node_transpose_18\n",
       "                   %\"transpose_18\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
       "            200 |  # node_Transpose_59\n",
       "                   %\"transpose_19\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_17\") {perm=(0, 2, 3, 1)}\n",
       "            201 |  # node_matmul_8\n",
       "                   %\"matmul_8\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_16\", %\"transpose_19\")\n",
       "            202 |  # node_div_6\n",
       "                   %\"div_6\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_8\", %\"scalar_tensor_default\"{8.0})\n",
       "            203 |  # node_add_673\n",
       "                   %\"add_673\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_6\", %\"expand\")\n",
       "            204 |  # node_add_679\n",
       "                   %\"add_679\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_673\", %\"mul_22\")\n",
       "            205 |  # node_softmax_4\n",
       "                   %\"softmax_4\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_679\") {axis=-1}\n",
       "            206 |  # node_matmul_9\n",
       "                   %\"matmul_9\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_4\", %\"transpose_18\")\n",
       "            207 |  # node_permute_5\n",
       "                   %\"permute_5\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_9\") {perm=(0, 2, 1, 3)}\n",
       "            208 |  # node_view_19\n",
       "                   %\"view_19\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_5\", %\"val_101\") {allowzero=1}\n",
       "            209 |  # node_MatMul_269\n",
       "                   %\"val_287\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_19\", %\"val_286\"{...})\n",
       "            210 |  # node_linear_27\n",
       "                   %\"linear_27\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_287\", %\"encoder.layer.4.attention.attn.o.bias\"{...})\n",
       "            211 |  # node_add_713\n",
       "                   %\"add_713\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_27\", %\"layer_norm_8\")\n",
       "            212 |  # node_layer_norm_9\n",
       "                   %\"layer_norm_9\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_713\", %\"encoder.layer.4.attention.LayerNorm.weight\"{...}, %\"encoder.layer.4.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            213 |  # node_MatMul_271\n",
       "                   %\"val_291\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_290\"{...})\n",
       "            214 |  # node_linear_28\n",
       "                   %\"linear_28\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_291\", %\"encoder.layer.4.intermediate.dense.bias\"{...})\n",
       "            215 |  # node_Div_273\n",
       "                   %\"val_293\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_28\", %\"val_108\"{1.4142135381698608})\n",
       "            216 |  # node_Erf_274\n",
       "                   %\"val_294\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_293\")\n",
       "            217 |  # node_Add_276\n",
       "                   %\"val_296\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_294\", %\"val_27\"{1.0})\n",
       "            218 |  # node_Mul_278\n",
       "                   %\"val_298\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_296\")\n",
       "            219 |  # node_gelu_4\n",
       "                   %\"gelu_4\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_28\", %\"val_298\")\n",
       "            220 |  # node_MatMul_280\n",
       "                   %\"val_300\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_299\"{...})\n",
       "            221 |  # node_linear_29\n",
       "                   %\"linear_29\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_300\", %\"encoder.layer.4.output.dense.bias\"{...})\n",
       "            222 |  # node_add_732\n",
       "                   %\"add_732\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_29\", %\"layer_norm_9\")\n",
       "            223 |  # node_layer_norm_10\n",
       "                   %\"layer_norm_10\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_732\", %\"encoder.layer.4.output.LayerNorm.weight\"{...}, %\"encoder.layer.4.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            224 |  # node_MatMul_282\n",
       "                   %\"val_304\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_303\"{...})\n",
       "            225 |  # node_linear_30\n",
       "                   %\"linear_30\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_304\", %\"encoder.layer.5.attention.attn.q.bias\"{...})\n",
       "            226 |  # node_view_20\n",
       "                   %\"view_20\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_30\", %\"val_79\") {allowzero=1}\n",
       "            227 |  # node_transpose_20\n",
       "                   %\"transpose_20\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_20\") {perm=(0, 2, 1, 3)}\n",
       "            228 |  # node_MatMul_290\n",
       "                   %\"val_312\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_311\"{...})\n",
       "            229 |  # node_linear_31\n",
       "                   %\"linear_31\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_312\", %\"encoder.layer.5.attention.attn.k.bias\"{...})\n",
       "            230 |  # node_view_21\n",
       "                   %\"view_21\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_31\", %\"val_79\") {allowzero=1}\n",
       "            231 |  # node_MatMul_298\n",
       "                   %\"val_320\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_319\"{...})\n",
       "            232 |  # node_linear_32\n",
       "                   %\"linear_32\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_320\", %\"encoder.layer.5.attention.attn.v.bias\"{...})\n",
       "            233 |  # node_view_22\n",
       "                   %\"view_22\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_79\") {allowzero=1}\n",
       "            234 |  # node_transpose_22\n",
       "                   %\"transpose_22\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
       "            235 |  # node_Transpose_65\n",
       "                   %\"transpose_23\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_21\") {perm=(0, 2, 3, 1)}\n",
       "            236 |  # node_matmul_10\n",
       "                   %\"matmul_10\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_20\", %\"transpose_23\")\n",
       "            237 |  # node_div_7\n",
       "                   %\"div_7\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_10\", %\"scalar_tensor_default\"{8.0})\n",
       "            238 |  # node_add_788\n",
       "                   %\"add_788\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_7\", %\"expand\")\n",
       "            239 |  # node_add_794\n",
       "                   %\"add_794\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_788\", %\"mul_22\")\n",
       "            240 |  # node_softmax_5\n",
       "                   %\"softmax_5\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_794\") {axis=-1}\n",
       "            241 |  # node_matmul_11\n",
       "                   %\"matmul_11\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_5\", %\"transpose_22\")\n",
       "            242 |  # node_permute_6\n",
       "                   %\"permute_6\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_11\") {perm=(0, 2, 1, 3)}\n",
       "            243 |  # node_view_23\n",
       "                   %\"view_23\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_6\", %\"val_101\") {allowzero=1}\n",
       "            244 |  # node_MatMul_311\n",
       "                   %\"val_333\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_23\", %\"val_332\"{...})\n",
       "            245 |  # node_linear_33\n",
       "                   %\"linear_33\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_333\", %\"encoder.layer.5.attention.attn.o.bias\"{...})\n",
       "            246 |  # node_add_828\n",
       "                   %\"add_828\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_33\", %\"layer_norm_10\")\n",
       "            247 |  # node_layer_norm_11\n",
       "                   %\"layer_norm_11\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_828\", %\"encoder.layer.5.attention.LayerNorm.weight\"{...}, %\"encoder.layer.5.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            248 |  # node_MatMul_313\n",
       "                   %\"val_337\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_336\"{...})\n",
       "            249 |  # node_linear_34\n",
       "                   %\"linear_34\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_337\", %\"encoder.layer.5.intermediate.dense.bias\"{...})\n",
       "            250 |  # node_Div_315\n",
       "                   %\"val_339\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_34\", %\"val_108\"{1.4142135381698608})\n",
       "            251 |  # node_Erf_316\n",
       "                   %\"val_340\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_339\")\n",
       "            252 |  # node_Add_318\n",
       "                   %\"val_342\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_340\", %\"val_27\"{1.0})\n",
       "            253 |  # node_Mul_320\n",
       "                   %\"val_344\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_342\")\n",
       "            254 |  # node_gelu_5\n",
       "                   %\"gelu_5\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_34\", %\"val_344\")\n",
       "            255 |  # node_MatMul_322\n",
       "                   %\"val_346\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_345\"{...})\n",
       "            256 |  # node_linear_35\n",
       "                   %\"linear_35\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_346\", %\"encoder.layer.5.output.dense.bias\"{...})\n",
       "            257 |  # node_add_847\n",
       "                   %\"add_847\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_35\", %\"layer_norm_11\")\n",
       "            258 |  # node_layer_norm_12\n",
       "                   %\"layer_norm_12\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_847\", %\"encoder.layer.5.output.LayerNorm.weight\"{...}, %\"encoder.layer.5.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            259 |  # node_MatMul_324\n",
       "                   %\"val_350\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_349\"{...})\n",
       "            260 |  # node_linear_36\n",
       "                   %\"linear_36\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_350\", %\"encoder.layer.6.attention.attn.q.bias\"{...})\n",
       "            261 |  # node_view_24\n",
       "                   %\"view_24\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_36\", %\"val_79\") {allowzero=1}\n",
       "            262 |  # node_transpose_24\n",
       "                   %\"transpose_24\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_24\") {perm=(0, 2, 1, 3)}\n",
       "            263 |  # node_MatMul_332\n",
       "                   %\"val_358\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_357\"{...})\n",
       "            264 |  # node_linear_37\n",
       "                   %\"linear_37\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_358\", %\"encoder.layer.6.attention.attn.k.bias\"{...})\n",
       "            265 |  # node_view_25\n",
       "                   %\"view_25\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_37\", %\"val_79\") {allowzero=1}\n",
       "            266 |  # node_MatMul_340\n",
       "                   %\"val_366\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_365\"{...})\n",
       "            267 |  # node_linear_38\n",
       "                   %\"linear_38\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_366\", %\"encoder.layer.6.attention.attn.v.bias\"{...})\n",
       "            268 |  # node_view_26\n",
       "                   %\"view_26\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_38\", %\"val_79\") {allowzero=1}\n",
       "            269 |  # node_transpose_26\n",
       "                   %\"transpose_26\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_26\") {perm=(0, 2, 1, 3)}\n",
       "            270 |  # node_Transpose_71\n",
       "                   %\"transpose_27\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_25\") {perm=(0, 2, 3, 1)}\n",
       "            271 |  # node_matmul_12\n",
       "                   %\"matmul_12\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_24\", %\"transpose_27\")\n",
       "            272 |  # node_div_8\n",
       "                   %\"div_8\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_12\", %\"scalar_tensor_default\"{8.0})\n",
       "            273 |  # node_add_903\n",
       "                   %\"add_903\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_8\", %\"expand\")\n",
       "            274 |  # node_add_909\n",
       "                   %\"add_909\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_903\", %\"mul_22\")\n",
       "            275 |  # node_softmax_6\n",
       "                   %\"softmax_6\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_909\") {axis=-1}\n",
       "            276 |  # node_matmul_13\n",
       "                   %\"matmul_13\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_6\", %\"transpose_26\")\n",
       "            277 |  # node_permute_7\n",
       "                   %\"permute_7\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_13\") {perm=(0, 2, 1, 3)}\n",
       "            278 |  # node_view_27\n",
       "                   %\"view_27\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_7\", %\"val_101\") {allowzero=1}\n",
       "            279 |  # node_MatMul_353\n",
       "                   %\"val_379\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_27\", %\"val_378\"{...})\n",
       "            280 |  # node_linear_39\n",
       "                   %\"linear_39\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_379\", %\"encoder.layer.6.attention.attn.o.bias\"{...})\n",
       "            281 |  # node_add_943\n",
       "                   %\"add_943\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_39\", %\"layer_norm_12\")\n",
       "            282 |  # node_layer_norm_13\n",
       "                   %\"layer_norm_13\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_943\", %\"encoder.layer.6.attention.LayerNorm.weight\"{...}, %\"encoder.layer.6.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            283 |  # node_MatMul_355\n",
       "                   %\"val_383\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_382\"{...})\n",
       "            284 |  # node_linear_40\n",
       "                   %\"linear_40\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_383\", %\"encoder.layer.6.intermediate.dense.bias\"{...})\n",
       "            285 |  # node_Div_357\n",
       "                   %\"val_385\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_40\", %\"val_108\"{1.4142135381698608})\n",
       "            286 |  # node_Erf_358\n",
       "                   %\"val_386\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_385\")\n",
       "            287 |  # node_Add_360\n",
       "                   %\"val_388\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_386\", %\"val_27\"{1.0})\n",
       "            288 |  # node_Mul_362\n",
       "                   %\"val_390\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_388\")\n",
       "            289 |  # node_gelu_6\n",
       "                   %\"gelu_6\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_40\", %\"val_390\")\n",
       "            290 |  # node_MatMul_364\n",
       "                   %\"val_392\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_391\"{...})\n",
       "            291 |  # node_linear_41\n",
       "                   %\"linear_41\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_392\", %\"encoder.layer.6.output.dense.bias\"{...})\n",
       "            292 |  # node_add_962\n",
       "                   %\"add_962\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_41\", %\"layer_norm_13\")\n",
       "            293 |  # node_layer_norm_14\n",
       "                   %\"layer_norm_14\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_962\", %\"encoder.layer.6.output.LayerNorm.weight\"{...}, %\"encoder.layer.6.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            294 |  # node_MatMul_366\n",
       "                   %\"val_396\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_395\"{...})\n",
       "            295 |  # node_linear_42\n",
       "                   %\"linear_42\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_396\", %\"encoder.layer.7.attention.attn.q.bias\"{...})\n",
       "            296 |  # node_view_28\n",
       "                   %\"view_28\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_42\", %\"val_79\") {allowzero=1}\n",
       "            297 |  # node_transpose_28\n",
       "                   %\"transpose_28\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_28\") {perm=(0, 2, 1, 3)}\n",
       "            298 |  # node_MatMul_374\n",
       "                   %\"val_404\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_403\"{...})\n",
       "            299 |  # node_linear_43\n",
       "                   %\"linear_43\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_404\", %\"encoder.layer.7.attention.attn.k.bias\"{...})\n",
       "            300 |  # node_view_29\n",
       "                   %\"view_29\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_43\", %\"val_79\") {allowzero=1}\n",
       "            301 |  # node_MatMul_382\n",
       "                   %\"val_412\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_411\"{...})\n",
       "            302 |  # node_linear_44\n",
       "                   %\"linear_44\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_412\", %\"encoder.layer.7.attention.attn.v.bias\"{...})\n",
       "            303 |  # node_view_30\n",
       "                   %\"view_30\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_79\") {allowzero=1}\n",
       "            304 |  # node_transpose_30\n",
       "                   %\"transpose_30\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_30\") {perm=(0, 2, 1, 3)}\n",
       "            305 |  # node_Transpose_77\n",
       "                   %\"transpose_31\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_29\") {perm=(0, 2, 3, 1)}\n",
       "            306 |  # node_matmul_14\n",
       "                   %\"matmul_14\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_28\", %\"transpose_31\")\n",
       "            307 |  # node_div_9\n",
       "                   %\"div_9\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_14\", %\"scalar_tensor_default\"{8.0})\n",
       "            308 |  # node_add_1018\n",
       "                   %\"add_1018\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_9\", %\"expand\")\n",
       "            309 |  # node_add_1024\n",
       "                   %\"add_1024\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1018\", %\"mul_22\")\n",
       "            310 |  # node_softmax_7\n",
       "                   %\"softmax_7\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1024\") {axis=-1}\n",
       "            311 |  # node_matmul_15\n",
       "                   %\"matmul_15\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_7\", %\"transpose_30\")\n",
       "            312 |  # node_permute_8\n",
       "                   %\"permute_8\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_15\") {perm=(0, 2, 1, 3)}\n",
       "            313 |  # node_view_31\n",
       "                   %\"view_31\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_8\", %\"val_101\") {allowzero=1}\n",
       "            314 |  # node_MatMul_395\n",
       "                   %\"val_425\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_31\", %\"val_424\"{...})\n",
       "            315 |  # node_linear_45\n",
       "                   %\"linear_45\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_425\", %\"encoder.layer.7.attention.attn.o.bias\"{...})\n",
       "            316 |  # node_add_1058\n",
       "                   %\"add_1058\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_45\", %\"layer_norm_14\")\n",
       "            317 |  # node_layer_norm_15\n",
       "                   %\"layer_norm_15\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1058\", %\"encoder.layer.7.attention.LayerNorm.weight\"{...}, %\"encoder.layer.7.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            318 |  # node_MatMul_397\n",
       "                   %\"val_429\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_428\"{...})\n",
       "            319 |  # node_linear_46\n",
       "                   %\"linear_46\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_429\", %\"encoder.layer.7.intermediate.dense.bias\"{...})\n",
       "            320 |  # node_Div_399\n",
       "                   %\"val_431\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_46\", %\"val_108\"{1.4142135381698608})\n",
       "            321 |  # node_Erf_400\n",
       "                   %\"val_432\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_431\")\n",
       "            322 |  # node_Add_402\n",
       "                   %\"val_434\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_432\", %\"val_27\"{1.0})\n",
       "            323 |  # node_Mul_404\n",
       "                   %\"val_436\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_434\")\n",
       "            324 |  # node_gelu_7\n",
       "                   %\"gelu_7\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_46\", %\"val_436\")\n",
       "            325 |  # node_MatMul_406\n",
       "                   %\"val_438\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_437\"{...})\n",
       "            326 |  # node_linear_47\n",
       "                   %\"linear_47\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_438\", %\"encoder.layer.7.output.dense.bias\"{...})\n",
       "            327 |  # node_add_1077\n",
       "                   %\"add_1077\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_47\", %\"layer_norm_15\")\n",
       "            328 |  # node_layer_norm_16\n",
       "                   %\"layer_norm_16\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1077\", %\"encoder.layer.7.output.LayerNorm.weight\"{...}, %\"encoder.layer.7.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            329 |  # node_MatMul_408\n",
       "                   %\"val_442\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_441\"{...})\n",
       "            330 |  # node_linear_48\n",
       "                   %\"linear_48\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_442\", %\"encoder.layer.8.attention.attn.q.bias\"{...})\n",
       "            331 |  # node_view_32\n",
       "                   %\"view_32\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_48\", %\"val_79\") {allowzero=1}\n",
       "            332 |  # node_transpose_32\n",
       "                   %\"transpose_32\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_32\") {perm=(0, 2, 1, 3)}\n",
       "            333 |  # node_MatMul_416\n",
       "                   %\"val_450\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_449\"{...})\n",
       "            334 |  # node_linear_49\n",
       "                   %\"linear_49\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_450\", %\"encoder.layer.8.attention.attn.k.bias\"{...})\n",
       "            335 |  # node_view_33\n",
       "                   %\"view_33\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_49\", %\"val_79\") {allowzero=1}\n",
       "            336 |  # node_MatMul_424\n",
       "                   %\"val_458\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_457\"{...})\n",
       "            337 |  # node_linear_50\n",
       "                   %\"linear_50\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_458\", %\"encoder.layer.8.attention.attn.v.bias\"{...})\n",
       "            338 |  # node_view_34\n",
       "                   %\"view_34\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_50\", %\"val_79\") {allowzero=1}\n",
       "            339 |  # node_transpose_34\n",
       "                   %\"transpose_34\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_34\") {perm=(0, 2, 1, 3)}\n",
       "            340 |  # node_Transpose_83\n",
       "                   %\"transpose_35\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_33\") {perm=(0, 2, 3, 1)}\n",
       "            341 |  # node_matmul_16\n",
       "                   %\"matmul_16\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_32\", %\"transpose_35\")\n",
       "            342 |  # node_div_10\n",
       "                   %\"div_10\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_16\", %\"scalar_tensor_default\"{8.0})\n",
       "            343 |  # node_add_1133\n",
       "                   %\"add_1133\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_10\", %\"expand\")\n",
       "            344 |  # node_add_1139\n",
       "                   %\"add_1139\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1133\", %\"mul_22\")\n",
       "            345 |  # node_softmax_8\n",
       "                   %\"softmax_8\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1139\") {axis=-1}\n",
       "            346 |  # node_matmul_17\n",
       "                   %\"matmul_17\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_8\", %\"transpose_34\")\n",
       "            347 |  # node_permute_9\n",
       "                   %\"permute_9\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_17\") {perm=(0, 2, 1, 3)}\n",
       "            348 |  # node_view_35\n",
       "                   %\"view_35\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_9\", %\"val_101\") {allowzero=1}\n",
       "            349 |  # node_MatMul_437\n",
       "                   %\"val_471\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_35\", %\"val_470\"{...})\n",
       "            350 |  # node_linear_51\n",
       "                   %\"linear_51\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_471\", %\"encoder.layer.8.attention.attn.o.bias\"{...})\n",
       "            351 |  # node_add_1173\n",
       "                   %\"add_1173\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_51\", %\"layer_norm_16\")\n",
       "            352 |  # node_layer_norm_17\n",
       "                   %\"layer_norm_17\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1173\", %\"encoder.layer.8.attention.LayerNorm.weight\"{...}, %\"encoder.layer.8.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            353 |  # node_MatMul_439\n",
       "                   %\"val_475\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_474\"{...})\n",
       "            354 |  # node_linear_52\n",
       "                   %\"linear_52\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_475\", %\"encoder.layer.8.intermediate.dense.bias\"{...})\n",
       "            355 |  # node_Div_441\n",
       "                   %\"val_477\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_52\", %\"val_108\"{1.4142135381698608})\n",
       "            356 |  # node_Erf_442\n",
       "                   %\"val_478\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_477\")\n",
       "            357 |  # node_Add_444\n",
       "                   %\"val_480\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_478\", %\"val_27\"{1.0})\n",
       "            358 |  # node_Mul_446\n",
       "                   %\"val_482\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_480\")\n",
       "            359 |  # node_gelu_8\n",
       "                   %\"gelu_8\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_52\", %\"val_482\")\n",
       "            360 |  # node_MatMul_448\n",
       "                   %\"val_484\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_483\"{...})\n",
       "            361 |  # node_linear_53\n",
       "                   %\"linear_53\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_484\", %\"encoder.layer.8.output.dense.bias\"{...})\n",
       "            362 |  # node_add_1192\n",
       "                   %\"add_1192\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_53\", %\"layer_norm_17\")\n",
       "            363 |  # node_layer_norm_18\n",
       "                   %\"layer_norm_18\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1192\", %\"encoder.layer.8.output.LayerNorm.weight\"{...}, %\"encoder.layer.8.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            364 |  # node_MatMul_450\n",
       "                   %\"val_488\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_487\"{...})\n",
       "            365 |  # node_linear_54\n",
       "                   %\"linear_54\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_488\", %\"encoder.layer.9.attention.attn.q.bias\"{...})\n",
       "            366 |  # node_view_36\n",
       "                   %\"view_36\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_54\", %\"val_79\") {allowzero=1}\n",
       "            367 |  # node_transpose_36\n",
       "                   %\"transpose_36\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_36\") {perm=(0, 2, 1, 3)}\n",
       "            368 |  # node_MatMul_458\n",
       "                   %\"val_496\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_495\"{...})\n",
       "            369 |  # node_linear_55\n",
       "                   %\"linear_55\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_496\", %\"encoder.layer.9.attention.attn.k.bias\"{...})\n",
       "            370 |  # node_view_37\n",
       "                   %\"view_37\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_55\", %\"val_79\") {allowzero=1}\n",
       "            371 |  # node_MatMul_466\n",
       "                   %\"val_504\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_503\"{...})\n",
       "            372 |  # node_linear_56\n",
       "                   %\"linear_56\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_504\", %\"encoder.layer.9.attention.attn.v.bias\"{...})\n",
       "            373 |  # node_view_38\n",
       "                   %\"view_38\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_56\", %\"val_79\") {allowzero=1}\n",
       "            374 |  # node_transpose_38\n",
       "                   %\"transpose_38\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_38\") {perm=(0, 2, 1, 3)}\n",
       "            375 |  # node_Transpose_89\n",
       "                   %\"transpose_39\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_37\") {perm=(0, 2, 3, 1)}\n",
       "            376 |  # node_matmul_18\n",
       "                   %\"matmul_18\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_36\", %\"transpose_39\")\n",
       "            377 |  # node_div_11\n",
       "                   %\"div_11\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_18\", %\"scalar_tensor_default\"{8.0})\n",
       "            378 |  # node_add_1248\n",
       "                   %\"add_1248\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_11\", %\"expand\")\n",
       "            379 |  # node_add_1254\n",
       "                   %\"add_1254\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1248\", %\"mul_22\")\n",
       "            380 |  # node_softmax_9\n",
       "                   %\"softmax_9\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1254\") {axis=-1}\n",
       "            381 |  # node_matmul_19\n",
       "                   %\"matmul_19\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_9\", %\"transpose_38\")\n",
       "            382 |  # node_permute_10\n",
       "                   %\"permute_10\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_19\") {perm=(0, 2, 1, 3)}\n",
       "            383 |  # node_view_39\n",
       "                   %\"view_39\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_10\", %\"val_101\") {allowzero=1}\n",
       "            384 |  # node_MatMul_479\n",
       "                   %\"val_517\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_39\", %\"val_516\"{...})\n",
       "            385 |  # node_linear_57\n",
       "                   %\"linear_57\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_517\", %\"encoder.layer.9.attention.attn.o.bias\"{...})\n",
       "            386 |  # node_add_1288\n",
       "                   %\"add_1288\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_57\", %\"layer_norm_18\")\n",
       "            387 |  # node_layer_norm_19\n",
       "                   %\"layer_norm_19\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1288\", %\"encoder.layer.9.attention.LayerNorm.weight\"{...}, %\"encoder.layer.9.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            388 |  # node_MatMul_481\n",
       "                   %\"val_521\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_520\"{...})\n",
       "            389 |  # node_linear_58\n",
       "                   %\"linear_58\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_521\", %\"encoder.layer.9.intermediate.dense.bias\"{...})\n",
       "            390 |  # node_Div_483\n",
       "                   %\"val_523\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_58\", %\"val_108\"{1.4142135381698608})\n",
       "            391 |  # node_Erf_484\n",
       "                   %\"val_524\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_523\")\n",
       "            392 |  # node_Add_486\n",
       "                   %\"val_526\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_524\", %\"val_27\"{1.0})\n",
       "            393 |  # node_Mul_488\n",
       "                   %\"val_528\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_526\")\n",
       "            394 |  # node_gelu_9\n",
       "                   %\"gelu_9\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_58\", %\"val_528\")\n",
       "            395 |  # node_MatMul_490\n",
       "                   %\"val_530\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_529\"{...})\n",
       "            396 |  # node_linear_59\n",
       "                   %\"linear_59\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_530\", %\"encoder.layer.9.output.dense.bias\"{...})\n",
       "            397 |  # node_add_1307\n",
       "                   %\"add_1307\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_59\", %\"layer_norm_19\")\n",
       "            398 |  # node_layer_norm_20\n",
       "                   %\"layer_norm_20\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1307\", %\"encoder.layer.9.output.LayerNorm.weight\"{...}, %\"encoder.layer.9.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            399 |  # node_MatMul_492\n",
       "                   %\"val_534\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_533\"{...})\n",
       "            400 |  # node_linear_60\n",
       "                   %\"linear_60\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_534\", %\"encoder.layer.10.attention.attn.q.bias\"{...})\n",
       "            401 |  # node_view_40\n",
       "                   %\"view_40\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_60\", %\"val_79\") {allowzero=1}\n",
       "            402 |  # node_transpose_40\n",
       "                   %\"transpose_40\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_40\") {perm=(0, 2, 1, 3)}\n",
       "            403 |  # node_MatMul_500\n",
       "                   %\"val_542\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_541\"{...})\n",
       "            404 |  # node_linear_61\n",
       "                   %\"linear_61\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_542\", %\"encoder.layer.10.attention.attn.k.bias\"{...})\n",
       "            405 |  # node_view_41\n",
       "                   %\"view_41\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_61\", %\"val_79\") {allowzero=1}\n",
       "            406 |  # node_MatMul_508\n",
       "                   %\"val_550\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_549\"{...})\n",
       "            407 |  # node_linear_62\n",
       "                   %\"linear_62\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_550\", %\"encoder.layer.10.attention.attn.v.bias\"{...})\n",
       "            408 |  # node_view_42\n",
       "                   %\"view_42\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_62\", %\"val_79\") {allowzero=1}\n",
       "            409 |  # node_transpose_42\n",
       "                   %\"transpose_42\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_42\") {perm=(0, 2, 1, 3)}\n",
       "            410 |  # node_Transpose_95\n",
       "                   %\"transpose_43\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_41\") {perm=(0, 2, 3, 1)}\n",
       "            411 |  # node_matmul_20\n",
       "                   %\"matmul_20\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_40\", %\"transpose_43\")\n",
       "            412 |  # node_div_12\n",
       "                   %\"div_12\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_20\", %\"scalar_tensor_default\"{8.0})\n",
       "            413 |  # node_add_1363\n",
       "                   %\"add_1363\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_12\", %\"expand\")\n",
       "            414 |  # node_add_1369\n",
       "                   %\"add_1369\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1363\", %\"mul_22\")\n",
       "            415 |  # node_softmax_10\n",
       "                   %\"softmax_10\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1369\") {axis=-1}\n",
       "            416 |  # node_matmul_21\n",
       "                   %\"matmul_21\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_10\", %\"transpose_42\")\n",
       "            417 |  # node_permute_11\n",
       "                   %\"permute_11\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_21\") {perm=(0, 2, 1, 3)}\n",
       "            418 |  # node_view_43\n",
       "                   %\"view_43\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_11\", %\"val_101\") {allowzero=1}\n",
       "            419 |  # node_MatMul_521\n",
       "                   %\"val_563\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_43\", %\"val_562\"{...})\n",
       "            420 |  # node_linear_63\n",
       "                   %\"linear_63\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_563\", %\"encoder.layer.10.attention.attn.o.bias\"{...})\n",
       "            421 |  # node_add_1403\n",
       "                   %\"add_1403\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_63\", %\"layer_norm_20\")\n",
       "            422 |  # node_layer_norm_21\n",
       "                   %\"layer_norm_21\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1403\", %\"encoder.layer.10.attention.LayerNorm.weight\"{...}, %\"encoder.layer.10.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            423 |  # node_MatMul_523\n",
       "                   %\"val_567\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_566\"{...})\n",
       "            424 |  # node_linear_64\n",
       "                   %\"linear_64\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_567\", %\"encoder.layer.10.intermediate.dense.bias\"{...})\n",
       "            425 |  # node_Div_525\n",
       "                   %\"val_569\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_64\", %\"val_108\"{1.4142135381698608})\n",
       "            426 |  # node_Erf_526\n",
       "                   %\"val_570\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_569\")\n",
       "            427 |  # node_Add_528\n",
       "                   %\"val_572\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_570\", %\"val_27\"{1.0})\n",
       "            428 |  # node_Mul_530\n",
       "                   %\"val_574\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_572\")\n",
       "            429 |  # node_gelu_10\n",
       "                   %\"gelu_10\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_64\", %\"val_574\")\n",
       "            430 |  # node_MatMul_532\n",
       "                   %\"val_576\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_575\"{...})\n",
       "            431 |  # node_linear_65\n",
       "                   %\"linear_65\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_576\", %\"encoder.layer.10.output.dense.bias\"{...})\n",
       "            432 |  # node_add_1422\n",
       "                   %\"add_1422\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_65\", %\"layer_norm_21\")\n",
       "            433 |  # node_layer_norm_22\n",
       "                   %\"layer_norm_22\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1422\", %\"encoder.layer.10.output.LayerNorm.weight\"{...}, %\"encoder.layer.10.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            434 |  # node_MatMul_534\n",
       "                   %\"val_580\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_579\"{...})\n",
       "            435 |  # node_linear_66\n",
       "                   %\"linear_66\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_580\", %\"encoder.layer.11.attention.attn.q.bias\"{...})\n",
       "            436 |  # node_view_44\n",
       "                   %\"view_44\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_66\", %\"val_79\") {allowzero=1}\n",
       "            437 |  # node_transpose_44\n",
       "                   %\"transpose_44\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_44\") {perm=(0, 2, 1, 3)}\n",
       "            438 |  # node_MatMul_542\n",
       "                   %\"val_588\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_587\"{...})\n",
       "            439 |  # node_linear_67\n",
       "                   %\"linear_67\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_588\", %\"encoder.layer.11.attention.attn.k.bias\"{...})\n",
       "            440 |  # node_view_45\n",
       "                   %\"view_45\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_67\", %\"val_79\") {allowzero=1}\n",
       "            441 |  # node_MatMul_550\n",
       "                   %\"val_596\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_595\"{...})\n",
       "            442 |  # node_linear_68\n",
       "                   %\"linear_68\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_596\", %\"encoder.layer.11.attention.attn.v.bias\"{...})\n",
       "            443 |  # node_view_46\n",
       "                   %\"view_46\"<FLOAT,[1,s53,12,64]> ⬅️ ::Reshape(%\"linear_68\", %\"val_79\") {allowzero=1}\n",
       "            444 |  # node_transpose_46\n",
       "                   %\"transpose_46\"<FLOAT,[1,12,s53,64]> ⬅️ ::Transpose(%\"view_46\") {perm=(0, 2, 1, 3)}\n",
       "            445 |  # node_Transpose_101\n",
       "                   %\"transpose_47\"<FLOAT,[1,12,64,s53]> ⬅️ ::Transpose(%\"view_45\") {perm=(0, 2, 3, 1)}\n",
       "            446 |  # node_matmul_22\n",
       "                   %\"matmul_22\"<FLOAT,[1,12,s53,s53]> ⬅️ ::MatMul(%\"transpose_44\", %\"transpose_47\")\n",
       "            447 |  # node_div_13\n",
       "                   %\"div_13\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Div(%\"matmul_22\", %\"scalar_tensor_default\"{8.0})\n",
       "            448 |  # node_add_1478\n",
       "                   %\"add_1478\"<FLOAT,[s43,12,s53,s53]> ⬅️ ::Add(%\"div_13\", %\"expand\")\n",
       "            449 |  # node_add_1484\n",
       "                   %\"add_1484\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Add(%\"add_1478\", %\"mul_22\")\n",
       "            450 |  # node_softmax_11\n",
       "                   %\"softmax_11\"<FLOAT,[1,12,s53,s53]> ⬅️ ::Softmax(%\"add_1484\") {axis=-1}\n",
       "            451 |  # node_matmul_23\n",
       "                   %\"matmul_23\"<FLOAT,[1,12,s53,64]> ⬅️ ::MatMul(%\"softmax_11\", %\"transpose_46\")\n",
       "            452 |  # node_permute_12\n",
       "                   %\"permute_12\"<FLOAT,[1,s53,12,64]> ⬅️ ::Transpose(%\"matmul_23\") {perm=(0, 2, 1, 3)}\n",
       "            453 |  # node_view_47\n",
       "                   %\"view_47\"<FLOAT,[1,s53,768]> ⬅️ ::Reshape(%\"permute_12\", %\"val_101\") {allowzero=1}\n",
       "            454 |  # node_MatMul_563\n",
       "                   %\"val_609\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"view_47\", %\"val_608\"{...})\n",
       "            455 |  # node_linear_69\n",
       "                   %\"linear_69\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_609\", %\"encoder.layer.11.attention.attn.o.bias\"{...})\n",
       "            456 |  # node_add_1518\n",
       "                   %\"add_1518\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_69\", %\"layer_norm_22\")\n",
       "            457 |  # node_layer_norm_23\n",
       "                   %\"layer_norm_23\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1518\", %\"encoder.layer.11.attention.LayerNorm.weight\"{...}, %\"encoder.layer.11.attention.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            458 |  # node_MatMul_565\n",
       "                   %\"val_613\"<FLOAT,[1,s53,3072]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_612\"{...})\n",
       "            459 |  # node_linear_70\n",
       "                   %\"linear_70\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_613\", %\"encoder.layer.11.intermediate.dense.bias\"{...})\n",
       "            460 |  # node_Div_567\n",
       "                   %\"val_615\"<FLOAT,[1,s53,3072]> ⬅️ ::Div(%\"linear_70\", %\"val_108\"{1.4142135381698608})\n",
       "            461 |  # node_Erf_568\n",
       "                   %\"val_616\"<FLOAT,[1,s53,3072]> ⬅️ ::Erf(%\"val_615\")\n",
       "            462 |  # node_Add_570\n",
       "                   %\"val_618\"<FLOAT,[1,s53,3072]> ⬅️ ::Add(%\"val_616\", %\"val_27\"{1.0})\n",
       "            463 |  # node_Mul_572\n",
       "                   %\"val_620\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"val_113\"{0.5}, %\"val_618\")\n",
       "            464 |  # node_gelu_11\n",
       "                   %\"gelu_11\"<FLOAT,[1,s53,3072]> ⬅️ ::Mul(%\"linear_70\", %\"val_620\")\n",
       "            465 |  # node_MatMul_574\n",
       "                   %\"val_622\"<FLOAT,[1,s53,768]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_621\"{...})\n",
       "            466 |  # node_linear_71\n",
       "                   %\"linear_71\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"val_622\", %\"encoder.layer.11.output.dense.bias\"{...})\n",
       "            467 |  # node_add_1537\n",
       "                   %\"add_1537\"<FLOAT,[1,s53,768]> ⬅️ ::Add(%\"linear_71\", %\"layer_norm_23\")\n",
       "            468 |  # node_layer_norm_24\n",
       "                   %\"output\"<FLOAT,[1,s53,768]>, %\"\"<FLOAT,[1,s53,1]>, %\"\"<FLOAT,[1,s53,1]> ⬅️ ::LayerNormalization(%\"add_1537\", %\"encoder.layer.11.output.LayerNorm.weight\"{...}, %\"encoder.layer.11.output.LayerNorm.bias\"{...}) {stash_type=1, epsilon=9.999999747378752e-06, axis=-1}\n",
       "            469 |  # node_select\n",
       "                   %\"select\"<FLOAT,[1,768]> ⬅️ ::Gather(%\"output\", %\"val_34\"{0}) {axis=1}\n",
       "            470 |  # node_Gemm_105\n",
       "                   %\"linear_72\"<FLOAT,[1,768]> ⬅️ ::Gemm(%\"select\", %\"pooler.dense.weight\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            471 |  # node_tanh\n",
       "                   %\"tanh\"<FLOAT,[1,768]> ⬅️ ::Tanh(%\"linear_72\")\n",
       "            return %\"output\"<FLOAT,[1,s53,768]>, %\"tanh\"<FLOAT,[1,768]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_embeddings_word_embeddings_weight: \"f32[30527, 768]\", p_embeddings_position_embeddings_weight: \"f32[514, 768]\", p_embeddings_layernorm_weight: \"f32[768]\", p_embeddings_layernorm_bias: \"f32[768]\", p_encoder_layer_0_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_0_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_0_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_0_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_0_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_0_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_0_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_0_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_0_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_0_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_0_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_0_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_0_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_0_output_dense_bias: \"f32[768]\", p_encoder_layer_0_output_layernorm_weight: \"f32[768]\", p_encoder_layer_0_output_layernorm_bias: \"f32[768]\", p_encoder_layer_1_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_1_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_1_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_1_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_1_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_1_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_1_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_1_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_1_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_1_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_1_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_1_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_1_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_1_output_dense_bias: \"f32[768]\", p_encoder_layer_1_output_layernorm_weight: \"f32[768]\", p_encoder_layer_1_output_layernorm_bias: \"f32[768]\", p_encoder_layer_2_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_2_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_2_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_2_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_2_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_2_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_2_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_2_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_2_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_2_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_2_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_2_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_2_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_2_output_dense_bias: \"f32[768]\", p_encoder_layer_2_output_layernorm_weight: \"f32[768]\", p_encoder_layer_2_output_layernorm_bias: \"f32[768]\", p_encoder_layer_3_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_3_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_3_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_3_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_3_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_3_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_3_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_3_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_3_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_3_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_3_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_3_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_3_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_3_output_dense_bias: \"f32[768]\", p_encoder_layer_3_output_layernorm_weight: \"f32[768]\", p_encoder_layer_3_output_layernorm_bias: \"f32[768]\", p_encoder_layer_4_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_4_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_4_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_4_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_4_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_4_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_4_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_4_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_4_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_4_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_4_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_4_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_4_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_4_output_dense_bias: \"f32[768]\", p_encoder_layer_4_output_layernorm_weight: \"f32[768]\", p_encoder_layer_4_output_layernorm_bias: \"f32[768]\", p_encoder_layer_5_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_5_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_5_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_5_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_5_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_5_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_5_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_5_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_5_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_5_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_5_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_5_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_5_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_5_output_dense_bias: \"f32[768]\", p_encoder_layer_5_output_layernorm_weight: \"f32[768]\", p_encoder_layer_5_output_layernorm_bias: \"f32[768]\", p_encoder_layer_6_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_6_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_6_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_6_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_6_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_6_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_6_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_6_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_6_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_6_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_6_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_6_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_6_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_6_output_dense_bias: \"f32[768]\", p_encoder_layer_6_output_layernorm_weight: \"f32[768]\", p_encoder_layer_6_output_layernorm_bias: \"f32[768]\", p_encoder_layer_7_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_7_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_7_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_7_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_7_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_7_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_7_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_7_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_7_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_7_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_7_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_7_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_7_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_7_output_dense_bias: \"f32[768]\", p_encoder_layer_7_output_layernorm_weight: \"f32[768]\", p_encoder_layer_7_output_layernorm_bias: \"f32[768]\", p_encoder_layer_8_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_8_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_8_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_8_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_8_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_8_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_8_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_8_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_8_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_8_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_8_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_8_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_8_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_8_output_dense_bias: \"f32[768]\", p_encoder_layer_8_output_layernorm_weight: \"f32[768]\", p_encoder_layer_8_output_layernorm_bias: \"f32[768]\", p_encoder_layer_9_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_9_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_9_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_9_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_9_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_9_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_9_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_9_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_9_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_9_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_9_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_9_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_9_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_9_output_dense_bias: \"f32[768]\", p_encoder_layer_9_output_layernorm_weight: \"f32[768]\", p_encoder_layer_9_output_layernorm_bias: \"f32[768]\", p_encoder_layer_10_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_10_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_10_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_10_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_10_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_10_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_10_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_10_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_10_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_10_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_10_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_10_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_10_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_10_output_dense_bias: \"f32[768]\", p_encoder_layer_10_output_layernorm_weight: \"f32[768]\", p_encoder_layer_10_output_layernorm_bias: \"f32[768]\", p_encoder_layer_11_attention_attn_q_weight: \"f32[768, 768]\", p_encoder_layer_11_attention_attn_q_bias: \"f32[768]\", p_encoder_layer_11_attention_attn_k_weight: \"f32[768, 768]\", p_encoder_layer_11_attention_attn_k_bias: \"f32[768]\", p_encoder_layer_11_attention_attn_v_weight: \"f32[768, 768]\", p_encoder_layer_11_attention_attn_v_bias: \"f32[768]\", p_encoder_layer_11_attention_attn_o_weight: \"f32[768, 768]\", p_encoder_layer_11_attention_attn_o_bias: \"f32[768]\", p_encoder_layer_11_attention_layernorm_weight: \"f32[768]\", p_encoder_layer_11_attention_layernorm_bias: \"f32[768]\", p_encoder_layer_11_intermediate_dense_weight: \"f32[3072, 768]\", p_encoder_layer_11_intermediate_dense_bias: \"f32[3072]\", p_encoder_layer_11_output_dense_weight: \"f32[768, 3072]\", p_encoder_layer_11_output_dense_bias: \"f32[768]\", p_encoder_layer_11_output_layernorm_weight: \"f32[768]\", p_encoder_layer_11_output_layernorm_bias: \"f32[768]\", p_encoder_relative_attention_bias_weight: \"f32[32, 12]\", p_pooler_dense_weight: \"f32[768, 768]\", p_pooler_dense_bias: \"f32[768]\", b_embeddings_position_ids: \"i64[1, 514]\", input_ids: \"i64[s43, s53]\", attention_mask: \"i64[s43, s53]\"):\n",
       "                     # \n",
       "                    sym_size_int_85: \"Sym(s43)\" = torch.ops.aten.sym_size.int(attention_mask, 0)\n",
       "                    sym_size_int_86: \"Sym(s53)\" = torch.ops.aten.sym_size.int(attention_mask, 1)\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:482 in forward, code: extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
       "                    slice_1: \"i64[s43, s53]\" = torch.ops.aten.slice.Tensor(attention_mask, 0, 0, 9223372036854775807);  attention_mask = None\n",
       "                    unsqueeze: \"i64[s43, 1, s53]\" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None\n",
       "                    unsqueeze_1: \"i64[s43, 1, 1, s53]\" = torch.ops.aten.unsqueeze.default(unsqueeze, 2);  unsqueeze = None\n",
       "                    slice_2: \"i64[s43, 1, 1, s53]\" = torch.ops.aten.slice.Tensor(unsqueeze_1, 3, 0, 9223372036854775807);  unsqueeze_1 = None\n",
       "                    _to_copy: \"f32[s43, 1, 1, s53]\" = torch.ops.aten._to_copy.default(slice_2, dtype = torch.float32);  slice_2 = None\n",
       "                    sub_10: \"f32[s43, 1, 1, s53]\" = torch.ops.aten.sub.Tensor(1.0, _to_copy);  _to_copy = None\n",
       "                    mul_22: \"f32[1, 1, 1, s53]\" = torch.ops.aten.mul.Tensor(sub_10, -3.4028234663852886e+38);  sub_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:86 in forward, code: position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx)\n",
       "                    ne_3: \"b8[s43, s53]\" = torch.ops.aten.ne.Scalar(input_ids, 1)\n",
       "                    _to_copy_1: \"i32[s43, s53]\" = torch.ops.aten._to_copy.default(ne_3, dtype = torch.int32);  ne_3 = None\n",
       "                    convert_element_type_default: \"i64[s43, s53]\" = torch.ops.prims.convert_element_type.default(_to_copy_1, dtype = torch.int64)\n",
       "                    cumsum: \"i64[1, s53]\" = torch.ops.aten.cumsum.default(convert_element_type_default, 1);  convert_element_type_default = None\n",
       "                    type_as: \"i32[1, s53]\" = torch.ops.aten.type_as.default(cumsum, _to_copy_1);  cumsum = None\n",
       "                    mul_34: \"i32[s43, s53]\" = torch.ops.aten.mul.Tensor(type_as, _to_copy_1);  type_as = _to_copy_1 = None\n",
       "                    _to_copy_2: \"i64[s43, s53]\" = torch.ops.aten._to_copy.default(mul_34, dtype = torch.int64);  mul_34 = None\n",
       "                    add_45: \"i64[1, s53]\" = torch.ops.aten.add.Tensor(_to_copy_2, 1);  _to_copy_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding: \"f32[s43, s53, 768]\" = torch.ops.aten.embedding.default(p_embeddings_word_embeddings_weight, input_ids, 1);  p_embeddings_word_embeddings_weight = input_ids = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding_1: \"f32[1, s53, 768]\" = torch.ops.aten.embedding.default(p_embeddings_position_embeddings_weight, add_45, 1);  p_embeddings_position_embeddings_weight = add_45 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:104 in forward, code: embeddings = inputs_embeds + position_embeddings\n",
       "                    add_55: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_55, [768], p_embeddings_layernorm_weight, p_embeddings_layernorm_bias);  add_55 = p_embeddings_layernorm_weight = p_embeddings_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(layer_norm);  layer_norm = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:331 in forward, code: position_bias = self.compute_position_bias(hidden_states)\n",
       "                    arange: \"i64[s53]\" = torch.ops.aten.arange.default(sym_size_int_86, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
       "                    slice_3: \"i64[s53]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
       "                    unsqueeze_2: \"i64[s53, 1]\" = torch.ops.aten.unsqueeze.default(slice_3, 1);  slice_3 = None\n",
       "                    arange_1: \"i64[s53]\" = torch.ops.aten.arange.default(sym_size_int_86, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
       "                    unsqueeze_3: \"i64[1, s53]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
       "                    slice_4: \"i64[1, s53]\" = torch.ops.aten.slice.Tensor(unsqueeze_3, 1, 0, 9223372036854775807);  unsqueeze_3 = None\n",
       "                    sub_37: \"i64[s53, s53]\" = torch.ops.aten.sub.Tensor(slice_4, unsqueeze_2);  slice_4 = unsqueeze_2 = None\n",
       "                    neg: \"i64[s53, s53]\" = torch.ops.aten.neg.default(sub_37);  sub_37 = None\n",
       "                    lt: \"b8[s53, s53]\" = torch.ops.aten.lt.Scalar(neg, 0)\n",
       "                    _to_copy_3: \"i64[s53, s53]\" = torch.ops.aten._to_copy.default(lt, dtype = torch.int64);  lt = None\n",
       "                    mul_71: \"i64[s53, s53]\" = torch.ops.aten.mul.Tensor(_to_copy_3, 16);  _to_copy_3 = None\n",
       "                    add_93: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(mul_71, 0);  mul_71 = None\n",
       "                    abs_1: \"i64[s53, s53]\" = torch.ops.aten.abs.default(neg);  neg = None\n",
       "                    lt_1: \"b8[s53, s53]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
       "                    _to_copy_4: \"f32[s53, s53]\" = torch.ops.aten._to_copy.default(abs_1, dtype = torch.float32)\n",
       "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(8, dtype = torch.float32)\n",
       "                    div: \"f32[s53, s53]\" = torch.ops.aten.div.Tensor(_to_copy_4, scalar_tensor_default);  _to_copy_4 = scalar_tensor_default = None\n",
       "                    log: \"f32[s53, s53]\" = torch.ops.aten.log.default(div);  div = None\n",
       "                    div_1: \"f32[s53, s53]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
       "                    scalar_tensor_default_1: \"f32[]\" = torch.ops.aten.scalar_tensor.default(8, dtype = torch.float32)\n",
       "                    mul_87: \"f32[s53, s53]\" = torch.ops.aten.mul.Tensor(div_1, scalar_tensor_default_1);  div_1 = scalar_tensor_default_1 = None\n",
       "                    _to_copy_5: \"i64[s53, s53]\" = torch.ops.aten._to_copy.default(mul_87, dtype = torch.int64);  mul_87 = None\n",
       "                    add_121: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(_to_copy_5, 8);  _to_copy_5 = None\n",
       "                    full_like: \"i64[s53, s53]\" = torch.ops.aten.full_like.default(add_121, 15, pin_memory = False)\n",
       "                    minimum: \"i64[s53, s53]\" = torch.ops.aten.minimum.default(add_121, full_like);  add_121 = full_like = None\n",
       "                    where: \"i64[s53, s53]\" = torch.ops.aten.where.self(lt_1, abs_1, minimum);  lt_1 = abs_1 = minimum = None\n",
       "                    add_140: \"i64[s53, s53]\" = torch.ops.aten.add.Tensor(add_93, where);  add_93 = where = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding_2: \"f32[s53, s53, 12]\" = torch.ops.aten.embedding.default(p_encoder_relative_attention_bias_weight, add_140);  p_encoder_relative_attention_bias_weight = add_140 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:331 in forward, code: position_bias = self.compute_position_bias(hidden_states)\n",
       "                    permute: \"f32[12, s53, s53]\" = torch.ops.aten.permute.default(embedding_2, [2, 0, 1]);  embedding_2 = None\n",
       "                    unsqueeze_4: \"f32[1, 12, s53, s53]\" = torch.ops.aten.unsqueeze.default(permute, 0);  permute = None\n",
       "                    expand: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.expand.default(unsqueeze_4, [sym_size_int_85, -1, sym_size_int_86, sym_size_int_86]);  unsqueeze_4 = None\n",
       "                    clone_1: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.clone.default(expand, memory_format = torch.contiguous_format);  expand = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_q_weight, p_encoder_layer_0_attention_attn_q_bias);  p_encoder_layer_0_attention_attn_q_weight = p_encoder_layer_0_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear, [sym_size_int_85, -1, 12, 64]);  linear = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_k_weight, p_encoder_layer_0_attention_attn_k_bias);  p_encoder_layer_0_attention_attn_k_weight = p_encoder_layer_0_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_1: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_1, [sym_size_int_85, -1, 12, 64]);  linear_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_1: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(clone, p_encoder_layer_0_attention_attn_v_weight, p_encoder_layer_0_attention_attn_v_bias);  p_encoder_layer_0_attention_attn_v_weight = p_encoder_layer_0_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_2: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_2, [sym_size_int_85, -1, 12, 64]);  linear_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_2: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_3: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_1, -1, -2);  transpose_1 = None\n",
       "                    matmul: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose, transpose_3);  transpose = transpose_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_2: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul, 8.0);  matmul = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_213: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_2, clone_1);  div_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_219: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_213, mul_22);  add_213 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_219, -1);  add_219 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax);  softmax = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_1: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_2, transpose_2);  clone_2 = transpose_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_1: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_1, [0, 2, 1, 3]);  matmul_1 = None\n",
       "                    clone_3: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_3: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_3, [sym_size_int_85, sym_size_int_86, 768]);  clone_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_3, p_encoder_layer_0_attention_attn_o_weight, p_encoder_layer_0_attention_attn_o_bias);  view_3 = p_encoder_layer_0_attention_attn_o_weight = p_encoder_layer_0_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_253: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_4, clone);  clone_4 = clone = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_253, [768], p_encoder_layer_0_attention_layernorm_weight, p_encoder_layer_0_attention_layernorm_bias);  add_253 = p_encoder_layer_0_attention_layernorm_weight = p_encoder_layer_0_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_1, p_encoder_layer_0_intermediate_dense_weight, p_encoder_layer_0_intermediate_dense_bias);  p_encoder_layer_0_intermediate_dense_weight = p_encoder_layer_0_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu, p_encoder_layer_0_output_dense_weight, p_encoder_layer_0_output_dense_bias);  gelu = p_encoder_layer_0_output_dense_weight = p_encoder_layer_0_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_272: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_5, layer_norm_1);  clone_5 = layer_norm_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_272, [768], p_encoder_layer_0_output_layernorm_weight, p_encoder_layer_0_output_layernorm_bias);  add_272 = p_encoder_layer_0_output_layernorm_weight = p_encoder_layer_0_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_q_weight, p_encoder_layer_1_attention_attn_q_bias);  p_encoder_layer_1_attention_attn_q_weight = p_encoder_layer_1_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_4: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_6, [sym_size_int_85, -1, 12, 64]);  linear_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_4: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_k_weight, p_encoder_layer_1_attention_attn_k_bias);  p_encoder_layer_1_attention_attn_k_weight = p_encoder_layer_1_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_5: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_7, [sym_size_int_85, -1, 12, 64]);  linear_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_5: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_5, 1, 2);  view_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layer_1_attention_attn_v_weight, p_encoder_layer_1_attention_attn_v_bias);  p_encoder_layer_1_attention_attn_v_weight = p_encoder_layer_1_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_6: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_8, [sym_size_int_85, -1, 12, 64]);  linear_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_6: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_7: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_5, -1, -2);  transpose_5 = None\n",
       "                    matmul_2: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_4, transpose_7);  transpose_4 = transpose_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_3: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_2, 8.0);  matmul_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_328: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_3, clone_1);  div_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_334: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_328, mul_22);  add_328 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_1: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_334, -1);  add_334 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_6: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_1);  softmax_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_3: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_6, transpose_6);  clone_6 = transpose_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_2: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_3, [0, 2, 1, 3]);  matmul_3 = None\n",
       "                    clone_7: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_7: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_7, [sym_size_int_85, sym_size_int_86, 768]);  clone_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_7, p_encoder_layer_1_attention_attn_o_weight, p_encoder_layer_1_attention_attn_o_bias);  view_7 = p_encoder_layer_1_attention_attn_o_weight = p_encoder_layer_1_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_8: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_368: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_8, layer_norm_2);  clone_8 = layer_norm_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_368, [768], p_encoder_layer_1_attention_layernorm_weight, p_encoder_layer_1_attention_layernorm_bias);  add_368 = p_encoder_layer_1_attention_layernorm_weight = p_encoder_layer_1_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_3, p_encoder_layer_1_intermediate_dense_weight, p_encoder_layer_1_intermediate_dense_bias);  p_encoder_layer_1_intermediate_dense_weight = p_encoder_layer_1_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_1: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_1, p_encoder_layer_1_output_dense_weight, p_encoder_layer_1_output_dense_bias);  gelu_1 = p_encoder_layer_1_output_dense_weight = p_encoder_layer_1_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_9: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_387: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_9, layer_norm_3);  clone_9 = layer_norm_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_4: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_387, [768], p_encoder_layer_1_output_layernorm_weight, p_encoder_layer_1_output_layernorm_bias);  add_387 = p_encoder_layer_1_output_layernorm_weight = p_encoder_layer_1_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_q_weight, p_encoder_layer_2_attention_attn_q_bias);  p_encoder_layer_2_attention_attn_q_weight = p_encoder_layer_2_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_8: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_12, [sym_size_int_85, -1, 12, 64]);  linear_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_8: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_k_weight, p_encoder_layer_2_attention_attn_k_bias);  p_encoder_layer_2_attention_attn_k_weight = p_encoder_layer_2_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_9: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_13, [sym_size_int_85, -1, 12, 64]);  linear_13 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_9: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_4, p_encoder_layer_2_attention_attn_v_weight, p_encoder_layer_2_attention_attn_v_bias);  p_encoder_layer_2_attention_attn_v_weight = p_encoder_layer_2_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_10: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_14, [sym_size_int_85, -1, 12, 64]);  linear_14 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_10: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_11: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_9, -1, -2);  transpose_9 = None\n",
       "                    matmul_4: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_8, transpose_11);  transpose_8 = transpose_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_4: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_4, 8.0);  matmul_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_443: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_4, clone_1);  div_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_449: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_443, mul_22);  add_443 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_2: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_449, -1);  add_449 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_2);  softmax_2 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_5: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_10, transpose_10);  clone_10 = transpose_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_3: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_5, [0, 2, 1, 3]);  matmul_5 = None\n",
       "                    clone_11: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_11: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_11, [sym_size_int_85, sym_size_int_86, 768]);  clone_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_11, p_encoder_layer_2_attention_attn_o_weight, p_encoder_layer_2_attention_attn_o_bias);  view_11 = p_encoder_layer_2_attention_attn_o_weight = p_encoder_layer_2_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_12: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_483: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_12, layer_norm_4);  clone_12 = layer_norm_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_483, [768], p_encoder_layer_2_attention_layernorm_weight, p_encoder_layer_2_attention_layernorm_bias);  add_483 = p_encoder_layer_2_attention_layernorm_weight = p_encoder_layer_2_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_5, p_encoder_layer_2_intermediate_dense_weight, p_encoder_layer_2_intermediate_dense_bias);  p_encoder_layer_2_intermediate_dense_weight = p_encoder_layer_2_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_2: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_2, p_encoder_layer_2_output_dense_weight, p_encoder_layer_2_output_dense_bias);  gelu_2 = p_encoder_layer_2_output_dense_weight = p_encoder_layer_2_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_13: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_502: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_13, layer_norm_5);  clone_13 = layer_norm_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_502, [768], p_encoder_layer_2_output_layernorm_weight, p_encoder_layer_2_output_layernorm_bias);  add_502 = p_encoder_layer_2_output_layernorm_weight = p_encoder_layer_2_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_q_weight, p_encoder_layer_3_attention_attn_q_bias);  p_encoder_layer_3_attention_attn_q_weight = p_encoder_layer_3_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_12: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_18, [sym_size_int_85, -1, 12, 64]);  linear_18 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_12: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_12, 1, 2);  view_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_k_weight, p_encoder_layer_3_attention_attn_k_bias);  p_encoder_layer_3_attention_attn_k_weight = p_encoder_layer_3_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_13: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_19, [sym_size_int_85, -1, 12, 64]);  linear_19 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_13: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_encoder_layer_3_attention_attn_v_weight, p_encoder_layer_3_attention_attn_v_bias);  p_encoder_layer_3_attention_attn_v_weight = p_encoder_layer_3_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_14: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_20, [sym_size_int_85, -1, 12, 64]);  linear_20 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_14: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_14, 1, 2);  view_14 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_15: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_13, -1, -2);  transpose_13 = None\n",
       "                    matmul_6: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_12, transpose_15);  transpose_12 = transpose_15 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_5: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_6, 8.0);  matmul_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_558: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_5, clone_1);  div_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_564: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_558, mul_22);  add_558 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_3: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_564, -1);  add_564 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_14: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_3);  softmax_3 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_7: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_14, transpose_14);  clone_14 = transpose_14 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_4: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_7, [0, 2, 1, 3]);  matmul_7 = None\n",
       "                    clone_15: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_15: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_15, [sym_size_int_85, sym_size_int_86, 768]);  clone_15 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_15, p_encoder_layer_3_attention_attn_o_weight, p_encoder_layer_3_attention_attn_o_bias);  view_15 = p_encoder_layer_3_attention_attn_o_weight = p_encoder_layer_3_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_16: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_598: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_16, layer_norm_6);  clone_16 = layer_norm_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_598, [768], p_encoder_layer_3_attention_layernorm_weight, p_encoder_layer_3_attention_layernorm_bias);  add_598 = p_encoder_layer_3_attention_layernorm_weight = p_encoder_layer_3_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_7, p_encoder_layer_3_intermediate_dense_weight, p_encoder_layer_3_intermediate_dense_bias);  p_encoder_layer_3_intermediate_dense_weight = p_encoder_layer_3_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_3: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_3, p_encoder_layer_3_output_dense_weight, p_encoder_layer_3_output_dense_bias);  gelu_3 = p_encoder_layer_3_output_dense_weight = p_encoder_layer_3_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_17: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_617: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_17, layer_norm_7);  clone_17 = layer_norm_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_8: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_617, [768], p_encoder_layer_3_output_layernorm_weight, p_encoder_layer_3_output_layernorm_bias);  add_617 = p_encoder_layer_3_output_layernorm_weight = p_encoder_layer_3_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_q_weight, p_encoder_layer_4_attention_attn_q_bias);  p_encoder_layer_4_attention_attn_q_weight = p_encoder_layer_4_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_16: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_24, [sym_size_int_85, -1, 12, 64]);  linear_24 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_16: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_16, 1, 2);  view_16 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_k_weight, p_encoder_layer_4_attention_attn_k_bias);  p_encoder_layer_4_attention_attn_k_weight = p_encoder_layer_4_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_17: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_25, [sym_size_int_85, -1, 12, 64]);  linear_25 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_17: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_17, 1, 2);  view_17 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_8, p_encoder_layer_4_attention_attn_v_weight, p_encoder_layer_4_attention_attn_v_bias);  p_encoder_layer_4_attention_attn_v_weight = p_encoder_layer_4_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_18: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_26, [sym_size_int_85, -1, 12, 64]);  linear_26 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_18: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_19: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_17, -1, -2);  transpose_17 = None\n",
       "                    matmul_8: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_16, transpose_19);  transpose_16 = transpose_19 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_6: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_8, 8.0);  matmul_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_673: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_6, clone_1);  div_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_679: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_673, mul_22);  add_673 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_4: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_679, -1);  add_679 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_18: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_4);  softmax_4 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_9: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_18, transpose_18);  clone_18 = transpose_18 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_5: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_9, [0, 2, 1, 3]);  matmul_9 = None\n",
       "                    clone_19: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_19: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_19, [sym_size_int_85, sym_size_int_86, 768]);  clone_19 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_19, p_encoder_layer_4_attention_attn_o_weight, p_encoder_layer_4_attention_attn_o_bias);  view_19 = p_encoder_layer_4_attention_attn_o_weight = p_encoder_layer_4_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_713: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_20, layer_norm_8);  clone_20 = layer_norm_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_713, [768], p_encoder_layer_4_attention_layernorm_weight, p_encoder_layer_4_attention_layernorm_bias);  add_713 = p_encoder_layer_4_attention_layernorm_weight = p_encoder_layer_4_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_9, p_encoder_layer_4_intermediate_dense_weight, p_encoder_layer_4_intermediate_dense_bias);  p_encoder_layer_4_intermediate_dense_weight = p_encoder_layer_4_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_4: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_28);  linear_28 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_4, p_encoder_layer_4_output_dense_weight, p_encoder_layer_4_output_dense_bias);  gelu_4 = p_encoder_layer_4_output_dense_weight = p_encoder_layer_4_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_21: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_732: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_21, layer_norm_9);  clone_21 = layer_norm_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_732, [768], p_encoder_layer_4_output_layernorm_weight, p_encoder_layer_4_output_layernorm_bias);  add_732 = p_encoder_layer_4_output_layernorm_weight = p_encoder_layer_4_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_q_weight, p_encoder_layer_5_attention_attn_q_bias);  p_encoder_layer_5_attention_attn_q_weight = p_encoder_layer_5_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_20: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_30, [sym_size_int_85, -1, 12, 64]);  linear_30 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_20: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_k_weight, p_encoder_layer_5_attention_attn_k_bias);  p_encoder_layer_5_attention_attn_k_weight = p_encoder_layer_5_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_21: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_31, [sym_size_int_85, -1, 12, 64]);  linear_31 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_21: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_21, 1, 2);  view_21 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_10, p_encoder_layer_5_attention_attn_v_weight, p_encoder_layer_5_attention_attn_v_bias);  p_encoder_layer_5_attention_attn_v_weight = p_encoder_layer_5_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_22: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_32, [sym_size_int_85, -1, 12, 64]);  linear_32 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_22: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_23: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_21, -1, -2);  transpose_21 = None\n",
       "                    matmul_10: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_20, transpose_23);  transpose_20 = transpose_23 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_7: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_10, 8.0);  matmul_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_788: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_7, clone_1);  div_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_794: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_788, mul_22);  add_788 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_5: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_794, -1);  add_794 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_22: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_5);  softmax_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_11: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_22, transpose_22);  clone_22 = transpose_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_6: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_11, [0, 2, 1, 3]);  matmul_11 = None\n",
       "                    clone_23: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_23: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_23, [sym_size_int_85, sym_size_int_86, 768]);  clone_23 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_23, p_encoder_layer_5_attention_attn_o_weight, p_encoder_layer_5_attention_attn_o_bias);  view_23 = p_encoder_layer_5_attention_attn_o_weight = p_encoder_layer_5_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_24: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_828: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_24, layer_norm_10);  clone_24 = layer_norm_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_828, [768], p_encoder_layer_5_attention_layernorm_weight, p_encoder_layer_5_attention_layernorm_bias);  add_828 = p_encoder_layer_5_attention_layernorm_weight = p_encoder_layer_5_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_11, p_encoder_layer_5_intermediate_dense_weight, p_encoder_layer_5_intermediate_dense_bias);  p_encoder_layer_5_intermediate_dense_weight = p_encoder_layer_5_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_5: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_5, p_encoder_layer_5_output_dense_weight, p_encoder_layer_5_output_dense_bias);  gelu_5 = p_encoder_layer_5_output_dense_weight = p_encoder_layer_5_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_25: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_847: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_25, layer_norm_11);  clone_25 = layer_norm_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_847, [768], p_encoder_layer_5_output_layernorm_weight, p_encoder_layer_5_output_layernorm_bias);  add_847 = p_encoder_layer_5_output_layernorm_weight = p_encoder_layer_5_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_q_weight, p_encoder_layer_6_attention_attn_q_bias);  p_encoder_layer_6_attention_attn_q_weight = p_encoder_layer_6_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_24: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_36, [sym_size_int_85, -1, 12, 64]);  linear_36 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_24: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_24, 1, 2);  view_24 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_k_weight, p_encoder_layer_6_attention_attn_k_bias);  p_encoder_layer_6_attention_attn_k_weight = p_encoder_layer_6_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_25: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_37, [sym_size_int_85, -1, 12, 64]);  linear_37 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_25: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_25, 1, 2);  view_25 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_12, p_encoder_layer_6_attention_attn_v_weight, p_encoder_layer_6_attention_attn_v_bias);  p_encoder_layer_6_attention_attn_v_weight = p_encoder_layer_6_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_26: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_38, [sym_size_int_85, -1, 12, 64]);  linear_38 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_26: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_26, 1, 2);  view_26 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_27: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_25, -1, -2);  transpose_25 = None\n",
       "                    matmul_12: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_24, transpose_27);  transpose_24 = transpose_27 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_8: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_12, 8.0);  matmul_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_903: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_8, clone_1);  div_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_909: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_903, mul_22);  add_903 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_6: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_909, -1);  add_909 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_26: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_6);  softmax_6 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_13: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_26, transpose_26);  clone_26 = transpose_26 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_7: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_13, [0, 2, 1, 3]);  matmul_13 = None\n",
       "                    clone_27: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_27: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_27, [sym_size_int_85, sym_size_int_86, 768]);  clone_27 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_27, p_encoder_layer_6_attention_attn_o_weight, p_encoder_layer_6_attention_attn_o_bias);  view_27 = p_encoder_layer_6_attention_attn_o_weight = p_encoder_layer_6_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_28: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_943: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_28, layer_norm_12);  clone_28 = layer_norm_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_943, [768], p_encoder_layer_6_attention_layernorm_weight, p_encoder_layer_6_attention_layernorm_bias);  add_943 = p_encoder_layer_6_attention_layernorm_weight = p_encoder_layer_6_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_13, p_encoder_layer_6_intermediate_dense_weight, p_encoder_layer_6_intermediate_dense_bias);  p_encoder_layer_6_intermediate_dense_weight = p_encoder_layer_6_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_6: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_40);  linear_40 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_6, p_encoder_layer_6_output_dense_weight, p_encoder_layer_6_output_dense_bias);  gelu_6 = p_encoder_layer_6_output_dense_weight = p_encoder_layer_6_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_29: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_962: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_29, layer_norm_13);  clone_29 = layer_norm_13 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_962, [768], p_encoder_layer_6_output_layernorm_weight, p_encoder_layer_6_output_layernorm_bias);  add_962 = p_encoder_layer_6_output_layernorm_weight = p_encoder_layer_6_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_q_weight, p_encoder_layer_7_attention_attn_q_bias);  p_encoder_layer_7_attention_attn_q_weight = p_encoder_layer_7_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_28: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_42, [sym_size_int_85, -1, 12, 64]);  linear_42 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_28: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_28, 1, 2);  view_28 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_k_weight, p_encoder_layer_7_attention_attn_k_bias);  p_encoder_layer_7_attention_attn_k_weight = p_encoder_layer_7_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_29: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_43, [sym_size_int_85, -1, 12, 64]);  linear_43 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_29: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_14, p_encoder_layer_7_attention_attn_v_weight, p_encoder_layer_7_attention_attn_v_bias);  p_encoder_layer_7_attention_attn_v_weight = p_encoder_layer_7_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_30: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_44, [sym_size_int_85, -1, 12, 64]);  linear_44 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_30: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_31: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_29, -1, -2);  transpose_29 = None\n",
       "                    matmul_14: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_28, transpose_31);  transpose_28 = transpose_31 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_9: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_14, 8.0);  matmul_14 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1018: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_9, clone_1);  div_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1024: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1018, mul_22);  add_1018 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_7: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1024, -1);  add_1024 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_30: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_7);  softmax_7 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_15: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_30, transpose_30);  clone_30 = transpose_30 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_8: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_15, [0, 2, 1, 3]);  matmul_15 = None\n",
       "                    clone_31: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_31: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_31, [sym_size_int_85, sym_size_int_86, 768]);  clone_31 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_31, p_encoder_layer_7_attention_attn_o_weight, p_encoder_layer_7_attention_attn_o_bias);  view_31 = p_encoder_layer_7_attention_attn_o_weight = p_encoder_layer_7_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_32: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1058: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_32, layer_norm_14);  clone_32 = layer_norm_14 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1058, [768], p_encoder_layer_7_attention_layernorm_weight, p_encoder_layer_7_attention_layernorm_bias);  add_1058 = p_encoder_layer_7_attention_layernorm_weight = p_encoder_layer_7_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_15, p_encoder_layer_7_intermediate_dense_weight, p_encoder_layer_7_intermediate_dense_bias);  p_encoder_layer_7_intermediate_dense_weight = p_encoder_layer_7_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_7: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_7, p_encoder_layer_7_output_dense_weight, p_encoder_layer_7_output_dense_bias);  gelu_7 = p_encoder_layer_7_output_dense_weight = p_encoder_layer_7_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_33: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1077: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_33, layer_norm_15);  clone_33 = layer_norm_15 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1077, [768], p_encoder_layer_7_output_layernorm_weight, p_encoder_layer_7_output_layernorm_bias);  add_1077 = p_encoder_layer_7_output_layernorm_weight = p_encoder_layer_7_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_q_weight, p_encoder_layer_8_attention_attn_q_bias);  p_encoder_layer_8_attention_attn_q_weight = p_encoder_layer_8_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_32: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_48, [sym_size_int_85, -1, 12, 64]);  linear_48 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_32: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_k_weight, p_encoder_layer_8_attention_attn_k_bias);  p_encoder_layer_8_attention_attn_k_weight = p_encoder_layer_8_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_33: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_49, [sym_size_int_85, -1, 12, 64]);  linear_49 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_33: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_33, 1, 2);  view_33 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_16, p_encoder_layer_8_attention_attn_v_weight, p_encoder_layer_8_attention_attn_v_bias);  p_encoder_layer_8_attention_attn_v_weight = p_encoder_layer_8_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_34: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_50, [sym_size_int_85, -1, 12, 64]);  linear_50 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_34: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_34, 1, 2);  view_34 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_35: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_33, -1, -2);  transpose_33 = None\n",
       "                    matmul_16: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_32, transpose_35);  transpose_32 = transpose_35 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_10: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_16, 8.0);  matmul_16 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1133: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_10, clone_1);  div_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1139: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1133, mul_22);  add_1133 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_8: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1139, -1);  add_1139 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_34: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_8);  softmax_8 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_17: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_34, transpose_34);  clone_34 = transpose_34 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_9: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_17, [0, 2, 1, 3]);  matmul_17 = None\n",
       "                    clone_35: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_35: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_35, [sym_size_int_85, sym_size_int_86, 768]);  clone_35 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_35, p_encoder_layer_8_attention_attn_o_weight, p_encoder_layer_8_attention_attn_o_bias);  view_35 = p_encoder_layer_8_attention_attn_o_weight = p_encoder_layer_8_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_36: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_51);  linear_51 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1173: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_36, layer_norm_16);  clone_36 = layer_norm_16 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1173, [768], p_encoder_layer_8_attention_layernorm_weight, p_encoder_layer_8_attention_layernorm_bias);  add_1173 = p_encoder_layer_8_attention_layernorm_weight = p_encoder_layer_8_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_17, p_encoder_layer_8_intermediate_dense_weight, p_encoder_layer_8_intermediate_dense_bias);  p_encoder_layer_8_intermediate_dense_weight = p_encoder_layer_8_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_8: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_8, p_encoder_layer_8_output_dense_weight, p_encoder_layer_8_output_dense_bias);  gelu_8 = p_encoder_layer_8_output_dense_weight = p_encoder_layer_8_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_37: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_53);  linear_53 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1192: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_37, layer_norm_17);  clone_37 = layer_norm_17 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1192, [768], p_encoder_layer_8_output_layernorm_weight, p_encoder_layer_8_output_layernorm_bias);  add_1192 = p_encoder_layer_8_output_layernorm_weight = p_encoder_layer_8_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_q_weight, p_encoder_layer_9_attention_attn_q_bias);  p_encoder_layer_9_attention_attn_q_weight = p_encoder_layer_9_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_36: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_54, [sym_size_int_85, -1, 12, 64]);  linear_54 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_36: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_36, 1, 2);  view_36 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_k_weight, p_encoder_layer_9_attention_attn_k_bias);  p_encoder_layer_9_attention_attn_k_weight = p_encoder_layer_9_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_37: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_55, [sym_size_int_85, -1, 12, 64]);  linear_55 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_37: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_37, 1, 2);  view_37 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_18, p_encoder_layer_9_attention_attn_v_weight, p_encoder_layer_9_attention_attn_v_bias);  p_encoder_layer_9_attention_attn_v_weight = p_encoder_layer_9_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_38: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_56, [sym_size_int_85, -1, 12, 64]);  linear_56 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_38: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_38, 1, 2);  view_38 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_39: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_37, -1, -2);  transpose_37 = None\n",
       "                    matmul_18: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_36, transpose_39);  transpose_36 = transpose_39 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_11: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_18, 8.0);  matmul_18 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1248: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_11, clone_1);  div_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1254: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1248, mul_22);  add_1248 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_9: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1254, -1);  add_1254 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_38: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_9);  softmax_9 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_19: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_38, transpose_38);  clone_38 = transpose_38 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_10: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_19, [0, 2, 1, 3]);  matmul_19 = None\n",
       "                    clone_39: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_39: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_39, [sym_size_int_85, sym_size_int_86, 768]);  clone_39 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_39, p_encoder_layer_9_attention_attn_o_weight, p_encoder_layer_9_attention_attn_o_bias);  view_39 = p_encoder_layer_9_attention_attn_o_weight = p_encoder_layer_9_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_40: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_57);  linear_57 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1288: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_40, layer_norm_18);  clone_40 = layer_norm_18 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1288, [768], p_encoder_layer_9_attention_layernorm_weight, p_encoder_layer_9_attention_layernorm_bias);  add_1288 = p_encoder_layer_9_attention_layernorm_weight = p_encoder_layer_9_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_19, p_encoder_layer_9_intermediate_dense_weight, p_encoder_layer_9_intermediate_dense_bias);  p_encoder_layer_9_intermediate_dense_weight = p_encoder_layer_9_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_9: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_58);  linear_58 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_9, p_encoder_layer_9_output_dense_weight, p_encoder_layer_9_output_dense_bias);  gelu_9 = p_encoder_layer_9_output_dense_weight = p_encoder_layer_9_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_41: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_59);  linear_59 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1307: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_41, layer_norm_19);  clone_41 = layer_norm_19 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1307, [768], p_encoder_layer_9_output_layernorm_weight, p_encoder_layer_9_output_layernorm_bias);  add_1307 = p_encoder_layer_9_output_layernorm_weight = p_encoder_layer_9_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_q_weight, p_encoder_layer_10_attention_attn_q_bias);  p_encoder_layer_10_attention_attn_q_weight = p_encoder_layer_10_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_40: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_60, [sym_size_int_85, -1, 12, 64]);  linear_60 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_40: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_40, 1, 2);  view_40 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_k_weight, p_encoder_layer_10_attention_attn_k_bias);  p_encoder_layer_10_attention_attn_k_weight = p_encoder_layer_10_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_41: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_61, [sym_size_int_85, -1, 12, 64]);  linear_61 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_41: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_41, 1, 2);  view_41 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_20, p_encoder_layer_10_attention_attn_v_weight, p_encoder_layer_10_attention_attn_v_bias);  p_encoder_layer_10_attention_attn_v_weight = p_encoder_layer_10_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_42: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_62, [sym_size_int_85, -1, 12, 64]);  linear_62 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_42: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_42, 1, 2);  view_42 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_43: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_41, -1, -2);  transpose_41 = None\n",
       "                    matmul_20: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_40, transpose_43);  transpose_40 = transpose_43 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_12: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_20, 8.0);  matmul_20 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1363: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_12, clone_1);  div_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1369: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1363, mul_22);  add_1363 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_10: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1369, -1);  add_1369 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_42: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_10);  softmax_10 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_21: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_42, transpose_42);  clone_42 = transpose_42 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_11: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_21, [0, 2, 1, 3]);  matmul_21 = None\n",
       "                    clone_43: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_11, memory_format = torch.contiguous_format);  permute_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_43: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_43, [sym_size_int_85, sym_size_int_86, 768]);  clone_43 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_43, p_encoder_layer_10_attention_attn_o_weight, p_encoder_layer_10_attention_attn_o_bias);  view_43 = p_encoder_layer_10_attention_attn_o_weight = p_encoder_layer_10_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_44: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1403: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_44, layer_norm_20);  clone_44 = layer_norm_20 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1403, [768], p_encoder_layer_10_attention_layernorm_weight, p_encoder_layer_10_attention_layernorm_bias);  add_1403 = p_encoder_layer_10_attention_layernorm_weight = p_encoder_layer_10_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_21, p_encoder_layer_10_intermediate_dense_weight, p_encoder_layer_10_intermediate_dense_bias);  p_encoder_layer_10_intermediate_dense_weight = p_encoder_layer_10_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_10: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_64);  linear_64 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_10, p_encoder_layer_10_output_dense_weight, p_encoder_layer_10_output_dense_bias);  gelu_10 = p_encoder_layer_10_output_dense_weight = p_encoder_layer_10_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_45: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_65);  linear_65 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1422: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_45, layer_norm_21);  clone_45 = layer_norm_21 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1422, [768], p_encoder_layer_10_output_layernorm_weight, p_encoder_layer_10_output_layernorm_bias);  add_1422 = p_encoder_layer_10_output_layernorm_weight = p_encoder_layer_10_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_q_weight, p_encoder_layer_11_attention_attn_q_bias);  p_encoder_layer_11_attention_attn_q_weight = p_encoder_layer_11_attention_attn_q_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:159 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_44: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_66, [sym_size_int_85, -1, 12, 64]);  linear_66 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:160 in forward, code: .transpose(1, 2)\n",
       "                    transpose_44: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_44, 1, 2);  view_44 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_k_weight, p_encoder_layer_11_attention_attn_k_bias);  p_encoder_layer_11_attention_attn_k_weight = p_encoder_layer_11_attention_attn_k_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:164 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_45: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_67, [sym_size_int_85, -1, 12, 64]);  linear_67 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:165 in forward, code: .transpose(1, 2)\n",
       "                    transpose_45: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_45, 1, 2);  view_45 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(layer_norm_22, p_encoder_layer_11_attention_attn_v_weight, p_encoder_layer_11_attention_attn_v_bias);  p_encoder_layer_11_attention_attn_v_weight = p_encoder_layer_11_attention_attn_v_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:169 in forward, code: .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
       "                    view_46: \"f32[1, s53, 12, 64]\" = torch.ops.aten.view.default(linear_68, [sym_size_int_85, -1, 12, 64]);  linear_68 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:170 in forward, code: .transpose(1, 2)\n",
       "                    transpose_46: \"f32[1, 12, s53, 64]\" = torch.ops.aten.transpose.int(view_46, 1, 2);  view_46 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:174 in forward, code: attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
       "                    transpose_47: \"f32[1, 12, 64, s53]\" = torch.ops.aten.transpose.int(transpose_45, -1, -2);  transpose_45 = None\n",
       "                    matmul_22: \"f32[1, 12, s53, s53]\" = torch.ops.aten.matmul.default(transpose_44, transpose_47);  transpose_44 = transpose_47 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:175 in forward, code: attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
       "                    div_13: \"f32[1, 12, s53, s53]\" = torch.ops.aten.div.Tensor(matmul_22, 8.0);  matmul_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:179 in forward, code: attention_scores += position_bias\n",
       "                    add_1478: \"f32[s43, 12, s53, s53]\" = torch.ops.aten.add.Tensor(div_13, clone_1);  div_13 = clone_1 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:182 in forward, code: attention_scores = attention_scores + attention_mask\n",
       "                    add_1484: \"f32[1, 12, s53, s53]\" = torch.ops.aten.add.Tensor(add_1478, mul_22);  add_1478 = mul_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:185 in forward, code: attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
       "                    softmax_11: \"f32[1, 12, s53, s53]\" = torch.ops.aten.softmax.int(add_1484, -1);  add_1484 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_46: \"f32[1, 12, s53, s53]\" = torch.ops.aten.clone.default(softmax_11);  softmax_11 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:192 in forward, code: c = torch.matmul(attention_probs, v)\n",
       "                    matmul_23: \"f32[1, 12, s53, 64]\" = torch.ops.aten.matmul.default(clone_46, transpose_46);  clone_46 = transpose_46 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:194 in forward, code: c = c.permute(0, 2, 1, 3).contiguous()\n",
       "                    permute_12: \"f32[1, s53, 12, 64]\" = torch.ops.aten.permute.default(matmul_23, [0, 2, 1, 3]);  matmul_23 = None\n",
       "                    clone_47: \"f32[1, s53, 12, 64]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:196 in forward, code: c = c.view(*new_c_shape)\n",
       "                    view_47: \"f32[1, s53, 768]\" = torch.ops.aten.view.default(clone_47, [sym_size_int_85, sym_size_int_86, 768]);  clone_47 = sym_size_int_85 = sym_size_int_86 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(view_47, p_encoder_layer_11_attention_attn_o_weight, p_encoder_layer_11_attention_attn_o_bias);  view_47 = p_encoder_layer_11_attention_attn_o_weight = p_encoder_layer_11_attention_attn_o_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_48: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_69);  linear_69 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:245 in forward, code: attention_output = self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)\n",
       "                    add_1518: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_48, layer_norm_22);  clone_48 = layer_norm_22 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1518, [768], p_encoder_layer_11_attention_layernorm_weight, p_encoder_layer_11_attention_layernorm_bias);  add_1518 = p_encoder_layer_11_attention_layernorm_weight = p_encoder_layer_11_attention_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f32[1, s53, 3072]\" = torch.ops.aten.linear.default(layer_norm_23, p_encoder_layer_11_intermediate_dense_weight, p_encoder_layer_11_intermediate_dense_bias);  p_encoder_layer_11_intermediate_dense_weight = p_encoder_layer_11_intermediate_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/activations.py:85 in forward, code: return self.act(input)\n",
       "                    gelu_11: \"f32[1, s53, 3072]\" = torch.ops.aten.gelu.default(linear_70);  linear_70 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f32[1, s53, 768]\" = torch.ops.aten.linear.default(gelu_11, p_encoder_layer_11_output_dense_weight, p_encoder_layer_11_output_dense_bias);  gelu_11 = p_encoder_layer_11_output_dense_weight = p_encoder_layer_11_output_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_49: \"f32[1, s53, 768]\" = torch.ops.aten.clone.default(linear_71);  linear_71 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:277 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
       "                    add_1537: \"f32[1, s53, 768]\" = torch.ops.aten.add.Tensor(clone_49, layer_norm_23);  clone_49 = layer_norm_23 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f32[1, s53, 768]\" = torch.ops.aten.layer_norm.default(add_1537, [768], p_encoder_layer_11_output_layernorm_weight, p_encoder_layer_11_output_layernorm_bias);  add_1537 = p_encoder_layer_11_output_layernorm_weight = p_encoder_layer_11_output_layernorm_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py:412 in forward, code: first_token_tensor = hidden_states[:, 0]\n",
       "                    slice_5: \"f32[1, s53, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_24, 0, 0, 9223372036854775807)\n",
       "                    select: \"f32[1, 768]\" = torch.ops.aten.select.int(slice_5, 1, 0);  slice_5 = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_72: \"f32[1, 768]\" = torch.ops.aten.linear.default(select, p_pooler_dense_weight, p_pooler_dense_bias);  select = p_pooler_dense_weight = p_pooler_dense_bias = None\n",
       "            \n",
       "                     # File: /Users/aszarata/informatyka/9s/agh-mlops/lab07/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:430 in forward, code: return torch.tanh(input)\n",
       "                    tanh: \"f32[1, 768]\" = torch.ops.aten.tanh.default(linear_72);  linear_72 = None\n",
       "                    return (layer_norm_24, tanh)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_embeddings_word_embeddings_weight: PARAMETER target='embeddings.word_embeddings.weight'\n",
       "            p_embeddings_position_embeddings_weight: PARAMETER target='embeddings.position_embeddings.weight'\n",
       "            p_embeddings_layernorm_weight: PARAMETER target='embeddings.LayerNorm.weight'\n",
       "            p_embeddings_layernorm_bias: PARAMETER target='embeddings.LayerNorm.bias'\n",
       "            p_encoder_layer_0_attention_attn_q_weight: PARAMETER target='encoder.layer.0.attention.attn.q.weight'\n",
       "            p_encoder_layer_0_attention_attn_q_bias: PARAMETER target='encoder.layer.0.attention.attn.q.bias'\n",
       "            p_encoder_layer_0_attention_attn_k_weight: PARAMETER target='encoder.layer.0.attention.attn.k.weight'\n",
       "            p_encoder_layer_0_attention_attn_k_bias: PARAMETER target='encoder.layer.0.attention.attn.k.bias'\n",
       "            p_encoder_layer_0_attention_attn_v_weight: PARAMETER target='encoder.layer.0.attention.attn.v.weight'\n",
       "            p_encoder_layer_0_attention_attn_v_bias: PARAMETER target='encoder.layer.0.attention.attn.v.bias'\n",
       "            p_encoder_layer_0_attention_attn_o_weight: PARAMETER target='encoder.layer.0.attention.attn.o.weight'\n",
       "            p_encoder_layer_0_attention_attn_o_bias: PARAMETER target='encoder.layer.0.attention.attn.o.bias'\n",
       "            p_encoder_layer_0_attention_layernorm_weight: PARAMETER target='encoder.layer.0.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_0_attention_layernorm_bias: PARAMETER target='encoder.layer.0.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_0_intermediate_dense_weight: PARAMETER target='encoder.layer.0.intermediate.dense.weight'\n",
       "            p_encoder_layer_0_intermediate_dense_bias: PARAMETER target='encoder.layer.0.intermediate.dense.bias'\n",
       "            p_encoder_layer_0_output_dense_weight: PARAMETER target='encoder.layer.0.output.dense.weight'\n",
       "            p_encoder_layer_0_output_dense_bias: PARAMETER target='encoder.layer.0.output.dense.bias'\n",
       "            p_encoder_layer_0_output_layernorm_weight: PARAMETER target='encoder.layer.0.output.LayerNorm.weight'\n",
       "            p_encoder_layer_0_output_layernorm_bias: PARAMETER target='encoder.layer.0.output.LayerNorm.bias'\n",
       "            p_encoder_layer_1_attention_attn_q_weight: PARAMETER target='encoder.layer.1.attention.attn.q.weight'\n",
       "            p_encoder_layer_1_attention_attn_q_bias: PARAMETER target='encoder.layer.1.attention.attn.q.bias'\n",
       "            p_encoder_layer_1_attention_attn_k_weight: PARAMETER target='encoder.layer.1.attention.attn.k.weight'\n",
       "            p_encoder_layer_1_attention_attn_k_bias: PARAMETER target='encoder.layer.1.attention.attn.k.bias'\n",
       "            p_encoder_layer_1_attention_attn_v_weight: PARAMETER target='encoder.layer.1.attention.attn.v.weight'\n",
       "            p_encoder_layer_1_attention_attn_v_bias: PARAMETER target='encoder.layer.1.attention.attn.v.bias'\n",
       "            p_encoder_layer_1_attention_attn_o_weight: PARAMETER target='encoder.layer.1.attention.attn.o.weight'\n",
       "            p_encoder_layer_1_attention_attn_o_bias: PARAMETER target='encoder.layer.1.attention.attn.o.bias'\n",
       "            p_encoder_layer_1_attention_layernorm_weight: PARAMETER target='encoder.layer.1.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_1_attention_layernorm_bias: PARAMETER target='encoder.layer.1.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_1_intermediate_dense_weight: PARAMETER target='encoder.layer.1.intermediate.dense.weight'\n",
       "            p_encoder_layer_1_intermediate_dense_bias: PARAMETER target='encoder.layer.1.intermediate.dense.bias'\n",
       "            p_encoder_layer_1_output_dense_weight: PARAMETER target='encoder.layer.1.output.dense.weight'\n",
       "            p_encoder_layer_1_output_dense_bias: PARAMETER target='encoder.layer.1.output.dense.bias'\n",
       "            p_encoder_layer_1_output_layernorm_weight: PARAMETER target='encoder.layer.1.output.LayerNorm.weight'\n",
       "            p_encoder_layer_1_output_layernorm_bias: PARAMETER target='encoder.layer.1.output.LayerNorm.bias'\n",
       "            p_encoder_layer_2_attention_attn_q_weight: PARAMETER target='encoder.layer.2.attention.attn.q.weight'\n",
       "            p_encoder_layer_2_attention_attn_q_bias: PARAMETER target='encoder.layer.2.attention.attn.q.bias'\n",
       "            p_encoder_layer_2_attention_attn_k_weight: PARAMETER target='encoder.layer.2.attention.attn.k.weight'\n",
       "            p_encoder_layer_2_attention_attn_k_bias: PARAMETER target='encoder.layer.2.attention.attn.k.bias'\n",
       "            p_encoder_layer_2_attention_attn_v_weight: PARAMETER target='encoder.layer.2.attention.attn.v.weight'\n",
       "            p_encoder_layer_2_attention_attn_v_bias: PARAMETER target='encoder.layer.2.attention.attn.v.bias'\n",
       "            p_encoder_layer_2_attention_attn_o_weight: PARAMETER target='encoder.layer.2.attention.attn.o.weight'\n",
       "            p_encoder_layer_2_attention_attn_o_bias: PARAMETER target='encoder.layer.2.attention.attn.o.bias'\n",
       "            p_encoder_layer_2_attention_layernorm_weight: PARAMETER target='encoder.layer.2.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_2_attention_layernorm_bias: PARAMETER target='encoder.layer.2.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_2_intermediate_dense_weight: PARAMETER target='encoder.layer.2.intermediate.dense.weight'\n",
       "            p_encoder_layer_2_intermediate_dense_bias: PARAMETER target='encoder.layer.2.intermediate.dense.bias'\n",
       "            p_encoder_layer_2_output_dense_weight: PARAMETER target='encoder.layer.2.output.dense.weight'\n",
       "            p_encoder_layer_2_output_dense_bias: PARAMETER target='encoder.layer.2.output.dense.bias'\n",
       "            p_encoder_layer_2_output_layernorm_weight: PARAMETER target='encoder.layer.2.output.LayerNorm.weight'\n",
       "            p_encoder_layer_2_output_layernorm_bias: PARAMETER target='encoder.layer.2.output.LayerNorm.bias'\n",
       "            p_encoder_layer_3_attention_attn_q_weight: PARAMETER target='encoder.layer.3.attention.attn.q.weight'\n",
       "            p_encoder_layer_3_attention_attn_q_bias: PARAMETER target='encoder.layer.3.attention.attn.q.bias'\n",
       "            p_encoder_layer_3_attention_attn_k_weight: PARAMETER target='encoder.layer.3.attention.attn.k.weight'\n",
       "            p_encoder_layer_3_attention_attn_k_bias: PARAMETER target='encoder.layer.3.attention.attn.k.bias'\n",
       "            p_encoder_layer_3_attention_attn_v_weight: PARAMETER target='encoder.layer.3.attention.attn.v.weight'\n",
       "            p_encoder_layer_3_attention_attn_v_bias: PARAMETER target='encoder.layer.3.attention.attn.v.bias'\n",
       "            p_encoder_layer_3_attention_attn_o_weight: PARAMETER target='encoder.layer.3.attention.attn.o.weight'\n",
       "            p_encoder_layer_3_attention_attn_o_bias: PARAMETER target='encoder.layer.3.attention.attn.o.bias'\n",
       "            p_encoder_layer_3_attention_layernorm_weight: PARAMETER target='encoder.layer.3.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_3_attention_layernorm_bias: PARAMETER target='encoder.layer.3.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_3_intermediate_dense_weight: PARAMETER target='encoder.layer.3.intermediate.dense.weight'\n",
       "            p_encoder_layer_3_intermediate_dense_bias: PARAMETER target='encoder.layer.3.intermediate.dense.bias'\n",
       "            p_encoder_layer_3_output_dense_weight: PARAMETER target='encoder.layer.3.output.dense.weight'\n",
       "            p_encoder_layer_3_output_dense_bias: PARAMETER target='encoder.layer.3.output.dense.bias'\n",
       "            p_encoder_layer_3_output_layernorm_weight: PARAMETER target='encoder.layer.3.output.LayerNorm.weight'\n",
       "            p_encoder_layer_3_output_layernorm_bias: PARAMETER target='encoder.layer.3.output.LayerNorm.bias'\n",
       "            p_encoder_layer_4_attention_attn_q_weight: PARAMETER target='encoder.layer.4.attention.attn.q.weight'\n",
       "            p_encoder_layer_4_attention_attn_q_bias: PARAMETER target='encoder.layer.4.attention.attn.q.bias'\n",
       "            p_encoder_layer_4_attention_attn_k_weight: PARAMETER target='encoder.layer.4.attention.attn.k.weight'\n",
       "            p_encoder_layer_4_attention_attn_k_bias: PARAMETER target='encoder.layer.4.attention.attn.k.bias'\n",
       "            p_encoder_layer_4_attention_attn_v_weight: PARAMETER target='encoder.layer.4.attention.attn.v.weight'\n",
       "            p_encoder_layer_4_attention_attn_v_bias: PARAMETER target='encoder.layer.4.attention.attn.v.bias'\n",
       "            p_encoder_layer_4_attention_attn_o_weight: PARAMETER target='encoder.layer.4.attention.attn.o.weight'\n",
       "            p_encoder_layer_4_attention_attn_o_bias: PARAMETER target='encoder.layer.4.attention.attn.o.bias'\n",
       "            p_encoder_layer_4_attention_layernorm_weight: PARAMETER target='encoder.layer.4.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_4_attention_layernorm_bias: PARAMETER target='encoder.layer.4.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_4_intermediate_dense_weight: PARAMETER target='encoder.layer.4.intermediate.dense.weight'\n",
       "            p_encoder_layer_4_intermediate_dense_bias: PARAMETER target='encoder.layer.4.intermediate.dense.bias'\n",
       "            p_encoder_layer_4_output_dense_weight: PARAMETER target='encoder.layer.4.output.dense.weight'\n",
       "            p_encoder_layer_4_output_dense_bias: PARAMETER target='encoder.layer.4.output.dense.bias'\n",
       "            p_encoder_layer_4_output_layernorm_weight: PARAMETER target='encoder.layer.4.output.LayerNorm.weight'\n",
       "            p_encoder_layer_4_output_layernorm_bias: PARAMETER target='encoder.layer.4.output.LayerNorm.bias'\n",
       "            p_encoder_layer_5_attention_attn_q_weight: PARAMETER target='encoder.layer.5.attention.attn.q.weight'\n",
       "            p_encoder_layer_5_attention_attn_q_bias: PARAMETER target='encoder.layer.5.attention.attn.q.bias'\n",
       "            p_encoder_layer_5_attention_attn_k_weight: PARAMETER target='encoder.layer.5.attention.attn.k.weight'\n",
       "            p_encoder_layer_5_attention_attn_k_bias: PARAMETER target='encoder.layer.5.attention.attn.k.bias'\n",
       "            p_encoder_layer_5_attention_attn_v_weight: PARAMETER target='encoder.layer.5.attention.attn.v.weight'\n",
       "            p_encoder_layer_5_attention_attn_v_bias: PARAMETER target='encoder.layer.5.attention.attn.v.bias'\n",
       "            p_encoder_layer_5_attention_attn_o_weight: PARAMETER target='encoder.layer.5.attention.attn.o.weight'\n",
       "            p_encoder_layer_5_attention_attn_o_bias: PARAMETER target='encoder.layer.5.attention.attn.o.bias'\n",
       "            p_encoder_layer_5_attention_layernorm_weight: PARAMETER target='encoder.layer.5.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_5_attention_layernorm_bias: PARAMETER target='encoder.layer.5.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_5_intermediate_dense_weight: PARAMETER target='encoder.layer.5.intermediate.dense.weight'\n",
       "            p_encoder_layer_5_intermediate_dense_bias: PARAMETER target='encoder.layer.5.intermediate.dense.bias'\n",
       "            p_encoder_layer_5_output_dense_weight: PARAMETER target='encoder.layer.5.output.dense.weight'\n",
       "            p_encoder_layer_5_output_dense_bias: PARAMETER target='encoder.layer.5.output.dense.bias'\n",
       "            p_encoder_layer_5_output_layernorm_weight: PARAMETER target='encoder.layer.5.output.LayerNorm.weight'\n",
       "            p_encoder_layer_5_output_layernorm_bias: PARAMETER target='encoder.layer.5.output.LayerNorm.bias'\n",
       "            p_encoder_layer_6_attention_attn_q_weight: PARAMETER target='encoder.layer.6.attention.attn.q.weight'\n",
       "            p_encoder_layer_6_attention_attn_q_bias: PARAMETER target='encoder.layer.6.attention.attn.q.bias'\n",
       "            p_encoder_layer_6_attention_attn_k_weight: PARAMETER target='encoder.layer.6.attention.attn.k.weight'\n",
       "            p_encoder_layer_6_attention_attn_k_bias: PARAMETER target='encoder.layer.6.attention.attn.k.bias'\n",
       "            p_encoder_layer_6_attention_attn_v_weight: PARAMETER target='encoder.layer.6.attention.attn.v.weight'\n",
       "            p_encoder_layer_6_attention_attn_v_bias: PARAMETER target='encoder.layer.6.attention.attn.v.bias'\n",
       "            p_encoder_layer_6_attention_attn_o_weight: PARAMETER target='encoder.layer.6.attention.attn.o.weight'\n",
       "            p_encoder_layer_6_attention_attn_o_bias: PARAMETER target='encoder.layer.6.attention.attn.o.bias'\n",
       "            p_encoder_layer_6_attention_layernorm_weight: PARAMETER target='encoder.layer.6.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_6_attention_layernorm_bias: PARAMETER target='encoder.layer.6.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_6_intermediate_dense_weight: PARAMETER target='encoder.layer.6.intermediate.dense.weight'\n",
       "            p_encoder_layer_6_intermediate_dense_bias: PARAMETER target='encoder.layer.6.intermediate.dense.bias'\n",
       "            p_encoder_layer_6_output_dense_weight: PARAMETER target='encoder.layer.6.output.dense.weight'\n",
       "            p_encoder_layer_6_output_dense_bias: PARAMETER target='encoder.layer.6.output.dense.bias'\n",
       "            p_encoder_layer_6_output_layernorm_weight: PARAMETER target='encoder.layer.6.output.LayerNorm.weight'\n",
       "            p_encoder_layer_6_output_layernorm_bias: PARAMETER target='encoder.layer.6.output.LayerNorm.bias'\n",
       "            p_encoder_layer_7_attention_attn_q_weight: PARAMETER target='encoder.layer.7.attention.attn.q.weight'\n",
       "            p_encoder_layer_7_attention_attn_q_bias: PARAMETER target='encoder.layer.7.attention.attn.q.bias'\n",
       "            p_encoder_layer_7_attention_attn_k_weight: PARAMETER target='encoder.layer.7.attention.attn.k.weight'\n",
       "            p_encoder_layer_7_attention_attn_k_bias: PARAMETER target='encoder.layer.7.attention.attn.k.bias'\n",
       "            p_encoder_layer_7_attention_attn_v_weight: PARAMETER target='encoder.layer.7.attention.attn.v.weight'\n",
       "            p_encoder_layer_7_attention_attn_v_bias: PARAMETER target='encoder.layer.7.attention.attn.v.bias'\n",
       "            p_encoder_layer_7_attention_attn_o_weight: PARAMETER target='encoder.layer.7.attention.attn.o.weight'\n",
       "            p_encoder_layer_7_attention_attn_o_bias: PARAMETER target='encoder.layer.7.attention.attn.o.bias'\n",
       "            p_encoder_layer_7_attention_layernorm_weight: PARAMETER target='encoder.layer.7.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_7_attention_layernorm_bias: PARAMETER target='encoder.layer.7.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_7_intermediate_dense_weight: PARAMETER target='encoder.layer.7.intermediate.dense.weight'\n",
       "            p_encoder_layer_7_intermediate_dense_bias: PARAMETER target='encoder.layer.7.intermediate.dense.bias'\n",
       "            p_encoder_layer_7_output_dense_weight: PARAMETER target='encoder.layer.7.output.dense.weight'\n",
       "            p_encoder_layer_7_output_dense_bias: PARAMETER target='encoder.layer.7.output.dense.bias'\n",
       "            p_encoder_layer_7_output_layernorm_weight: PARAMETER target='encoder.layer.7.output.LayerNorm.weight'\n",
       "            p_encoder_layer_7_output_layernorm_bias: PARAMETER target='encoder.layer.7.output.LayerNorm.bias'\n",
       "            p_encoder_layer_8_attention_attn_q_weight: PARAMETER target='encoder.layer.8.attention.attn.q.weight'\n",
       "            p_encoder_layer_8_attention_attn_q_bias: PARAMETER target='encoder.layer.8.attention.attn.q.bias'\n",
       "            p_encoder_layer_8_attention_attn_k_weight: PARAMETER target='encoder.layer.8.attention.attn.k.weight'\n",
       "            p_encoder_layer_8_attention_attn_k_bias: PARAMETER target='encoder.layer.8.attention.attn.k.bias'\n",
       "            p_encoder_layer_8_attention_attn_v_weight: PARAMETER target='encoder.layer.8.attention.attn.v.weight'\n",
       "            p_encoder_layer_8_attention_attn_v_bias: PARAMETER target='encoder.layer.8.attention.attn.v.bias'\n",
       "            p_encoder_layer_8_attention_attn_o_weight: PARAMETER target='encoder.layer.8.attention.attn.o.weight'\n",
       "            p_encoder_layer_8_attention_attn_o_bias: PARAMETER target='encoder.layer.8.attention.attn.o.bias'\n",
       "            p_encoder_layer_8_attention_layernorm_weight: PARAMETER target='encoder.layer.8.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_8_attention_layernorm_bias: PARAMETER target='encoder.layer.8.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_8_intermediate_dense_weight: PARAMETER target='encoder.layer.8.intermediate.dense.weight'\n",
       "            p_encoder_layer_8_intermediate_dense_bias: PARAMETER target='encoder.layer.8.intermediate.dense.bias'\n",
       "            p_encoder_layer_8_output_dense_weight: PARAMETER target='encoder.layer.8.output.dense.weight'\n",
       "            p_encoder_layer_8_output_dense_bias: PARAMETER target='encoder.layer.8.output.dense.bias'\n",
       "            p_encoder_layer_8_output_layernorm_weight: PARAMETER target='encoder.layer.8.output.LayerNorm.weight'\n",
       "            p_encoder_layer_8_output_layernorm_bias: PARAMETER target='encoder.layer.8.output.LayerNorm.bias'\n",
       "            p_encoder_layer_9_attention_attn_q_weight: PARAMETER target='encoder.layer.9.attention.attn.q.weight'\n",
       "            p_encoder_layer_9_attention_attn_q_bias: PARAMETER target='encoder.layer.9.attention.attn.q.bias'\n",
       "            p_encoder_layer_9_attention_attn_k_weight: PARAMETER target='encoder.layer.9.attention.attn.k.weight'\n",
       "            p_encoder_layer_9_attention_attn_k_bias: PARAMETER target='encoder.layer.9.attention.attn.k.bias'\n",
       "            p_encoder_layer_9_attention_attn_v_weight: PARAMETER target='encoder.layer.9.attention.attn.v.weight'\n",
       "            p_encoder_layer_9_attention_attn_v_bias: PARAMETER target='encoder.layer.9.attention.attn.v.bias'\n",
       "            p_encoder_layer_9_attention_attn_o_weight: PARAMETER target='encoder.layer.9.attention.attn.o.weight'\n",
       "            p_encoder_layer_9_attention_attn_o_bias: PARAMETER target='encoder.layer.9.attention.attn.o.bias'\n",
       "            p_encoder_layer_9_attention_layernorm_weight: PARAMETER target='encoder.layer.9.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_9_attention_layernorm_bias: PARAMETER target='encoder.layer.9.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_9_intermediate_dense_weight: PARAMETER target='encoder.layer.9.intermediate.dense.weight'\n",
       "            p_encoder_layer_9_intermediate_dense_bias: PARAMETER target='encoder.layer.9.intermediate.dense.bias'\n",
       "            p_encoder_layer_9_output_dense_weight: PARAMETER target='encoder.layer.9.output.dense.weight'\n",
       "            p_encoder_layer_9_output_dense_bias: PARAMETER target='encoder.layer.9.output.dense.bias'\n",
       "            p_encoder_layer_9_output_layernorm_weight: PARAMETER target='encoder.layer.9.output.LayerNorm.weight'\n",
       "            p_encoder_layer_9_output_layernorm_bias: PARAMETER target='encoder.layer.9.output.LayerNorm.bias'\n",
       "            p_encoder_layer_10_attention_attn_q_weight: PARAMETER target='encoder.layer.10.attention.attn.q.weight'\n",
       "            p_encoder_layer_10_attention_attn_q_bias: PARAMETER target='encoder.layer.10.attention.attn.q.bias'\n",
       "            p_encoder_layer_10_attention_attn_k_weight: PARAMETER target='encoder.layer.10.attention.attn.k.weight'\n",
       "            p_encoder_layer_10_attention_attn_k_bias: PARAMETER target='encoder.layer.10.attention.attn.k.bias'\n",
       "            p_encoder_layer_10_attention_attn_v_weight: PARAMETER target='encoder.layer.10.attention.attn.v.weight'\n",
       "            p_encoder_layer_10_attention_attn_v_bias: PARAMETER target='encoder.layer.10.attention.attn.v.bias'\n",
       "            p_encoder_layer_10_attention_attn_o_weight: PARAMETER target='encoder.layer.10.attention.attn.o.weight'\n",
       "            p_encoder_layer_10_attention_attn_o_bias: PARAMETER target='encoder.layer.10.attention.attn.o.bias'\n",
       "            p_encoder_layer_10_attention_layernorm_weight: PARAMETER target='encoder.layer.10.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_10_attention_layernorm_bias: PARAMETER target='encoder.layer.10.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_10_intermediate_dense_weight: PARAMETER target='encoder.layer.10.intermediate.dense.weight'\n",
       "            p_encoder_layer_10_intermediate_dense_bias: PARAMETER target='encoder.layer.10.intermediate.dense.bias'\n",
       "            p_encoder_layer_10_output_dense_weight: PARAMETER target='encoder.layer.10.output.dense.weight'\n",
       "            p_encoder_layer_10_output_dense_bias: PARAMETER target='encoder.layer.10.output.dense.bias'\n",
       "            p_encoder_layer_10_output_layernorm_weight: PARAMETER target='encoder.layer.10.output.LayerNorm.weight'\n",
       "            p_encoder_layer_10_output_layernorm_bias: PARAMETER target='encoder.layer.10.output.LayerNorm.bias'\n",
       "            p_encoder_layer_11_attention_attn_q_weight: PARAMETER target='encoder.layer.11.attention.attn.q.weight'\n",
       "            p_encoder_layer_11_attention_attn_q_bias: PARAMETER target='encoder.layer.11.attention.attn.q.bias'\n",
       "            p_encoder_layer_11_attention_attn_k_weight: PARAMETER target='encoder.layer.11.attention.attn.k.weight'\n",
       "            p_encoder_layer_11_attention_attn_k_bias: PARAMETER target='encoder.layer.11.attention.attn.k.bias'\n",
       "            p_encoder_layer_11_attention_attn_v_weight: PARAMETER target='encoder.layer.11.attention.attn.v.weight'\n",
       "            p_encoder_layer_11_attention_attn_v_bias: PARAMETER target='encoder.layer.11.attention.attn.v.bias'\n",
       "            p_encoder_layer_11_attention_attn_o_weight: PARAMETER target='encoder.layer.11.attention.attn.o.weight'\n",
       "            p_encoder_layer_11_attention_attn_o_bias: PARAMETER target='encoder.layer.11.attention.attn.o.bias'\n",
       "            p_encoder_layer_11_attention_layernorm_weight: PARAMETER target='encoder.layer.11.attention.LayerNorm.weight'\n",
       "            p_encoder_layer_11_attention_layernorm_bias: PARAMETER target='encoder.layer.11.attention.LayerNorm.bias'\n",
       "            p_encoder_layer_11_intermediate_dense_weight: PARAMETER target='encoder.layer.11.intermediate.dense.weight'\n",
       "            p_encoder_layer_11_intermediate_dense_bias: PARAMETER target='encoder.layer.11.intermediate.dense.bias'\n",
       "            p_encoder_layer_11_output_dense_weight: PARAMETER target='encoder.layer.11.output.dense.weight'\n",
       "            p_encoder_layer_11_output_dense_bias: PARAMETER target='encoder.layer.11.output.dense.bias'\n",
       "            p_encoder_layer_11_output_layernorm_weight: PARAMETER target='encoder.layer.11.output.LayerNorm.weight'\n",
       "            p_encoder_layer_11_output_layernorm_bias: PARAMETER target='encoder.layer.11.output.LayerNorm.bias'\n",
       "            p_encoder_relative_attention_bias_weight: PARAMETER target='encoder.relative_attention_bias.weight'\n",
       "            p_pooler_dense_weight: PARAMETER target='pooler.dense.weight'\n",
       "            p_pooler_dense_bias: PARAMETER target='pooler.dense.bias'\n",
       "            b_embeddings_position_ids: BUFFER target='embeddings.position_ids' persistent=False\n",
       "            input_ids: USER_INPUT\n",
       "            attention_mask: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            layer_norm_24: USER_OUTPUT\n",
       "            tanh: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s43: VR[0, int_oo], s53: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cpu_for_onnx = model.to('cpu').eval()\n",
    "\n",
    "onnx_model_path = \"models/model.onnx\"\n",
    "\n",
    "input_ids_cpu = sample_input_for_export['input_ids'].cpu()\n",
    "attention_mask_cpu = sample_input_for_export['attention_mask'].cpu()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu_for_onnx,\n",
    "    (input_ids_cpu, attention_mask_cpu),\n",
    "    onnx_model_path,\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc50f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_onnx_inference = tokenizer(\n",
    "    sample_text_for_onnx_inference,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "inputs_onnx_dict = {\n",
    "    \"input_ids\": inputs_onnx_inference[\"input_ids\"].astype(np.int64),\n",
    "    \"attention_mask\": inputs_onnx_inference[\"attention_mask\"].astype(np.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daab3463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold start time (Online mode): 0.135782 s\n"
     ]
    }
   ],
   "source": [
    "# 1. Measure cold start time\n",
    "start_cold_online = time.perf_counter()\n",
    "ort_session_online = ort.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers=[\"CPUExecutionProvider\"] # Explicitly use CPU\n",
    ")\n",
    "_ = ort_session_online.run(None, inputs_onnx_dict)\n",
    "end_cold_online = time.perf_counter()\n",
    "cold_start_time_online = end_cold_online - start_cold_online\n",
    "print(f\"Cold start time (Online mode): {cold_start_time_online:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c3c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg inference time (100 runs): 0.004580 s\n"
     ]
    }
   ],
   "source": [
    "def measure_onnx_inference_time(session, inputs, num_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        _ = session.run(None, inputs)\n",
    "    end_time = time.perf_counter()\n",
    "    return (end_time - start_time) / num_runs\n",
    "\n",
    "inference_time_online = measure_onnx_inference_time(ort_session_online, inputs_onnx_dict, num_runs)\n",
    "print(f\"Avg inference time (100 runs, Online mode): {inference_time_online:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8964fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize and save time (Offline mode): 0.138693 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 21:49:40.320 python[15579:438600] 2025-11-25 21:49:40.314153 [W:onnxruntime:, inference_session.cc:2473 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    }
   ],
   "source": [
    "# Offline\n",
    "optimized_onnx_model_path = \"models/model_optimized.onnx\"\n",
    "\n",
    "start_offline_optimization_save = time.perf_counter()\n",
    "options_offline = ort.SessionOptions()\n",
    "options_offline.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "options_offline.optimized_model_filepath = optimized_onnx_model_path\n",
    "\n",
    "_ = ort.InferenceSession(\n",
    "onnx_model_path,\n",
    "    sess_options=options_offline,\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "end_offline_optimization_save = time.perf_counter()\n",
    "save_optimized_time = end_offline_optimization_save - start_offline_optimization_save\n",
    "print(f\"Optimize and save time (Offline mode): {save_optimized_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c98563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold start time: 0.116052 s\n",
      "Avg inference time (Offline mode, 100 runs): 0.004716 s\n"
     ]
    }
   ],
   "source": [
    "start_cold_offline = time.perf_counter()\n",
    "options_load_optimized = ort.SessionOptions()\n",
    "options_load_optimized.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "ort_session_offline = ort.InferenceSession(\n",
    "    optimized_onnx_model_path,\n",
    "    sess_options=options_load_optimized,\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "_ = ort_session_offline.run(None, inputs_onnx_dict)\n",
    "end_cold_offline = time.perf_counter()\n",
    "cold_start_time_offline = end_cold_offline - start_cold_offline\n",
    "print(f\"Cold start time: {cold_start_time_offline:.6f} s\")\n",
    "\n",
    "inference_time_offline = measure_onnx_inference_time(ort_session_offline, inputs_onnx_dict, num_runs)\n",
    "print(f\"Avg inference time (Offline mode, 100 runs): {inference_time_offline:.6f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450400f3",
   "metadata": {},
   "source": [
    "# ANALYSYS OF TOCKER APPS\n",
    "\n",
    "1. Sizes:\n",
    "\n",
    "ONNX | torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a9ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6679b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(url, text, num_runs):\n",
    "    times = 0\n",
    "    for _ in range(num_runs):\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            json={\"text\": text}\n",
    "        )\n",
    "        times += float(response.json()['inference_time'])\n",
    "\n",
    "    return times / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75efdf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX-based app inference time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45872572422027585"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ONNX-based app inference time\")\n",
    "measure_inference_time(\"http://localhost:8000/inference\", sample_text_for_onnx_inference, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0b55dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch-based app inference time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44559636592864993"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Torch-based app inference time\")\n",
    "measure_inference_time(\"http://localhost:8001/inference\", sample_text_for_onnx_inference, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b52edcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE              COMMAND                  CREATED          STATUS          PORTS                                         NAMES                SIZE\n",
      "87f99bab5c2f   torch_app-ml-app   \"uv run uvicorn torc…\"   3 minutes ago    Up 3 minutes    0.0.0.0:8001->8001/tcp, [::]:8001->8001/tcp   torch_app-ml-app-1   5.54GB (virtual 13.2GB)\n",
      "82a9f9a94e13   onnx_app-ml-app    \"uv run uvicorn onnx…\"   21 minutes ago   Up 17 minutes   0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp   onnx_app-ml-app-1    5.5GB (virtual 12.7GB)\n"
     ]
    }
   ],
   "source": [
    "!docker ps --size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97b132",
   "metadata": {},
   "source": [
    "Again I have no idea why there are no differences..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
