{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2RmotNhQOzh"
      },
      "source": [
        "# Lab 7 - Model Optimization for Inference\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this lab, we will focus on optimizing neural network models for faster inference.\n",
        "There are many techniques available in PyTorch, including:\n",
        "\n",
        "* switching the model to **evaluation** mode and disabling gradient computation\n",
        "* various strategies for GPU speedup, e.g. optimized tensor placement, pinning memory,\n",
        "  lower precision calculations\n",
        "* using the `torch.compile()` function for automatic model compilation\n",
        "* **model quantization** to reduce size and speed up computations\n",
        "* exporting the model to ONNX format and ONNX Runtime optimization\n",
        "\n",
        "These techniques allow you to **speed up** the inference and **reduce resource usage**,\n",
        "which are crucial when deploying ML models to production systems. They are particularly\n",
        "useful for low-latency applications (e.g. online services, streaming ML), as well as for\n",
        "mobile and edge deployments with limited resources.\n",
        "\n",
        "### Environment note\n",
        "\n",
        "We recommend using a local Python environment managed with `uv`. If you encounter problems\n",
        "or do not have a CUDA-compatible GPU (e.g. on macOS), you can use Google Colab. In that case,\n",
        "remember to enable the GPU accelerator in the runtime settings.\n",
        "\n",
        "In the following exercises, we will use a pretrained Sentence Transformer model,\n",
        "`sentence-transformers/multi-qa-mpnet-base-cos-v1`. It embeds sentences as 768-dimensional vectors.\n",
        "\n",
        "## 1. Evaluation mode\n",
        "\n",
        "When using PyTorch for inference, there are several optimizations that can be applied to reduce the overhead of the model.\n",
        "They include:\n",
        "\n",
        "1. Model evaluation (eval) mode - it disables layers used only during training (e.g. dropout, batch normalization).\n",
        "   Used with `model.eval()` method.\n",
        "2. Disabling gradients - during inference, gradients are not needed, so it omits tracking them and allocating memory\n",
        "   for them. Used with `torch.no_grad()` context manager, or preferably with a more recently added and more performant\n",
        "   `torch.inference_mode()`.\n",
        "3. Inference loop optimization - avoid unnecessary repetition of operations, e.g. move model to a device beforehand,\n",
        "   pre-allocate memory.\n",
        "\n",
        "For differences between `no_grad()` and `inference_mode()`, see:\n",
        "- [this StackOverflow answer](https://stackoverflow.com/a/74197846/9472066)\n",
        "- [PyTorch forum discussion](https://discuss.pytorch.org/t/pytorch-torch-no-grad-vs-torch-inference-mode/134099)\n",
        "- [PyTorch docs on grad modes](https://docs.pytorch.org/docs/stable/notes/autograd.html#grad-modes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSHfi8RQscM"
      },
      "source": [
        "\n",
        "### Exercise 1 (3 points)\n",
        "\n",
        "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and tokenizer. Use the `AutoModel` and\n",
        "   `AutoTokenizer` classes from `tranformers` library.\n",
        "2. Create a sample input text and tokenize it (padding, truncation, `return_tensors=\"pt\"`).\n",
        "3. Measure the inference time of the model in various inference modes (average time over 100 runs):\n",
        "   - no optimizations (simple PyTorch)\n",
        "   - `model.eval()`\n",
        "   - `model.eval()` and `no_grad()`\n",
        "   - `model.eval()` and `inference_mode()`\n",
        "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To calculate speedup, divide the\n",
        "   PyTorch time by the current time.\n",
        "\n",
        "In general, the time should decrease for subsequent options. If `inference_mode()` is slower than `no_grad()`,\n",
        "it may be due some not supported operations in the model, so `no_grad()` is preferred in such cases.\n",
        "But when models contain many operations and overhead with autograd is significant, `inference_mode()` should be faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4cS5GygSCRs",
        "outputId": "415de3ec-eefb-4a71-d122-0e360469117a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 1. Load the model and tokenizer\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeaTh-v2S03y",
        "outputId": "fda0b87e-eaa2-4d35-d7a2-78b73177c168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "- No optimizations (model.train()): 0.021875 s\n",
            "- model.eval(): 0.028562 s\n",
            "- model.eval() and no_grad(): 0.025060s\n",
            "- model.eval() and inference_mode(): 0.017609 s\n"
          ]
        }
      ],
      "source": [
        "# 2. Create a sample input text and tokenize it\n",
        "sample_text = \"Rudawa – rzeka w województwie małopolskim, lewy dopływ Wisły, do której uchodzi w 75,4 km biegu Wisły, na granicy między Zwierzyńcem.\"\n",
        "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "num_runs = 100\n",
        "\n",
        "def measure_inference_time(model, inputs, num_runs):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "\n",
        "# no optimizations\n",
        "model.train()\n",
        "time_no_optim = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- No optimizations (model.train()): {time_no_optim:.6f} s\")\n",
        "\n",
        "# model.eval()\n",
        "model.eval()\n",
        "time_eval = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval(): {time_eval:.6f} s\")\n",
        "\n",
        "# model.eval() and no_grad()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    time_eval_no_grad = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval() and no_grad(): {time_eval_no_grad:.6f}s\")\n",
        "\n",
        "# model.eval() and inference_mode()\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    time_eval_inference_mode = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval() and inference_mode(): {time_eval_inference_mode:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYDBAtYYTM3I",
        "outputId": "1825a568-bbb3-4524-dc72-388de304f7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup (model.eval() over no optimizations): 0.77x\n",
            "Speedup (model.eval() + no_grad() over no optimizations): 0.87x\n",
            "Speedup (model.eval() + inference_mode() over no optimizations): 1.24x\n"
          ]
        }
      ],
      "source": [
        "# 4. Compare the speedup\n",
        "speedup_eval = time_no_optim / time_eval\n",
        "speedup_eval_no_grad = time_no_optim / time_eval_no_grad\n",
        "speedup_eval_inference_mode = time_no_optim / time_eval_inference_mode\n",
        "\n",
        "print(f\"Speedup (model.eval() over no optimizations): {speedup_eval:.2f}x\")\n",
        "print(f\"Speedup (model.eval() + no_grad() over no optimizations): {speedup_eval_no_grad:.2f}x\")\n",
        "print(f\"Speedup (model.eval() + inference_mode() over no optimizations): {speedup_eval_inference_mode:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIG1KqrpQxXq"
      },
      "source": [
        "## 2. PyTorch model compilation\n",
        "\n",
        "PyTorch 2.0 introduced a new functionality, model compilation, which automatically optimizes model execution\n",
        "via `torch.compile()` function.\n",
        "\n",
        "This mechanism uses modules such as **TorchDynamo** and **TorchInductor** under the hood to capture the model\n",
        "computation graph and generate optimized low-level code. The default backend (TorchInductor) can generate\n",
        "optimized CUDA kernels on GPU, and optimized vectorized code on CPU. It can also fuse operations together and\n",
        "bypass the overhead of memory transfers and Python interpreter.\n",
        "\n",
        "Note that `torch.compile()` is a lossless model optimization technique, changing only its physical execution.\n",
        "You should call it after setting the model to evaluation mode, so that the computation graph contains only the\n",
        "final inference operations.\n",
        "\n",
        "Example usage:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "compiled_model = torch.compile(model)\n",
        "```\n",
        "\n",
        "The above line returns a compiled version of the model that can be used just like the original model.\n",
        "During the first inference call, the model execution operations are traced and its computation graph is optimized,\n",
        "which incurs an overhead, which can be quite significant. Further calls will use the generated optimized code,\n",
        "which should be significantly faster.\n",
        "\n",
        "### Exercise 2 (2 points)\n",
        "\n",
        "In this exercise, we will verify the gains from model compilation with `torch.compile()`.\n",
        "\n",
        "1. Compile the model using `torch.compile()` after switching it to evaluation mode, and warm-up the model\n",
        "   by running a single inference call. Measure this compilation + warm-up time (just once).\n",
        "2. Measure the inference time (average of 100 runs) of the compiled model in inference mode.\n",
        "3. Calculate the speedup, and compare results with those from the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHK2z4bdQx5r",
        "outputId": "f43b11d3-0089-4b62-de88-f924d7a38cb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1125 19:54:38.386000 21573 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compilation and warm-up time: 22.410585s\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "# 1. Compile the model using torch.compile() and warm-up\n",
        "start_compile_warmup = time.perf_counter()\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = compiled_model(**inputs)\n",
        "\n",
        "end_compile_warmup = time.perf_counter()\n",
        "time_compile_warmup = end_compile_warmup - start_compile_warmup\n",
        "print(f\"Compilation and warm-up time: {time_compile_warmup:.6f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JChWfPYMVVN_",
        "outputId": "fa9ca8c7-4448-4959-efe7-ee0d969e3002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "- Compiled model inference time (with inference_mode): 0.008229s\n"
          ]
        }
      ],
      "source": [
        "# 2. Measure the inference time (average of 100 runs) of the compiled model\n",
        "\n",
        "def measure_compiled_inference_time(model_to_measure, inputs, num_runs):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.inference_mode():\n",
        "            _ = model_to_measure(**inputs)\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "time_compiled_inference = measure_compiled_inference_time(compiled_model, inputs, num_runs)\n",
        "print(f\"- Compiled model inference time (with inference_mode): {time_compiled_inference:.6f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNNVONELVXLZ",
        "outputId": "fac03c8f-1c6a-4151-9858-fa90efdc4a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup (compiled model over no optimizations): 2.66x\n",
            "Speedup (compiled model over model.eval() + inference_mode()): 2.14x\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Calculate speedup and compare results with those from the previous exercise\n",
        "\n",
        "speedup_compiled_over_no_optim = time_no_optim / time_compiled_inference\n",
        "speedup_compiled_over_inference_mode = time_eval_inference_mode / time_compiled_inference\n",
        "\n",
        "print(f\"Speedup (compiled model over no optimizations): {speedup_compiled_over_no_optim:.2f}x\")\n",
        "print(f\"Speedup (compiled model over model.eval() + inference_mode()): {speedup_compiled_over_inference_mode:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3NuuNnwV3M9"
      },
      "source": [
        "Compiled model turned out over 2 times faster in inference than not optimized. Also inference after adding model.ecal() and inference_model() was significantly slower than after compilation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_mf0-vQ2J8"
      },
      "source": [
        "## 3. Quantization\n",
        "\n",
        "Another way to optimize a model is to **quantize** its weights, reducing its size, but also the precision.\n",
        "Quantization means representing parameters (weights, and optionally also activations) with lower precision\n",
        "than the standard 32 bits. Most often, this means switching to 8-bit integers, i.e. dtype `int8`.\n",
        "\n",
        "PyTorch provides built-in tools for both **dynamic** and **static quantization**.\n",
        "\n",
        "**Dynamic quantization:**\n",
        "- convert weights `fp32 -> int8`, while activations remain in `float32` and are quantized dynamically during\n",
        "  model execution\n",
        "- does not require any post-training calibration\n",
        "- slower and more complex than static quantization, but also more precise\n",
        "- most effective and popular on CPU, which widely support integer operations\n",
        "- GPU usage requires specialized software & hardware (supporting `int8` operations)\n",
        "\n",
        "**Static quantization:**\n",
        "- quantize both weights and activations to `int8`\n",
        "- typically requires calibration, i.e. passing data through the model to estimate value ranges to know\n",
        "  how to quantize\n",
        "- faster and simpler to execute, but may be less precise (due to rounding activations)\n",
        "- more frequently used in production, particularly because the saved model files are smaller in this mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOrgd8LrQ5tu"
      },
      "source": [
        "### Exercise 3 (3 points)\n",
        "\n",
        "We will perform a dynamic quantization for our model, which is very simple operationally to use with PyTorch.\n",
        "It provides the `torch.ao.quantization.quantize_dynamic()` function, to which we pass the model and a\n",
        "list of layer types that we want to quantize. In the case of transformers, those are primarily the linear\n",
        "layers, which contain the majority of weights and perform most computations.\n",
        "\n",
        "1. Ensure the model is on the CPU.\n",
        "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting the target weight to `torch.qint8` and\n",
        "   layers to a single-element set with `nn.Linear`.\n",
        "3. Save the model to a new variable (e.g. `model_quantized`), and print it to verify that linear layers have been\n",
        "   quantized properly (i.e. `DynamicQuantizedLinear` instead of `Linear`).\n",
        "4. Save both models to disk (`state_dict` for both) and compare the file sizes (e.g. `os.path.getsize()`).\n",
        "5. Compare the inference speed and speedup on CPU for original and quantized models (again, average of 100 runs).\n",
        "6. Display the comparison. Do you think that quantization is helpful in this case?\n",
        "\n",
        "Typically, we would observe the reduction in model size up to 4x and speedup of 1.5-2x, depending on the model type\n",
        "and what parameters exactly are quantized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kYuRonNIQ808"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "model_cpu = model.to('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV3C9hGKWosZ",
        "outputId": "abd376b4-1076-468d-9086-d4fa14efc607"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1737255331.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = torch.ao.quantization.quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "# 2. Quantize the model with torch.ao.quantization.quantize_dynamic()\n",
        "model_quantized = torch.ao.quantization.quantize_dynamic(\n",
        "    model_cpu,\n",
        "    {nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbiLFkq7WxqY",
        "outputId": "49bec12d-d91c-4971-c57c-6bfea229b5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPNetModel(\n",
            "  (embeddings): MPNetEmbeddings(\n",
            "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): MPNetEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x MPNetLayer(\n",
            "        (attention): MPNetAttention(\n",
            "          (attn): MPNetSelfAttention(\n",
            "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (intermediate): MPNetIntermediate(\n",
            "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): MPNetOutput(\n",
            "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (relative_attention_bias): Embedding(32, 12)\n",
            "  )\n",
            "  (pooler): MPNetPooler(\n",
            "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# 3. Save the model to a new variable (e.g. model_quantized), and print\n",
        "print(model_quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGm3zvE5W6ZJ",
        "outputId": "dbd34724-4975-40bd-8c25-0375d8f6a986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Size Comparison\n",
            "Original model size: 417.73 MB\n",
            "Quantized model size: 173.10 MB\n",
            "Size reduction: 58.56%\n"
          ]
        }
      ],
      "source": [
        "# 4. Save both models to disk and compare file sizes\n",
        "original_model_path = \"original_model.pth\"\n",
        "quantized_model_path = \"quantized_model.pth\"\n",
        "\n",
        "torch.save(model_cpu.state_dict(), original_model_path)\n",
        "torch.save(model_quantized.state_dict(), quantized_model_path)\n",
        "\n",
        "original_size = os.path.getsize(original_model_path)\n",
        "quantized_size = os.path.getsize(quantized_model_path)\n",
        "\n",
        "print(f\"Model Size Comparison\")\n",
        "print(f\"Original model size: {original_size / (1024*1024):.2f} MB\")\n",
        "print(f\"Quantized model size: {quantized_size / (1024*1024):.2f} MB\")\n",
        "print(f\"Size reduction: {(original_size - quantized_size) / original_size * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3capdHJXNuY",
        "outputId": "35a977cb-71d6-4491-8c49-b5619fc5e872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inference Speed Comparison\n",
            "Original model inference: 0.205795 s\n",
            "Quantized model inference: 0.113990 s\n",
            "Speedup quantized over original: 1.81x\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5. Compare the inference speed and speedup on CPU\n",
        "print(\"\\nInference Speed Comparison\")\n",
        "\n",
        "model_cpu.eval()\n",
        "model_quantized.eval()\n",
        "\n",
        "inputs_cpu = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\").to('cpu')\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = model_cpu(**inputs_cpu)\n",
        "\n",
        "time_original_cpu = measure_inference_time(model_cpu, inputs_cpu, num_runs)\n",
        "print(f\"Original model inference: {time_original_cpu:.6f} s\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = model_quantized(**inputs_cpu)\n",
        "\n",
        "time_quantized_cpu = measure_inference_time(model_quantized, inputs_cpu, num_runs)\n",
        "print(f\"Quantized model inference: {time_quantized_cpu:.6f} s\")\n",
        "\n",
        "speedup_quantized = time_original_cpu / time_quantized_cpu\n",
        "print(f\"Speedup quantized over original: {speedup_quantized:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBLfsYPX7EY"
      },
      "source": [
        "Quantisation enabled significant speedup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYgkbqJQ9CN"
      },
      "source": [
        "## 4. GPU optimization strategies\n",
        "\n",
        "### GPU inference\n",
        "\n",
        "The most straightforward way to speed up inference is to run the model on a GPU if you have a suitable card\n",
        "and can afford that in the production environment. Deep models typically run much faster on GPU than on CPU,\n",
        "especially for larger batches.\n",
        "\n",
        "For example:\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
        "with torch.inference_mode():\n",
        "    outputs = model(**inputs_gpu)\n",
        "```\n",
        "\n",
        "Transferring data to the GPU involves additional overhead, so it's done explicitly in PyTorch as above.\n",
        "Due to this overhead, it may not be beneficial for tiny models and single inputs, so this should be\n",
        "kept in mind for inference.\n",
        "\n",
        "After transferring data to the GPU, it is also worth considering the use of `torch.compile()` on the model\n",
        "to gain additional acceleration through operator fusion and generation of optimized CUDA code. It works\n",
        "similarly to CPU compilation that we tried before.\n",
        "\n",
        "![torch_compile_1](images/with_torch_compile.png)\n",
        "\n",
        "### CUDA Graphs\n",
        "\n",
        "Launching individual GPU kernels for single operations incurs a significant overhead for many operations.\n",
        "Each one requires memory allocation, memory transfer, and synchronization. Instead, we can combine them\n",
        "in a **CUDA Graph**, replacing a sequence of kernels with a single, efficient operation.\n",
        "\n",
        "```python\n",
        "# Enable CUDA Graphs for maximum throughput\n",
        "compiled_model_with_cudagraphs = torch.compile(model, mode=\"max-autotune\")\n",
        "```\n",
        "\n",
        "![torch_compile_2](images/with_torch_compile_with_cuda_graphs.png)\n",
        "\n",
        "The `max-autotune` mode of PyTorch compilation can generate entirely new operations on the fly. In this mode,\n",
        "PyTorch creates several Triton kernel implementations for each operation, benchmarks their performance, and\n",
        "selects the fastest one.\n",
        "\n",
        "![torch_compile_3](images/with_torch_compile_with_cuda_kernels.png)\n",
        "\n",
        "These automatically generated kernels often outperform naive operations, or even handwritten generic\n",
        "implementations, because they are tailored for a given model. For example, tensor shapes are known and\n",
        "constant, and memory access patterns are predictable.\n",
        "\n",
        "However, CUDA Graphs are **static** by design - they record a fixed sequence of operations with predefined\n",
        "tensor shapes. This is problematic for models handling dynamic input sizes, e.g. variable-length sentences\n",
        "in transformers or images with different size in CNNs. CUDA Graphs become invalid when input dimensions\n",
        "change.\n",
        "\n",
        "The `max-autotune-no-cudagraphs` mode addresses this limitation. It still creates custom Triton kernels,\n",
        "optimized computation graphs, and fused operations, but allows the model to handle dynamic inputs without\n",
        "recompilation. This is relevant to many production environments with unpredictable input sizes, providing\n",
        "both flexibility and high performance.\n",
        "\n",
        "```python\n",
        "# Enable max-autotune without CUDA Graphs for dynamic input shapes\n",
        "compiled_model_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
        "```\n",
        "\n",
        "### Pinning GPU memory\n",
        "\n",
        "When transferring data from CPU to GPU, using **pinned (page-locked) memory** can speed up the transfer process.\n",
        "By default, PyTorch allocates tensors in pageable memory, which can be slower to transfer to GPU.\n",
        "To allocate pinned memory, use the `pin_memory=True` argument when creating tensors or DataLoader.\n",
        "\n",
        "Examples:\n",
        "\n",
        "```python\n",
        "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\", pin_memory=True)\n",
        "```\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, pin_memory=True)\n",
        "```\n",
        "\n",
        "When transferring to GPU, pinned memory allows for faster transfers, improving overall throughput.\n",
        "\n",
        "### Exercise 4 (2 points)\n",
        "\n",
        "1. Compare inference time of:\n",
        "   - `torch.compile()` with default settings\n",
        "   - `torch.compile()` with `mode=\"max-autotune\"`\n",
        "   - `torch.compile()` with `mode=\"max-autotune-no-cudagraphs\"`\n",
        "2. Report the average time of 100 runs and speedup of the latter two modes.\n",
        "\n",
        "Check a few different text input sizes. What happens in the latter two modes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HtbKeiAxRKcS"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "def measure_inference_time_gpu(model, inputs, num_runs):\n",
        "    with torch.inference_mode():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.inference_mode():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rGKjV_vmaEZg"
      },
      "outputs": [],
      "source": [
        "sample_texts_dynamic = {\n",
        "    \"short\": \"Short prompt.\",\n",
        "    \"medium\": \"Rudawa – rzeka w województwie małopolskim, lewy dopływ Wisły, do której uchodzi w 75,4 km biegu Wisły, na granicy między Zwierzyńcem a Półwsiem Zwierzynieckim, przy zachodnim krańcu bulwarze Rodła w Krakowie.\",\n",
        "    \"long\": \"Theodore John „Ted” Kaczynski, ps. Unabomber (ur. 22 maja 1942 w Chicago, zm. 10 czerwca 2023 w Butner[1]) – amerykański matematyk, terrorysta i seryjny morderca motywujący swoje działania sprzeciwem wobec społeczeństwa i cywilizacji opartych na nowoczesnej technice. Przydomek Unabomber powstał z kryptonimu UNABOM (ang. university and airline bombings), który agenci Federalnego Biura Śledczego (FBI) nadali sprawie Theodore’a Kaczynskiego.\",\n",
        "}\n",
        "\n",
        "inputs_collection = collections.OrderedDict()\n",
        "for key, text in sample_texts_dynamic.items():\n",
        "    inputs_collection[key] = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "inference_times_gpu = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IUNB7veaI3s",
        "outputId": "c9df1d50-b6f3-488c-8efb-9f5b8141033a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Uncompiled GPU (short): 0.011171 s\n",
            "  Uncompiled GPU (medium): 0.012495 s\n",
            "  Uncompiled GPU (long): 0.012553 s\n"
          ]
        }
      ],
      "source": [
        "model.to(device).eval()\n",
        "inference_times_gpu[\"uncompiled_gpu\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"uncompiled_gpu\"][key] = measure_inference_time_gpu(model, inputs_gpu, num_runs)\n",
        "    print(f\"  Uncompiled GPU ({key}): {inference_times_gpu['uncompiled_gpu'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBkf7RWlaSR3",
        "outputId": "b666d7bf-3247-45ce-e45e-5677eb972921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with default settings\n",
            "  Default compiled (short): 0.005558 s\n",
            "  Default compiled (medium): 0.009388 s\n",
            "  Default compiled (long): 0.013214 s\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Compare inference time of: torch.compile() with default settings\n",
        "print(\"torch.compile() with default settings\")\n",
        "model.to(device).eval()\n",
        "compiled_model_default = torch.compile(model)\n",
        "inference_times_gpu[\"default_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"default_compiled\"][key] = measure_inference_time_gpu(compiled_model_default, inputs_gpu, num_runs)\n",
        "    print(f\"  Default compiled ({key}): {inference_times_gpu['default_compiled'][key]:.6f} s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW7CAU5_ar4T",
        "outputId": "381f37af-fa53-477b-c7b9-a58d2744b175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with mode=`max-autotune`\n",
            "Max-autotune compiled (short): 0.004050 s\n",
            "Max-autotune compiled (medium): 0.009638 s\n",
            "Max-autotune compiled (long): 0.011872 s\n"
          ]
        }
      ],
      "source": [
        "# 1.2 Compare inference time of: torch.compile() with mode=\"max-autotune\"\n",
        "print(\"torch.compile() with mode=`max-autotune`\")\n",
        "model.to(device).eval()\n",
        "compiled_model_max_autotune = torch.compile(model, mode=\"max-autotune\")\n",
        "inference_times_gpu[\"max_autotune_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"max_autotune_compiled\"][key] = measure_inference_time_gpu(compiled_model_max_autotune, inputs_gpu, num_runs)\n",
        "    print(f\"Max-autotune compiled ({key}): {inference_times_gpu['max_autotune_compiled'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oErqEWxoa6Oe",
        "outputId": "ea993e4f-f74a-4d73-dddd-b1f86703779d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with mode=`max-autotune-no-cudagraphs`\n",
            "  Max-autotune-no-cudagraphs compiled (short): 0.004132 s\n",
            "  Max-autotune-no-cudagraphs compiled (medium): 0.009552 s\n",
            "  Max-autotune-no-cudagraphs compiled (long): 0.011757 s\n"
          ]
        }
      ],
      "source": [
        "# 1.3 Compare inference time of: torch.compile() with mode=\"max-autotune-no-cudagraphs\"\n",
        "print(\"torch.compile() with mode=`max-autotune-no-cudagraphs`\")\n",
        "model.to(device).eval()\n",
        "compiled_model_max_autotune_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
        "inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"][key] = measure_inference_time_gpu(compiled_model_max_autotune_dynamic, inputs_gpu, num_runs)\n",
        "    print(f\"  Max-autotune-no-cudagraphs compiled ({key}): {inference_times_gpu['max_autotune_no_cudagraphs_compiled'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhwT7uGpa_kR",
        "outputId": "d988702d-44c7-4bf9-b0fc-80041424f6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input Size: Short\n",
            "  uncompiled GPU: 0.011171 s\n",
            "  default compiled GPU: 0.005558 s (Speedup: 2.01x)\n",
            "  max-autotune compiled GPU: 0.004050 s (Speedup: 2.76x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.004132 s (Speedup: 2.70x)\n",
            "\n",
            "Input Size: Medium\n",
            "  uncompiled GPU: 0.012495 s\n",
            "  default compiled GPU: 0.009388 s (Speedup: 1.33x)\n",
            "  max-autotune compiled GPU: 0.009638 s (Speedup: 1.30x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.009552 s (Speedup: 1.31x)\n",
            "\n",
            "Input Size: Long\n",
            "  uncompiled GPU: 0.012553 s\n",
            "  default compiled GPU: 0.013214 s (Speedup: 0.95x)\n",
            "  max-autotune compiled GPU: 0.011872 s (Speedup: 1.06x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.011757 s (Speedup: 1.07x)\n"
          ]
        }
      ],
      "source": [
        "for size_key in sample_texts_dynamic.keys():\n",
        "    uncompiled_time = inference_times_gpu[\"uncompiled_gpu\"][size_key]\n",
        "    default_compiled_time = inference_times_gpu[\"default_compiled\"][size_key]\n",
        "    max_autotune_time = inference_times_gpu[\"max_autotune_compiled\"][size_key]\n",
        "    max_autotune_no_cudagraphs_time = inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"][size_key]\n",
        "\n",
        "    speedup_default = uncompiled_time / default_compiled_time if default_compiled_time > 0 else float('inf')\n",
        "    speedup_max_autotune = uncompiled_time / max_autotune_time if max_autotune_time > 0 else float('inf')\n",
        "    speedup_max_autotune_no_cudagraphs = uncompiled_time / max_autotune_no_cudagraphs_time if max_autotune_no_cudagraphs_time > 0 else float('inf')\n",
        "\n",
        "    print(f\"\\nInput Size: {size_key.capitalize()}\")\n",
        "    print(f\"  uncompiled GPU: {uncompiled_time:.6f} s\")\n",
        "    print(f\"  default compiled GPU: {default_compiled_time:.6f} s (Speedup: {speedup_default:.2f}x)\")\n",
        "    print(f\"  max-autotune compiled GPU: {max_autotune_time:.6f} s (Speedup: {speedup_max_autotune:.2f}x)\")\n",
        "    print(f\"  max-autotune no-cuda-graphs compiled GPU: {max_autotune_no_cudagraphs_time:.6f} s (Speedup: {speedup_max_autotune_no_cudagraphs:.2f}x)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPh5biMacL9O"
      },
      "source": [
        "Generally it seems that bigger input size leads to smaller differensces in inference time. While uncompiled model inference times didn't differ significantly, the speedup of optimized models was most visible for shorter texts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00FyIExrRKq4"
      },
      "source": [
        "## 4. Changing numerical precision\n",
        "\n",
        "Most modern CPU and GPU hardware can perform operations on 16-bit numbers (`float16` / `fp16`)\n",
        "much faster than on 32-bit numbers (`float32` / `fp32`). This is because we can pack twice the\n",
        "number of vectors into the same amount of memory, theoretically doubling the throughput. This is\n",
        "also known as half-precision computation.\n",
        "\n",
        "If your application can tolerate a minimal drop in accuracy, this kind of quantization (or precision\n",
        "reduction, depending on definition) is really useful for inference. Since this is equal to just\n",
        "cutting particular bits, this can be done on the fly easily, and some frameworks support doing\n",
        "this on model loading for weights.\n",
        "\n",
        "There are also other dedicated formats for neural networks. Newer NVidia GPUs also support `bfloat16`\n",
        "type, which retains value range and only cuts precision bits, which typically works better for neural\n",
        "networks. Further, we can use mixed precision, i.e. perform less sensitive operations in `fp16`\n",
        "(e.g. convolution), and more precise ones in `fp32` (e.g. weights updates).\n",
        "\n",
        "PyTorch also supports simplified automated casting to reduced precision types with `autocast`, see:\n",
        "- [torch.amp documentation](https://docs.pytorch.org/docs/stable/amp.html)\n",
        "- [torch.amp autocasting docs](https://docs.pytorch.org/docs/stable/amp.html#autocasting)\n",
        "- [automated mixed precision PyTorch tutorial](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
        "\n",
        "However, if your hardware does not support those types and fast operations, they probably will not\n",
        "provide any speedup, or this may even slow down execution due to type casts.\n",
        "\n",
        "You can check if your NVidia GPU supports fast float16 (via Tensor Cores) using the following code:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "capability = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA device capability: {capability}\")\n",
        "\n",
        "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
        "if capability >= (7, 0):\n",
        "    print(\"Tensor Cores available: fast float16 supported.\")\n",
        "else:\n",
        "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")\n",
        "```\n",
        "\n",
        "Casting model weights and inputs to half-precision works as follows:\n",
        "\n",
        "```python\n",
        "model_half = model.half().to('cuda')\n",
        "outputs = model_half(input_ids.to('cuda').half(), attention_mask.to('cuda').half())\n",
        "```\n",
        "\n",
        "You can also verify it by running:\n",
        "\n",
        "```python\n",
        "model_fp32 = torch.nn.Linear(10, 1)\n",
        "data_fp32 = torch.randn(100, 10)\n",
        "labels_fp32 = torch.randn(100, 1)\n",
        "\n",
        "print(f\"Data type of model_fp32 parameters: {model_fp32.weight.dtype}\")\n",
        "print(f\"Data type of data_fp32: {data_fp32.dtype}\")\n",
        "print(f\"Data type of labels_fp32: {labels_fp32.dtype}\")\n",
        "\n",
        "output_fp32 = model_fp32(data_fp32)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "loss_fp32 = loss_fn(output_fp32, labels_fp32)\n",
        "\n",
        "print(f\"Loss fp32: {loss_fp32.item()}\")\n",
        "```\n",
        "\n",
        "```python\n",
        "model_fp16 = model_fp32.half()\n",
        "data_fp16 = data_fp32.half()\n",
        "labels_fp16 = labels_fp32.half()\n",
        "\n",
        "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
        "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
        "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
        "\n",
        "output_fp16 = model_fp16(data_fp16)\n",
        "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
        "\n",
        "print(f\"Loss fp16: {loss_fp16.item()}\")\n",
        "```\n",
        "\n",
        "### Exercise 5 (2 points)\n",
        "\n",
        "1. Check if your GPU supports Tensor Cores (capability >= (7,0)). If not, switch to Google Colab with GPU runtime.\n",
        "2. Measure inference time with:\n",
        "   - full precision (`float32`)\n",
        "   - manual half-precision (`float16`)\n",
        "   - automatic mixed precision (`torch.autocast`)\n",
        "3. Compare time and speedup. Which variant would you use in practice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TsTUpN_iROiN"
      },
      "outputs": [],
      "source": [
        "model.to(device).eval()\n",
        "\n",
        "def measure_inference_time_precision(model_to_measure, inputs_to_measure, num_runs, precision_mode='fp32'):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        if precision_mode == 'autocast':\n",
        "            with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float16):\n",
        "                _ = model_to_measure(**inputs_to_measure)\n",
        "        else:\n",
        "            with torch.inference_mode():\n",
        "                _ = model_to_measure(**inputs_to_measure)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "inference_times_precision = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmZYy9GhgV9T",
        "outputId": "7cf44d13-c0fc-4a8a-d389-fea3c0c58882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Measuring inference times with different precisions (average over 100 runs):\n",
            "\n",
            "input size: short\n",
            ".  -float32: 0.008301 s\n",
            "   -float16: 0.008319 s\n",
            "   -autocast: 0.010559 s\n",
            "input size: medium\n",
            ".  -float32: 0.008539 s\n",
            "   -float16: 0.008376 s\n",
            "   -autocast: 0.013878 s\n",
            "input size: long\n",
            ".  -float32: 0.008886 s\n",
            "   -float16: 0.008598 s\n",
            "   -autocast: 0.010720 s\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMeasuring inference times with different precisions (average over 100 runs):\\n\")\n",
        "\n",
        "for key, inputs in inputs_collection.items():\n",
        "    inference_times_precision[key] = {}\n",
        "\n",
        "    # float32\n",
        "    model.to(device).eval()\n",
        "    _ = measure_inference_time_precision(model, inputs, 100, precision_mode='fp32')\n",
        "\n",
        "    time_fp32 = measure_inference_time_precision(model, inputs, num_runs, precision_mode='fp32')\n",
        "    inference_times_precision[key][\"fp32\"] = time_fp32\n",
        "\n",
        "    # float16\n",
        "    model_fp16 = model.half()\n",
        "    _ = measure_inference_time_precision(model_fp16, inputs, 100, precision_mode='fp16')\n",
        "\n",
        "    time_fp16 = measure_inference_time_precision(model_fp16, inputs, num_runs, precision_mode='fp16')\n",
        "    inference_times_precision[key][\"fp16\"] = time_fp16\n",
        "\n",
        "    # torch.autocast\n",
        "    model.to(device).eval()\n",
        "    _ = measure_inference_time_precision(model, inputs, 100, precision_mode='autocast')\n",
        "\n",
        "    time_autocast = measure_inference_time_precision(model, inputs, num_runs, precision_mode='autocast')\n",
        "    inference_times_precision[key][\"autocast\"] = time_autocast\n",
        "\n",
        "    print(f\"input size: {key}\")\n",
        "    print(f\".  -float32: {time_fp32:.6f} s\")\n",
        "    print(f\"   -float16: {time_fp16:.6f} s\")\n",
        "    print(f\"   -autocast: {time_autocast:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7IL5tEKgXZx",
        "outputId": "032d7059-8e58-4f63-83c3-c02013551541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup comparison over full prec\n",
            "\n",
            "input size: short\n",
            "  float16 speedup: 1.00x\n",
            "  autocast speedup: 0.79x\n",
            "\n",
            "input size: medium\n",
            "  float16 speedup: 1.02x\n",
            "  autocast speedup: 0.62x\n",
            "\n",
            "input size: long\n",
            "  float16 speedup: 1.03x\n",
            "  autocast speedup: 0.83x\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSpeedup comparison over full prec\")\n",
        "\n",
        "for key in inference_times_precision.keys():\n",
        "    fp32_time = inference_times_precision[key][\"fp32\"]\n",
        "    fp16_time = inference_times_precision[key][\"fp16\"]\n",
        "    autocast_time = inference_times_precision[key][\"autocast\"]\n",
        "\n",
        "    speedup_fp16 = fp32_time / fp16_time\n",
        "    speedup_autocast = fp32_time / autocast_time\n",
        "\n",
        "    print(f\"\\ninput size: {key}\")\n",
        "    print(f\"  float16 speedup: {speedup_fp16:.2f}x\")\n",
        "    print(f\"  autocast speedup: {speedup_autocast:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc1tDa8Coejt"
      },
      "source": [
        "It's difficult to choose the best variant from this results. While autocast is generally expected to perform well it was slowest here. This might be due to overhead with small input sizes, also it's likely more effective with larger inputs or during model training. Hopefully I didn't use it wrong."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
