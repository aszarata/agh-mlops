{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2RmotNhQOzh"
      },
      "source": [
        "# Lab 7 - Model Optimization for Inference\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this lab, we will focus on optimizing neural network models for faster inference.\n",
        "There are many techniques available in PyTorch, including:\n",
        "\n",
        "* switching the model to **evaluation** mode and disabling gradient computation\n",
        "* various strategies for GPU speedup, e.g. optimized tensor placement, pinning memory,\n",
        "  lower precision calculations\n",
        "* using the `torch.compile()` function for automatic model compilation\n",
        "* **model quantization** to reduce size and speed up computations\n",
        "* exporting the model to ONNX format and ONNX Runtime optimization\n",
        "\n",
        "These techniques allow you to **speed up** the inference and **reduce resource usage**,\n",
        "which are crucial when deploying ML models to production systems. They are particularly\n",
        "useful for low-latency applications (e.g. online services, streaming ML), as well as for\n",
        "mobile and edge deployments with limited resources.\n",
        "\n",
        "### Environment note\n",
        "\n",
        "We recommend using a local Python environment managed with `uv`. If you encounter problems\n",
        "or do not have a CUDA-compatible GPU (e.g. on macOS), you can use Google Colab. In that case,\n",
        "remember to enable the GPU accelerator in the runtime settings.\n",
        "\n",
        "In the following exercises, we will use a pretrained Sentence Transformer model,\n",
        "`sentence-transformers/multi-qa-mpnet-base-cos-v1`. It embeds sentences as 768-dimensional vectors.\n",
        "\n",
        "## 1. Evaluation mode\n",
        "\n",
        "When using PyTorch for inference, there are several optimizations that can be applied to reduce the overhead of the model.\n",
        "They include:\n",
        "\n",
        "1. Model evaluation (eval) mode - it disables layers used only during training (e.g. dropout, batch normalization).\n",
        "   Used with `model.eval()` method.\n",
        "2. Disabling gradients - during inference, gradients are not needed, so it omits tracking them and allocating memory\n",
        "   for them. Used with `torch.no_grad()` context manager, or preferably with a more recently added and more performant\n",
        "   `torch.inference_mode()`.\n",
        "3. Inference loop optimization - avoid unnecessary repetition of operations, e.g. move model to a device beforehand,\n",
        "   pre-allocate memory.\n",
        "\n",
        "For differences between `no_grad()` and `inference_mode()`, see:\n",
        "- [this StackOverflow answer](https://stackoverflow.com/a/74197846/9472066)\n",
        "- [PyTorch forum discussion](https://discuss.pytorch.org/t/pytorch-torch-no-grad-vs-torch-inference-mode/134099)\n",
        "- [PyTorch docs on grad modes](https://docs.pytorch.org/docs/stable/notes/autograd.html#grad-modes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSHfi8RQscM"
      },
      "source": [
        "\n",
        "### Exercise 1 (3 points)\n",
        "\n",
        "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and tokenizer. Use the `AutoModel` and\n",
        "   `AutoTokenizer` classes from `tranformers` library.\n",
        "2. Create a sample input text and tokenize it (padding, truncation, `return_tensors=\"pt\"`).\n",
        "3. Measure the inference time of the model in various inference modes (average time over 100 runs):\n",
        "   - no optimizations (simple PyTorch)\n",
        "   - `model.eval()`\n",
        "   - `model.eval()` and `no_grad()`\n",
        "   - `model.eval()` and `inference_mode()`\n",
        "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To calculate speedup, divide the\n",
        "   PyTorch time by the current time.\n",
        "\n",
        "In general, the time should decrease for subsequent options. If `inference_mode()` is slower than `no_grad()`,\n",
        "it may be due some not supported operations in the model, so `no_grad()` is preferred in such cases.\n",
        "But when models contain many operations and overhead with autograd is significant, `inference_mode()` should be faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4cS5GygSCRs",
        "outputId": "415de3ec-eefb-4a71-d122-0e360469117a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 1. Load the model and tokenizer\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeaTh-v2S03y",
        "outputId": "fda0b87e-eaa2-4d35-d7a2-78b73177c168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "- No optimizations (model.train()): 0.021875 s\n",
            "- model.eval(): 0.028562 s\n",
            "- model.eval() and no_grad(): 0.025060s\n",
            "- model.eval() and inference_mode(): 0.017609 s\n"
          ]
        }
      ],
      "source": [
        "# 2. Create a sample input text and tokenize it\n",
        "sample_text = \"Rudawa – rzeka w województwie małopolskim, lewy dopływ Wisły, do której uchodzi w 75,4 km biegu Wisły, na granicy między Zwierzyńcem.\"\n",
        "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "num_runs = 100\n",
        "\n",
        "def measure_inference_time(model, inputs, num_runs):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "\n",
        "# no optimizations\n",
        "model.train()\n",
        "time_no_optim = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- No optimizations (model.train()): {time_no_optim:.6f} s\")\n",
        "\n",
        "# model.eval()\n",
        "model.eval()\n",
        "time_eval = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval(): {time_eval:.6f} s\")\n",
        "\n",
        "# model.eval() and no_grad()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    time_eval_no_grad = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval() and no_grad(): {time_eval_no_grad:.6f}s\")\n",
        "\n",
        "# model.eval() and inference_mode()\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    time_eval_inference_mode = measure_inference_time(model, inputs, num_runs)\n",
        "print(f\"- model.eval() and inference_mode(): {time_eval_inference_mode:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYDBAtYYTM3I",
        "outputId": "1825a568-bbb3-4524-dc72-388de304f7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup (model.eval() over no optimizations): 0.77x\n",
            "Speedup (model.eval() + no_grad() over no optimizations): 0.87x\n",
            "Speedup (model.eval() + inference_mode() over no optimizations): 1.24x\n"
          ]
        }
      ],
      "source": [
        "# 4. Compare the speedup\n",
        "speedup_eval = time_no_optim / time_eval\n",
        "speedup_eval_no_grad = time_no_optim / time_eval_no_grad\n",
        "speedup_eval_inference_mode = time_no_optim / time_eval_inference_mode\n",
        "\n",
        "print(f\"Speedup (model.eval() over no optimizations): {speedup_eval:.2f}x\")\n",
        "print(f\"Speedup (model.eval() + no_grad() over no optimizations): {speedup_eval_no_grad:.2f}x\")\n",
        "print(f\"Speedup (model.eval() + inference_mode() over no optimizations): {speedup_eval_inference_mode:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIG1KqrpQxXq"
      },
      "source": [
        "## 2. PyTorch model compilation\n",
        "\n",
        "PyTorch 2.0 introduced a new functionality, model compilation, which automatically optimizes model execution\n",
        "via `torch.compile()` function.\n",
        "\n",
        "This mechanism uses modules such as **TorchDynamo** and **TorchInductor** under the hood to capture the model\n",
        "computation graph and generate optimized low-level code. The default backend (TorchInductor) can generate\n",
        "optimized CUDA kernels on GPU, and optimized vectorized code on CPU. It can also fuse operations together and\n",
        "bypass the overhead of memory transfers and Python interpreter.\n",
        "\n",
        "Note that `torch.compile()` is a lossless model optimization technique, changing only its physical execution.\n",
        "You should call it after setting the model to evaluation mode, so that the computation graph contains only the\n",
        "final inference operations.\n",
        "\n",
        "Example usage:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "compiled_model = torch.compile(model)\n",
        "```\n",
        "\n",
        "The above line returns a compiled version of the model that can be used just like the original model.\n",
        "During the first inference call, the model execution operations are traced and its computation graph is optimized,\n",
        "which incurs an overhead, which can be quite significant. Further calls will use the generated optimized code,\n",
        "which should be significantly faster.\n",
        "\n",
        "### Exercise 2 (2 points)\n",
        "\n",
        "In this exercise, we will verify the gains from model compilation with `torch.compile()`.\n",
        "\n",
        "1. Compile the model using `torch.compile()` after switching it to evaluation mode, and warm-up the model\n",
        "   by running a single inference call. Measure this compilation + warm-up time (just once).\n",
        "2. Measure the inference time (average of 100 runs) of the compiled model in inference mode.\n",
        "3. Calculate the speedup, and compare results with those from the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHK2z4bdQx5r",
        "outputId": "f43b11d3-0089-4b62-de88-f924d7a38cb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1125 19:54:38.386000 21573 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compilation and warm-up time: 22.410585s\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "# 1. Compile the model using torch.compile() and warm-up\n",
        "start_compile_warmup = time.perf_counter()\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = compiled_model(**inputs)\n",
        "\n",
        "end_compile_warmup = time.perf_counter()\n",
        "time_compile_warmup = end_compile_warmup - start_compile_warmup\n",
        "print(f\"Compilation and warm-up time: {time_compile_warmup:.6f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JChWfPYMVVN_",
        "outputId": "fa9ca8c7-4448-4959-efe7-ee0d969e3002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "- Compiled model inference time (with inference_mode): 0.008229s\n"
          ]
        }
      ],
      "source": [
        "# 2. Measure the inference time (average of 100 runs) of the compiled model\n",
        "\n",
        "def measure_compiled_inference_time(model_to_measure, inputs, num_runs):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.inference_mode():\n",
        "            _ = model_to_measure(**inputs)\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "time_compiled_inference = measure_compiled_inference_time(compiled_model, inputs, num_runs)\n",
        "print(f\"- Compiled model inference time (with inference_mode): {time_compiled_inference:.6f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNNVONELVXLZ",
        "outputId": "fac03c8f-1c6a-4151-9858-fa90efdc4a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup (compiled model over no optimizations): 2.66x\n",
            "Speedup (compiled model over model.eval() + inference_mode()): 2.14x\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Calculate speedup and compare results with those from the previous exercise\n",
        "\n",
        "speedup_compiled_over_no_optim = time_no_optim / time_compiled_inference\n",
        "speedup_compiled_over_inference_mode = time_eval_inference_mode / time_compiled_inference\n",
        "\n",
        "print(f\"Speedup (compiled model over no optimizations): {speedup_compiled_over_no_optim:.2f}x\")\n",
        "print(f\"Speedup (compiled model over model.eval() + inference_mode()): {speedup_compiled_over_inference_mode:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3NuuNnwV3M9"
      },
      "source": [
        "Compiled model turned out over 2 times faster in inference than not optimized. Also inference after adding model.ecal() and inference_model() was significantly slower than after compilation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_mf0-vQ2J8"
      },
      "source": [
        "## 3. Quantization\n",
        "\n",
        "Another way to optimize a model is to **quantize** its weights, reducing its size, but also the precision.\n",
        "Quantization means representing parameters (weights, and optionally also activations) with lower precision\n",
        "than the standard 32 bits. Most often, this means switching to 8-bit integers, i.e. dtype `int8`.\n",
        "\n",
        "PyTorch provides built-in tools for both **dynamic** and **static quantization**.\n",
        "\n",
        "**Dynamic quantization:**\n",
        "- convert weights `fp32 -> int8`, while activations remain in `float32` and are quantized dynamically during\n",
        "  model execution\n",
        "- does not require any post-training calibration\n",
        "- slower and more complex than static quantization, but also more precise\n",
        "- most effective and popular on CPU, which widely support integer operations\n",
        "- GPU usage requires specialized software & hardware (supporting `int8` operations)\n",
        "\n",
        "**Static quantization:**\n",
        "- quantize both weights and activations to `int8`\n",
        "- typically requires calibration, i.e. passing data through the model to estimate value ranges to know\n",
        "  how to quantize\n",
        "- faster and simpler to execute, but may be less precise (due to rounding activations)\n",
        "- more frequently used in production, particularly because the saved model files are smaller in this mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOrgd8LrQ5tu"
      },
      "source": [
        "### Exercise 3 (3 points)\n",
        "\n",
        "We will perform a dynamic quantization for our model, which is very simple operationally to use with PyTorch.\n",
        "It provides the `torch.ao.quantization.quantize_dynamic()` function, to which we pass the model and a\n",
        "list of layer types that we want to quantize. In the case of transformers, those are primarily the linear\n",
        "layers, which contain the majority of weights and perform most computations.\n",
        "\n",
        "1. Ensure the model is on the CPU.\n",
        "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting the target weight to `torch.qint8` and\n",
        "   layers to a single-element set with `nn.Linear`.\n",
        "3. Save the model to a new variable (e.g. `model_quantized`), and print it to verify that linear layers have been\n",
        "   quantized properly (i.e. `DynamicQuantizedLinear` instead of `Linear`).\n",
        "4. Save both models to disk (`state_dict` for both) and compare the file sizes (e.g. `os.path.getsize()`).\n",
        "5. Compare the inference speed and speedup on CPU for original and quantized models (again, average of 100 runs).\n",
        "6. Display the comparison. Do you think that quantization is helpful in this case?\n",
        "\n",
        "Typically, we would observe the reduction in model size up to 4x and speedup of 1.5-2x, depending on the model type\n",
        "and what parameters exactly are quantized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kYuRonNIQ808"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "model_cpu = model.to('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV3C9hGKWosZ",
        "outputId": "abd376b4-1076-468d-9086-d4fa14efc607"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1737255331.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_quantized = torch.ao.quantization.quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "# 2. Quantize the model with torch.ao.quantization.quantize_dynamic()\n",
        "model_quantized = torch.ao.quantization.quantize_dynamic(\n",
        "    model_cpu,\n",
        "    {nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbiLFkq7WxqY",
        "outputId": "49bec12d-d91c-4971-c57c-6bfea229b5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPNetModel(\n",
            "  (embeddings): MPNetEmbeddings(\n",
            "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): MPNetEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x MPNetLayer(\n",
            "        (attention): MPNetAttention(\n",
            "          (attn): MPNetSelfAttention(\n",
            "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (intermediate): MPNetIntermediate(\n",
            "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): MPNetOutput(\n",
            "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (relative_attention_bias): Embedding(32, 12)\n",
            "  )\n",
            "  (pooler): MPNetPooler(\n",
            "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# 3. Save the model to a new variable (e.g. model_quantized), and print\n",
        "print(model_quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGm3zvE5W6ZJ",
        "outputId": "dbd34724-4975-40bd-8c25-0375d8f6a986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Size Comparison\n",
            "Original model size: 417.73 MB\n",
            "Quantized model size: 173.10 MB\n",
            "Size reduction: 58.56%\n"
          ]
        }
      ],
      "source": [
        "# 4. Save both models to disk and compare file sizes\n",
        "original_model_path = \"original_model.pth\"\n",
        "quantized_model_path = \"quantized_model.pth\"\n",
        "\n",
        "torch.save(model_cpu.state_dict(), original_model_path)\n",
        "torch.save(model_quantized.state_dict(), quantized_model_path)\n",
        "\n",
        "original_size = os.path.getsize(original_model_path)\n",
        "quantized_size = os.path.getsize(quantized_model_path)\n",
        "\n",
        "print(f\"Model Size Comparison\")\n",
        "print(f\"Original model size: {original_size / (1024*1024):.2f} MB\")\n",
        "print(f\"Quantized model size: {quantized_size / (1024*1024):.2f} MB\")\n",
        "print(f\"Size reduction: {(original_size - quantized_size) / original_size * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3capdHJXNuY",
        "outputId": "35a977cb-71d6-4491-8c49-b5619fc5e872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inference Speed Comparison\n",
            "Original model inference: 0.205795 s\n",
            "Quantized model inference: 0.113990 s\n",
            "Speedup quantized over original: 1.81x\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 5. Compare the inference speed and speedup on CPU\n",
        "print(\"\\nInference Speed Comparison\")\n",
        "\n",
        "model_cpu.eval()\n",
        "model_quantized.eval()\n",
        "\n",
        "inputs_cpu = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\").to('cpu')\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = model_cpu(**inputs_cpu)\n",
        "\n",
        "time_original_cpu = measure_inference_time(model_cpu, inputs_cpu, num_runs)\n",
        "print(f\"Original model inference: {time_original_cpu:.6f} s\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    _ = model_quantized(**inputs_cpu)\n",
        "\n",
        "time_quantized_cpu = measure_inference_time(model_quantized, inputs_cpu, num_runs)\n",
        "print(f\"Quantized model inference: {time_quantized_cpu:.6f} s\")\n",
        "\n",
        "speedup_quantized = time_original_cpu / time_quantized_cpu\n",
        "print(f\"Speedup quantized over original: {speedup_quantized:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBLfsYPX7EY"
      },
      "source": [
        "Quantisation enabled significant speedup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfYgkbqJQ9CN"
      },
      "source": [
        "## 4. GPU optimization strategies\n",
        "\n",
        "### GPU inference\n",
        "\n",
        "The most straightforward way to speed up inference is to run the model on a GPU if you have a suitable card\n",
        "and can afford that in the production environment. Deep models typically run much faster on GPU than on CPU,\n",
        "especially for larger batches.\n",
        "\n",
        "For example:\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
        "with torch.inference_mode():\n",
        "    outputs = model(**inputs_gpu)\n",
        "```\n",
        "\n",
        "Transferring data to the GPU involves additional overhead, so it's done explicitly in PyTorch as above.\n",
        "Due to this overhead, it may not be beneficial for tiny models and single inputs, so this should be\n",
        "kept in mind for inference.\n",
        "\n",
        "After transferring data to the GPU, it is also worth considering the use of `torch.compile()` on the model\n",
        "to gain additional acceleration through operator fusion and generation of optimized CUDA code. It works\n",
        "similarly to CPU compilation that we tried before.\n",
        "\n",
        "![torch_compile_1](images/with_torch_compile.png)\n",
        "\n",
        "### CUDA Graphs\n",
        "\n",
        "Launching individual GPU kernels for single operations incurs a significant overhead for many operations.\n",
        "Each one requires memory allocation, memory transfer, and synchronization. Instead, we can combine them\n",
        "in a **CUDA Graph**, replacing a sequence of kernels with a single, efficient operation.\n",
        "\n",
        "```python\n",
        "# Enable CUDA Graphs for maximum throughput\n",
        "compiled_model_with_cudagraphs = torch.compile(model, mode=\"max-autotune\")\n",
        "```\n",
        "\n",
        "![torch_compile_2](images/with_torch_compile_with_cuda_graphs.png)\n",
        "\n",
        "The `max-autotune` mode of PyTorch compilation can generate entirely new operations on the fly. In this mode,\n",
        "PyTorch creates several Triton kernel implementations for each operation, benchmarks their performance, and\n",
        "selects the fastest one.\n",
        "\n",
        "![torch_compile_3](images/with_torch_compile_with_cuda_kernels.png)\n",
        "\n",
        "These automatically generated kernels often outperform naive operations, or even handwritten generic\n",
        "implementations, because they are tailored for a given model. For example, tensor shapes are known and\n",
        "constant, and memory access patterns are predictable.\n",
        "\n",
        "However, CUDA Graphs are **static** by design - they record a fixed sequence of operations with predefined\n",
        "tensor shapes. This is problematic for models handling dynamic input sizes, e.g. variable-length sentences\n",
        "in transformers or images with different size in CNNs. CUDA Graphs become invalid when input dimensions\n",
        "change.\n",
        "\n",
        "The `max-autotune-no-cudagraphs` mode addresses this limitation. It still creates custom Triton kernels,\n",
        "optimized computation graphs, and fused operations, but allows the model to handle dynamic inputs without\n",
        "recompilation. This is relevant to many production environments with unpredictable input sizes, providing\n",
        "both flexibility and high performance.\n",
        "\n",
        "```python\n",
        "# Enable max-autotune without CUDA Graphs for dynamic input shapes\n",
        "compiled_model_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
        "```\n",
        "\n",
        "### Pinning GPU memory\n",
        "\n",
        "When transferring data from CPU to GPU, using **pinned (page-locked) memory** can speed up the transfer process.\n",
        "By default, PyTorch allocates tensors in pageable memory, which can be slower to transfer to GPU.\n",
        "To allocate pinned memory, use the `pin_memory=True` argument when creating tensors or DataLoader.\n",
        "\n",
        "Examples:\n",
        "\n",
        "```python\n",
        "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\", pin_memory=True)\n",
        "```\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, pin_memory=True)\n",
        "```\n",
        "\n",
        "When transferring to GPU, pinned memory allows for faster transfers, improving overall throughput.\n",
        "\n",
        "### Exercise 4 (2 points)\n",
        "\n",
        "1. Compare inference time of:\n",
        "   - `torch.compile()` with default settings\n",
        "   - `torch.compile()` with `mode=\"max-autotune\"`\n",
        "   - `torch.compile()` with `mode=\"max-autotune-no-cudagraphs\"`\n",
        "2. Report the average time of 100 runs and speedup of the latter two modes.\n",
        "\n",
        "Check a few different text input sizes. What happens in the latter two modes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HtbKeiAxRKcS"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "def measure_inference_time_gpu(model, inputs, num_runs):\n",
        "    with torch.inference_mode():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.inference_mode():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rGKjV_vmaEZg"
      },
      "outputs": [],
      "source": [
        "sample_texts_dynamic = {\n",
        "    \"short\": \"Short prompt.\",\n",
        "    \"medium\": \"Rudawa – rzeka w województwie małopolskim, lewy dopływ Wisły, do której uchodzi w 75,4 km biegu Wisły, na granicy między Zwierzyńcem a Półwsiem Zwierzynieckim, przy zachodnim krańcu bulwarze Rodła w Krakowie.\",\n",
        "    \"long\": \"Theodore John „Ted” Kaczynski, ps. Unabomber (ur. 22 maja 1942 w Chicago, zm. 10 czerwca 2023 w Butner[1]) – amerykański matematyk, terrorysta i seryjny morderca motywujący swoje działania sprzeciwem wobec społeczeństwa i cywilizacji opartych na nowoczesnej technice. Przydomek Unabomber powstał z kryptonimu UNABOM (ang. university and airline bombings), który agenci Federalnego Biura Śledczego (FBI) nadali sprawie Theodore’a Kaczynskiego.\",\n",
        "}\n",
        "\n",
        "inputs_collection = collections.OrderedDict()\n",
        "for key, text in sample_texts_dynamic.items():\n",
        "    inputs_collection[key] = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "inference_times_gpu = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IUNB7veaI3s",
        "outputId": "c9df1d50-b6f3-488c-8efb-9f5b8141033a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Uncompiled GPU (short): 0.011171 s\n",
            "  Uncompiled GPU (medium): 0.012495 s\n",
            "  Uncompiled GPU (long): 0.012553 s\n"
          ]
        }
      ],
      "source": [
        "model.to(device).eval()\n",
        "inference_times_gpu[\"uncompiled_gpu\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"uncompiled_gpu\"][key] = measure_inference_time_gpu(model, inputs_gpu, num_runs)\n",
        "    print(f\"  Uncompiled GPU ({key}): {inference_times_gpu['uncompiled_gpu'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBkf7RWlaSR3",
        "outputId": "b666d7bf-3247-45ce-e45e-5677eb972921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with default settings\n",
            "  Default compiled (short): 0.005558 s\n",
            "  Default compiled (medium): 0.009388 s\n",
            "  Default compiled (long): 0.013214 s\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Compare inference time of: torch.compile() with default settings\n",
        "print(\"torch.compile() with default settings\")\n",
        "model.to(device).eval()\n",
        "compiled_model_default = torch.compile(model)\n",
        "inference_times_gpu[\"default_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"default_compiled\"][key] = measure_inference_time_gpu(compiled_model_default, inputs_gpu, num_runs)\n",
        "    print(f\"  Default compiled ({key}): {inference_times_gpu['default_compiled'][key]:.6f} s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW7CAU5_ar4T",
        "outputId": "381f37af-fa53-477b-c7b9-a58d2744b175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with mode=`max-autotune`\n",
            "Max-autotune compiled (short): 0.004050 s\n",
            "Max-autotune compiled (medium): 0.009638 s\n",
            "Max-autotune compiled (long): 0.011872 s\n"
          ]
        }
      ],
      "source": [
        "# 1.2 Compare inference time of: torch.compile() with mode=\"max-autotune\"\n",
        "print(\"torch.compile() with mode=`max-autotune`\")\n",
        "model.to(device).eval()\n",
        "compiled_model_max_autotune = torch.compile(model, mode=\"max-autotune\")\n",
        "inference_times_gpu[\"max_autotune_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"max_autotune_compiled\"][key] = measure_inference_time_gpu(compiled_model_max_autotune, inputs_gpu, num_runs)\n",
        "    print(f\"Max-autotune compiled ({key}): {inference_times_gpu['max_autotune_compiled'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oErqEWxoa6Oe",
        "outputId": "ea993e4f-f74a-4d73-dddd-b1f86703779d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile() with mode=`max-autotune-no-cudagraphs`\n",
            "  Max-autotune-no-cudagraphs compiled (short): 0.004132 s\n",
            "  Max-autotune-no-cudagraphs compiled (medium): 0.009552 s\n",
            "  Max-autotune-no-cudagraphs compiled (long): 0.011757 s\n"
          ]
        }
      ],
      "source": [
        "# 1.3 Compare inference time of: torch.compile() with mode=\"max-autotune-no-cudagraphs\"\n",
        "print(\"torch.compile() with mode=`max-autotune-no-cudagraphs`\")\n",
        "model.to(device).eval()\n",
        "compiled_model_max_autotune_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
        "inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"] = {}\n",
        "for key, inputs_gpu in inputs_collection.items():\n",
        "    inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"][key] = measure_inference_time_gpu(compiled_model_max_autotune_dynamic, inputs_gpu, num_runs)\n",
        "    print(f\"  Max-autotune-no-cudagraphs compiled ({key}): {inference_times_gpu['max_autotune_no_cudagraphs_compiled'][key]:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhwT7uGpa_kR",
        "outputId": "d988702d-44c7-4bf9-b0fc-80041424f6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input Size: Short\n",
            "  uncompiled GPU: 0.011171 s\n",
            "  default compiled GPU: 0.005558 s (Speedup: 2.01x)\n",
            "  max-autotune compiled GPU: 0.004050 s (Speedup: 2.76x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.004132 s (Speedup: 2.70x)\n",
            "\n",
            "Input Size: Medium\n",
            "  uncompiled GPU: 0.012495 s\n",
            "  default compiled GPU: 0.009388 s (Speedup: 1.33x)\n",
            "  max-autotune compiled GPU: 0.009638 s (Speedup: 1.30x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.009552 s (Speedup: 1.31x)\n",
            "\n",
            "Input Size: Long\n",
            "  uncompiled GPU: 0.012553 s\n",
            "  default compiled GPU: 0.013214 s (Speedup: 0.95x)\n",
            "  max-autotune compiled GPU: 0.011872 s (Speedup: 1.06x)\n",
            "  max-autotune no-cuda-graphs compiled GPU: 0.011757 s (Speedup: 1.07x)\n"
          ]
        }
      ],
      "source": [
        "for size_key in sample_texts_dynamic.keys():\n",
        "    uncompiled_time = inference_times_gpu[\"uncompiled_gpu\"][size_key]\n",
        "    default_compiled_time = inference_times_gpu[\"default_compiled\"][size_key]\n",
        "    max_autotune_time = inference_times_gpu[\"max_autotune_compiled\"][size_key]\n",
        "    max_autotune_no_cudagraphs_time = inference_times_gpu[\"max_autotune_no_cudagraphs_compiled\"][size_key]\n",
        "\n",
        "    speedup_default = uncompiled_time / default_compiled_time if default_compiled_time > 0 else float('inf')\n",
        "    speedup_max_autotune = uncompiled_time / max_autotune_time if max_autotune_time > 0 else float('inf')\n",
        "    speedup_max_autotune_no_cudagraphs = uncompiled_time / max_autotune_no_cudagraphs_time if max_autotune_no_cudagraphs_time > 0 else float('inf')\n",
        "\n",
        "    print(f\"\\nInput Size: {size_key.capitalize()}\")\n",
        "    print(f\"  uncompiled GPU: {uncompiled_time:.6f} s\")\n",
        "    print(f\"  default compiled GPU: {default_compiled_time:.6f} s (Speedup: {speedup_default:.2f}x)\")\n",
        "    print(f\"  max-autotune compiled GPU: {max_autotune_time:.6f} s (Speedup: {speedup_max_autotune:.2f}x)\")\n",
        "    print(f\"  max-autotune no-cuda-graphs compiled GPU: {max_autotune_no_cudagraphs_time:.6f} s (Speedup: {speedup_max_autotune_no_cudagraphs:.2f}x)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPh5biMacL9O"
      },
      "source": [
        "Generally it seems that bigger input size leads to smaller differensces in inference time. While uncompiled model inference times didn't differ significantly, the speedup of optimized models was most visible for shorter texts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00FyIExrRKq4"
      },
      "source": [
        "## 4. Changing numerical precision\n",
        "\n",
        "Most modern CPU and GPU hardware can perform operations on 16-bit numbers (`float16` / `fp16`)\n",
        "much faster than on 32-bit numbers (`float32` / `fp32`). This is because we can pack twice the\n",
        "number of vectors into the same amount of memory, theoretically doubling the throughput. This is\n",
        "also known as half-precision computation.\n",
        "\n",
        "If your application can tolerate a minimal drop in accuracy, this kind of quantization (or precision\n",
        "reduction, depending on definition) is really useful for inference. Since this is equal to just\n",
        "cutting particular bits, this can be done on the fly easily, and some frameworks support doing\n",
        "this on model loading for weights.\n",
        "\n",
        "There are also other dedicated formats for neural networks. Newer NVidia GPUs also support `bfloat16`\n",
        "type, which retains value range and only cuts precision bits, which typically works better for neural\n",
        "networks. Further, we can use mixed precision, i.e. perform less sensitive operations in `fp16`\n",
        "(e.g. convolution), and more precise ones in `fp32` (e.g. weights updates).\n",
        "\n",
        "PyTorch also supports simplified automated casting to reduced precision types with `autocast`, see:\n",
        "- [torch.amp documentation](https://docs.pytorch.org/docs/stable/amp.html)\n",
        "- [torch.amp autocasting docs](https://docs.pytorch.org/docs/stable/amp.html#autocasting)\n",
        "- [automated mixed precision PyTorch tutorial](https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
        "\n",
        "However, if your hardware does not support those types and fast operations, they probably will not\n",
        "provide any speedup, or this may even slow down execution due to type casts.\n",
        "\n",
        "You can check if your NVidia GPU supports fast float16 (via Tensor Cores) using the following code:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "capability = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA device capability: {capability}\")\n",
        "\n",
        "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
        "if capability >= (7, 0):\n",
        "    print(\"Tensor Cores available: fast float16 supported.\")\n",
        "else:\n",
        "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")\n",
        "```\n",
        "\n",
        "Casting model weights and inputs to half-precision works as follows:\n",
        "\n",
        "```python\n",
        "model_half = model.half().to('cuda')\n",
        "outputs = model_half(input_ids.to('cuda').half(), attention_mask.to('cuda').half())\n",
        "```\n",
        "\n",
        "You can also verify it by running:\n",
        "\n",
        "```python\n",
        "model_fp32 = torch.nn.Linear(10, 1)\n",
        "data_fp32 = torch.randn(100, 10)\n",
        "labels_fp32 = torch.randn(100, 1)\n",
        "\n",
        "print(f\"Data type of model_fp32 parameters: {model_fp32.weight.dtype}\")\n",
        "print(f\"Data type of data_fp32: {data_fp32.dtype}\")\n",
        "print(f\"Data type of labels_fp32: {labels_fp32.dtype}\")\n",
        "\n",
        "output_fp32 = model_fp32(data_fp32)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "loss_fp32 = loss_fn(output_fp32, labels_fp32)\n",
        "\n",
        "print(f\"Loss fp32: {loss_fp32.item()}\")\n",
        "```\n",
        "\n",
        "```python\n",
        "model_fp16 = model_fp32.half()\n",
        "data_fp16 = data_fp32.half()\n",
        "labels_fp16 = labels_fp32.half()\n",
        "\n",
        "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
        "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
        "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
        "\n",
        "output_fp16 = model_fp16(data_fp16)\n",
        "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
        "\n",
        "print(f\"Loss fp16: {loss_fp16.item()}\")\n",
        "```\n",
        "\n",
        "### Exercise 5 (2 points)\n",
        "\n",
        "1. Check if your GPU supports Tensor Cores (capability >= (7,0)). If not, switch to Google Colab with GPU runtime.\n",
        "2. Measure inference time with:\n",
        "   - full precision (`float32`)\n",
        "   - manual half-precision (`float16`)\n",
        "   - automatic mixed precision (`torch.autocast`)\n",
        "3. Compare time and speedup. Which variant would you use in practice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TsTUpN_iROiN"
      },
      "outputs": [],
      "source": [
        "model.to(device).eval()\n",
        "\n",
        "def measure_inference_time_precision(model_to_measure, inputs_to_measure, num_runs, precision_mode='fp32'):\n",
        "    start_time = time.perf_counter()\n",
        "    for _ in range(num_runs):\n",
        "        if precision_mode == 'autocast':\n",
        "            with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.float16):\n",
        "                _ = model_to_measure(**inputs_to_measure)\n",
        "        else:\n",
        "            with torch.inference_mode():\n",
        "                _ = model_to_measure(**inputs_to_measure)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.perf_counter()\n",
        "    return (end_time - start_time) / num_runs\n",
        "\n",
        "inference_times_precision = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmZYy9GhgV9T",
        "outputId": "7cf44d13-c0fc-4a8a-d389-fea3c0c58882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Measuring inference times with different precisions (average over 100 runs):\n",
            "\n",
            "input size: short\n",
            ".  -float32: 0.008301 s\n",
            "   -float16: 0.008319 s\n",
            "   -autocast: 0.010559 s\n",
            "input size: medium\n",
            ".  -float32: 0.008539 s\n",
            "   -float16: 0.008376 s\n",
            "   -autocast: 0.013878 s\n",
            "input size: long\n",
            ".  -float32: 0.008886 s\n",
            "   -float16: 0.008598 s\n",
            "   -autocast: 0.010720 s\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMeasuring inference times with different precisions (average over 100 runs):\\n\")\n",
        "\n",
        "for key, inputs in inputs_collection.items():\n",
        "    inference_times_precision[key] = {}\n",
        "\n",
        "    # float32\n",
        "    model.to(device).eval()\n",
        "    _ = measure_inference_time_precision(model, inputs, 100, precision_mode='fp32')\n",
        "\n",
        "    time_fp32 = measure_inference_time_precision(model, inputs, num_runs, precision_mode='fp32')\n",
        "    inference_times_precision[key][\"fp32\"] = time_fp32\n",
        "\n",
        "    # float16\n",
        "    model_fp16 = model.half()\n",
        "    _ = measure_inference_time_precision(model_fp16, inputs, 100, precision_mode='fp16')\n",
        "\n",
        "    time_fp16 = measure_inference_time_precision(model_fp16, inputs, num_runs, precision_mode='fp16')\n",
        "    inference_times_precision[key][\"fp16\"] = time_fp16\n",
        "\n",
        "    # torch.autocast\n",
        "    model.to(device).eval()\n",
        "    _ = measure_inference_time_precision(model, inputs, 100, precision_mode='autocast')\n",
        "\n",
        "    time_autocast = measure_inference_time_precision(model, inputs, num_runs, precision_mode='autocast')\n",
        "    inference_times_precision[key][\"autocast\"] = time_autocast\n",
        "\n",
        "    print(f\"input size: {key}\")\n",
        "    print(f\".  -float32: {time_fp32:.6f} s\")\n",
        "    print(f\"   -float16: {time_fp16:.6f} s\")\n",
        "    print(f\"   -autocast: {time_autocast:.6f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7IL5tEKgXZx",
        "outputId": "032d7059-8e58-4f63-83c3-c02013551541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Speedup comparison over full prec\n",
            "\n",
            "input size: short\n",
            "  float16 speedup: 1.00x\n",
            "  autocast speedup: 0.79x\n",
            "\n",
            "input size: medium\n",
            "  float16 speedup: 1.02x\n",
            "  autocast speedup: 0.62x\n",
            "\n",
            "input size: long\n",
            "  float16 speedup: 1.03x\n",
            "  autocast speedup: 0.83x\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSpeedup comparison over full prec\")\n",
        "\n",
        "for key in inference_times_precision.keys():\n",
        "    fp32_time = inference_times_precision[key][\"fp32\"]\n",
        "    fp16_time = inference_times_precision[key][\"fp16\"]\n",
        "    autocast_time = inference_times_precision[key][\"autocast\"]\n",
        "\n",
        "    speedup_fp16 = fp32_time / fp16_time\n",
        "    speedup_autocast = fp32_time / autocast_time\n",
        "\n",
        "    print(f\"\\ninput size: {key}\")\n",
        "    print(f\"  float16 speedup: {speedup_fp16:.2f}x\")\n",
        "    print(f\"  autocast speedup: {speedup_autocast:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc1tDa8Coejt"
      },
      "source": [
        "It's difficult to choose the best variant from this results. While autocast is generally expected to perform well it was slowest here. This might be due to overhead with small input sizes, also it's likely more effective with larger inputs or during model training. Hopefully I didn't use it wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bDSKQD8ROs2"
      },
      "source": [
        "## 5. ONNX\n",
        "\n",
        "ONNX (Open Neural Network eXchange) is a standardized format for representing neural networks. It abstracts\n",
        "operations, turning the framework-specific code into an **execution graph** built from standardized operators.\n",
        "It describes operations, input/output shapes, and model parameters in a hardware- and framework-agnostic way.\n",
        "Then, it can be run with via ONNX Runtime (ORT), which can execute the code with kernels and optimizations from\n",
        "specialized providers, like Intel OpenVINO or NVidia TensorRT.\n",
        "\n",
        "ONNX and ONNX Runtime have considerable advantages:\n",
        "1. Framework- and language-agnostic - ONNX runs on any framework and programming language, e.g. you can export\n",
        "   PyTorch model in Python, and then run it in a Java application.\n",
        "2. Execution graph optimization - ONNX Runtime provides a series of optimizations for the execution graph,\n",
        "   including hardware-specific operators provided by manufacturers.\n",
        "3. Lightweight deployment - ONNX & ORT have much smaller package size than the whole PyTorch (even CPU-only wheels),\n",
        "   reducing sizes of dependencies and Docker containers, and accelerating loading.\n",
        "\n",
        "In practice, `torch.compile()` works well for PyTorch optimization, but ONNX is preferable for deploying models,\n",
        "particularly for lightweight or mobile runtimes. It also supports GPU inference via NVidia TensorRT provider.\n",
        "\n",
        "Exporting to ONNX produces a raw computation graph in `.onnx` format. This file is:\n",
        "- a static description of operators, weights, and I/O tensors\n",
        "- a general graph - no hardware-specific rewrites happen during ONNX export\n",
        "- hardware-agnostic - it does not contain CUDA/CPU kernels or provider information\n",
        "\n",
        "We will export a Transformer model with dynamic batch size and dynamic sequence length.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Put the model in eval mode and move to CPU\n",
        "model_cpu = model.eval().cpu()\n",
        "\n",
        "# Example input for tracking (for onnx export)\n",
        "sample_input = tokenizer(\n",
        "    \"This is a sample input text for ONNX export.\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# Export to ONNX format\n",
        "torch.onnx.export(\n",
        "    model_cpu,\n",
        "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
        "    \"model.onnx\",\n",
        "    opset_version=17,\n",
        "    input_names=[\"input_ids\", \"attention_mask\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"output\": {0: \"batch_size\"},\n",
        "    },\n",
        ")\n",
        "```\n",
        "\n",
        "We export on CPU in `eval()` mode to get deterministic behavior.\n",
        "\n",
        "Look at how we marked dynamic axes in `dynamic_axes`:\n",
        "1. For `input_ids` and `attention_mask`, we marked axes 0 (batch size) and 1 (sequence length) as dynamic,\n",
        "   since they can vary during inference.\n",
        "   - axis 0 - batch size, depends on number of inputs\n",
        "   - axis 1 - sequence length, depends on text length\n",
        "   - axis 2 - embedding size, fixed and constant (768), so we don't mark it\n",
        "2. For `output`, we marked only axis 0 (batch size) as dynamic, since the output will have the same number\n",
        "   of rows as the input batch size.\n",
        "\n",
        "The exported `model.onnx` is a raw graph, not yet optimized. It can be changed during InferenceSession\n",
        "creation in ONNX Runtime or when we explicitly run offline optimizations.\n",
        "\n",
        "### Optimization & inference with ONNX Runtime\n",
        "\n",
        "First, we run inference using ONNX Runtime with default settings. By default, all optimizations are applied.\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "ort_session = ort.InferenceSession(\"model.onnx\")\n",
        "\n",
        "# Prepare input data\n",
        "sample_input = tokenizer(\n",
        "    \"This is a sample input text for ONNX inference.\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"np\",\n",
        ")\n",
        "\n",
        "\n",
        "# Create input dictionary, in same format as during export\n",
        "inputs_onnx = {\n",
        "    \"input_ids\": sample_input[\"input_ids\"],\n",
        "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
        "}\n",
        "\n",
        "# Run inference\n",
        "outputs_onnx = ort_session.run(None, inputs_onnx)\n",
        "```\n",
        "\n",
        "The raw ONNX is parsed, optimized (default level is `ORT_ENABLE_ALL`), and executed using the default\n",
        "execution provider (generally generic CPU by default).\n",
        "\n",
        "We did not specify a provider in this example to keep the code short. ONNX Runtime internally chooses\n",
        "providers based on how it was built (for example, CPU only, or CPU + CUDA). For production use, you\n",
        "should specify providers explicitly. We will do that in the next section.\n",
        "\n",
        "### Graph optimization settings\n",
        "\n",
        "ONNX Runtime groups graph optimizations into levels. Each level builds on the previous one:\n",
        "\n",
        "1. **Basic graph optimizations** - semantics-preserving rewrites that remove redundant work.\n",
        "   They run before graph partitioning, so they apply to nodes regardless of the target execution provider.\n",
        "2. **Extended graph optimizations** - They run after graph partitioning and are applied only to nodes\n",
        "   assigned to selected providers (CPU, CUDA, ROCm).\n",
        "3. **Layout optimizations** - change layout from NHCW to NCHWc for CPU provider.\n",
        "\n",
        "All optimizations are enabled by default. You can control them using the `GraphOptimizationLevel` enum:\n",
        "* `ORT_DISABLE_ALL` – disable all optimizations\n",
        "* `ORT_ENABLE_BASIC` – only basic\n",
        "* `ORT_ENABLE_EXTENDED` – basic and extended\n",
        "* `ORT_ENABLE_ALL` – basic + extended + layout optimizations (default)\n",
        "\n",
        "### Online mode (load-time optimization)\n",
        "\n",
        "In online mode, optimizations are applied each time you create an `InferenceSession`.\n",
        "This happens when you create it:\n",
        "```python\n",
        "ort_session = ort.InferenceSession(\"model.onnx\")\n",
        "```\n",
        "We can control the optimization level using `SessionOptions`:\n",
        "\n",
        "```python\n",
        "options = ort.SessionOptions()\n",
        "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "\n",
        "ort_session = ort.InferenceSession(\n",
        "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "```\n",
        "\n",
        "Online mode is most convenient for:\n",
        "- development and experimentation - you can quickly try out different settings\n",
        "- dynamic environments - when running on different hardware or deployments, depending on settings\n",
        "\n",
        "The cost of online mode is that optimization work is repeated each time a session is created, which\n",
        "may be noticeable for large models. When you deploy to a known target each time, offline mode is\n",
        "a better choice.\n",
        "\n",
        "### Offline mode (ahead-of-time optimization)\n",
        "\n",
        "In offline mode, optimizations are applied once, and the optimized model is saved to a new ONNX file.\n",
        "This can significantly reduce startup time in production environments. The key element is setting the\n",
        "`SessionOptions.optimized_model_filepath`, which specifies where to save the optimized model.\n",
        "When enabled, ONNX Runtime runs graph optimizations according to `graph_optimization_level`, and saves\n",
        "the optimized model to the file.\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "\n",
        "sess_options = ort.SessionOptions()\n",
        "\n",
        "# Choose the optimization level for the offline pass\n",
        "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
        "\n",
        "# Save the optimized model to this path\n",
        "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
        "\n",
        "# Create InferenceSession, which will perform offline optimization and save the optimized model\n",
        "ort.InferenceSession(\"model.onnx\", sess_options)\n",
        "```\n",
        "\n",
        "After you can load this file and disable optimizations to avoid re-optimizing:\n",
        "\n",
        "```python\n",
        "# Load the optimized model without re-optimizing\n",
        "sess_options = ort.SessionOptions()\n",
        "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
        "\n",
        "ort_session_optimized = ort.InferenceSession(\n",
        "    \"model_optimized.onnx\",\n",
        "    sess_options=sess_options,\n",
        "    providers=['CPUExecutionProvider']\n",
        ")\n",
        "```\n",
        "\n",
        "Offline mode is best suited for:\n",
        "- production deployments - startup time is important, and the model changes only during training\n",
        "- limited resource environments - repeated optimization is costly\n",
        "- static hardware setups - when we know the hardware configuration, there is no need for re-optimization\n",
        "\n",
        "### Executions Providers\n",
        "\n",
        "Execution providers decide how and where the nodes of the ONNX graph are executed. They are not an\n",
        "extra optimization pass on top of the graph. Instead, they are backends that provide concrete kernel\n",
        "implementations for operators such as `MatMul`, `Conv`, `LayerNorm`, and so on.\n",
        "\n",
        "Typical providers include:\n",
        "\n",
        "* `CPUExecutionProvider`\n",
        "* `CUDAExecutionProvider`\n",
        "* `TensorrtExecutionProvider`\n",
        "* `OpenVINOExecutionProvider`\n",
        "\n",
        "The ONNX file itself is always hardware-agnostic. It does not contain any provider information.\n",
        "Providers come into play only when you create an `InferenceSession`. Provider is responsible for:\n",
        "\n",
        "* mapping ONNX operations to actual kernels, e.g. CPU BLAS vs cuBLAS vs TensorRT engines\n",
        "* deciding which fused patterns it can execute efficiently for extended optimizations\n",
        "* executing its part of the graph on the target hardware\n",
        "\n",
        "So why do we need to care about providers? In production, it is better to be explicit, so that the behavior\n",
        "does not change when you move the same model to a different environment.\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "\n",
        "options = ort.SessionOptions()\n",
        "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "\n",
        "# Force CPU only\n",
        "session_cpu = ort.InferenceSession(\n",
        "    \"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "\n",
        "# Prefer CUDA, fall back to CPU if CUDA is not available\n",
        "session_cuda = ort.InferenceSession(\n",
        "    \"model.onnx\",\n",
        "    sess_options=options,\n",
        "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n",
        ")\n",
        "```\n",
        "\n",
        "For more information about providers, see the official [Execution Providers section](https://iot-robotics.github.io/ONNXRuntime/docs/execution-providers/).\n",
        "\n",
        "### Exercise 6 (3 points)\n",
        "\n",
        "1. Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes\n",
        "   on CPU.\n",
        "2. Measure inference time of the ONNX model on CPU using both optimization modes.\n",
        "3. Prepare deployment Docker images:\n",
        "   - build two images, for a) compiled PyTorch model b) ONNX model with ONNX Runtime\n",
        "   - select the best model in both cases in terms of the inference time\n",
        "   - install a minimal set of requirements in both cases, e.g. do not install PyTorch for ONNX image\n",
        "4. Compare for those apps:\n",
        "   - Docker container sizes\n",
        "   - response time (average of 100 requests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67600740"
      },
      "source": [
        "# Task\n",
        "Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and its corresponding tokenizer, and prepare a sample input text by tokenizing it. The model should be moved to the appropriate device (CPU for initial tests)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca609f4d"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and its corresponding tokenizer using `AutoModel` and `AutoTokenizer` from the `transformers` library. Ensure the model is moved to the appropriate device (CPU for initial tests, or GPU if available).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9f658e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the model and tokenizer and moving the model to the appropriate device. This code block will perform all these steps.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
